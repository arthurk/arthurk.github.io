<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>Arthur Koziel</title>
  <id>https://www.arthurkoziel.com/</id>
  <updated>2020-05-02T15:11:29+08:00</updated>
  <subtitle>Recent Blog Posts</subtitle>
  <link href="https://www.arthurkoziel.com/"></link>
  <entry>
    <title>Creating CI Pipelines with Tekton (Part 1/2)</title>
    <updated>2020-04-26T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-26:/creating-ci-pipelines-with-tekton-part-1/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post we&amp;#39;re going to build a continuous integration (CI) pipeline with &lt;a href=&#34;https://tekton.dev&#34;&gt;Tekton&lt;/a&gt;,&#xA;        an open-source framework for creating CI/CD pipelines in Kubernetes.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We&amp;#39;re going to provision a local Kubernetes cluster via &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; and install Tekton on it.&#xA;        After that we&amp;#39;ll create a pipeline consisting of two steps which will run application unit tests, build a Docker image,&#xA;        and push it to DockerHub.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;This is part 1 of 2 in which we will install Tekton and create a task that runs our application test.&#xA;        The second part will be available next week.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating the k8s cluster&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We use &lt;a href=&#34;http://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; to create a Kubernetes cluster for our Tekton installation:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind create cluster --name tekton&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Installing Tekton&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can install Tekton by applying the &lt;code&gt;release.yaml&lt;/code&gt; file from the latest release of the &lt;a href=&#34;https://github.com/tektoncd/pipeline&#34;&gt;tektoncd/pipeline&lt;/a&gt; GitHub repo:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/tektoncd/pipeline/releases/download/v0.11.3/release.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        This will install Tekton into the &lt;code&gt;tekton-pipelines&lt;/code&gt; namespace. We can check&#xA;        that the installation succeeded by listing the Pods in that namespace and&#xA;        making sure they&amp;#39;re in &lt;code&gt;Running&lt;/code&gt; state.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods --namespace tekton-pipelines&#xA;NAME                                           READY   STATUS    RESTARTS   AGE&#xA;tekton-pipelines-controller-74848c44df-m42gf   1/1     Running   0          20s&#xA;tekton-pipelines-webhook-6f764dc8bf-zq44s      1/1     Running   0          19s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Setting up the Tekton CLI&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Installing the CLI is optional but I found it to be more convenient&#xA;        than &lt;code&gt;kubectl&lt;/code&gt; when managing Tekton resources. The examples&#xA;        later on will show both ways.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can install it via Homebrew:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; brew tap tektoncd/tools&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; brew install tektoncd/tools/tektoncd-cli&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; tkn version&#xA;Client version: 0.8.0&#xA;Pipeline version: v0.11.3&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Concepts&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Tekton provides custom resource definitions (&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;CRDs&lt;/a&gt;) for Kubernetes that can be used to define our Pipelines.&#xA;        In this tutorial we will use the following custom resources:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Task: A series of steps that execute commands (In CircleCI this is called a &lt;em&gt;Job&lt;/em&gt;)&lt;/li&gt;&#xA;            &lt;li&gt;Pipeline: A set of Tasks (In CircleCI this is called a &lt;em&gt;Workflow&lt;/em&gt;)&lt;/li&gt;&#xA;            &lt;li&gt;PipelineResource: Input or Output of a Pipeline (for example a git repo or a tar file)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;We will use the following two resources to define the execution of our Tasks and Pipeline:&lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;TaskRun: Defines the execution of a Task&lt;/li&gt;&#xA;            &lt;li&gt;PipelineRun: Defines the execution of a Pipeline&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;For example,&#xA;        if we write a Task and want to test it we can execute it with a TaskRun. The same applies for a Pipeline:&#xA;        To execute a Pipeline we need to create a PipelineRun.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Application Code&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        In our example Pipeline we&amp;#39;re going to use a Go application&#xA;        that simply prints the sum of two integers. You can find the application code, test, and Dockerfile in the &lt;code&gt;src/&lt;/code&gt; directory in &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;this repo&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating our first task&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Our first Task will run the application tests inside the cloned git repo.&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/task-test.yaml&#34;&gt;task-test.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Task&#xA;metadata:&#xA;  name: test&#xA;spec:&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        type: git&#xA;  steps:&#xA;    - name: run-test&#xA;      image: golang:1.14-alpine&#xA;      workingDir: /workspace/repo/src&#xA;      command: [&amp;#34;go&amp;#34;]&#xA;      args: [&amp;#34;test&amp;#34;]&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;code&gt;resources&lt;/code&gt; block defines the inputs that our task needs to execute its steps.&#xA;        Our step needs to cloned git repository as an input to run the &lt;code&gt;go test&lt;/code&gt; command.&#xA;        &lt;/p&gt;&lt;p&gt;&#xA;        Tekton has a &lt;code&gt;git&lt;/code&gt; resource type that will automatically clone the repo into the &lt;code&gt;/workspace/$input_name&lt;/code&gt; directory.&#xA;        Since our input is named &lt;code&gt;repo&lt;/code&gt;, the code will be cloned to &lt;code&gt;/workspace/repo&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The next block defines the step that executes the command to run the tests.&#xA;        In our case we need to run the &lt;code&gt;go test&lt;/code&gt; command inside the &lt;code&gt;src/&lt;/code&gt;&#xA;        directory of the cloned repo. Note that the command (go) and args (test) need to be defined separately&#xA;        in the YAML file.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Create the task with kubectl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f task-test.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Running our task&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To run our &lt;code&gt;Task&lt;/code&gt; we have to create a &lt;code&gt;TaskRun&lt;/code&gt; that references it and&#xA;        provides all required inputs.&#xA;        To use our git repository as an input we have to create a &lt;code&gt;PipelineResource&lt;/code&gt; first.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/pipelineresource.yaml&#34;&gt;pipelineresource.yaml&lt;/a&gt;&lt;/code&gt; with the following content and apply it:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1alpha1&#xA;kind: PipelineResource&#xA;metadata:&#xA;  name: arthurk-tekton-example&#xA;spec:&#xA;  type: git&#xA;  params:&#xA;    - name: url&#xA;      value: https://github.com/arthurk/tekton-example&#xA;    - name: revision&#xA;      value: master&#xA;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f pipelineresource.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Now we can create the TaskRun. Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/taskrun.yaml&#34;&gt;taskrun.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: TaskRun&#xA;metadata:&#xA;  name: testrun&#xA;spec:&#xA;  taskRef:&#xA;    name: test&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        resourceRef:&#xA;          name: arthurk-tekton-example&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        This will take our Task (&lt;code&gt;taskRef&lt;/code&gt; is a reference to our previously created task named &lt;code&gt;test&lt;/code&gt;)&#xA;        with our git repo as an input (&lt;code&gt;resourceRef&lt;/code&gt; is a reference to our PipelineResource named &lt;code&gt;arthurk-tekton-example&lt;/code&gt;)&#xA;        and execute it.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Apply the file with kubectl and afterward check the pods and taskrun resources:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f taskrun.yaml&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods&#xA;NAME                READY   STATUS      RESTARTS   AGE&#xA;testrun-pod-pds5z   0/2     Completed   0          4m27s&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get taskrun&#xA;NAME      SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME&#xA;testrun   True        Succeeded   70s         57s&#xA;&lt;/pre&gt;&#xA;&#xA;&#xA;        &lt;p&gt;To see the stdout of the containers we can run:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl logs testrun-pod-pds5z --all-containers&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1587898529.358668,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:105&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ master in path /workspace/repo&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;warn&amp;#34;,&amp;#34;ts&amp;#34;:1587898529.3588462,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:152&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Unexpected error: creating symlink: symlink /tekton/home/.ssh /root/.ssh: file exists&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1587898529.4190812,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:133&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;PASS&#xA;ok  &#x9;_/workspace/repo/src&#x9;0.003s&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Our tests passed and our task succeeded.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;        &lt;h2&gt;Using the Tekton CLI to run a Task&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The Tekton CLI provides a faster and more convenient way to run Tasks.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Instead of manually&#xA;        writing a &lt;code&gt;TaskRun&lt;/code&gt; manifest we can run the following command&#xA;        which takes our Task (&lt;code&gt;test&lt;/code&gt;), generates a &lt;code&gt;TaskRun&lt;/code&gt; (with a random name) and shows&#xA;        its logs:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; tkn task start test --inputresource repo=arthurk-tekton-example --showlog&#xA;&#xA;Taskrun started: test-run-xlx57&#xA;Waiting for logs to be available...&#xA;[git-source-arthurk-tekton-example-4pxbh] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1587899369.4304056,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:105&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ master in path /workspace/repo&amp;#34;}&#xA;[git-source-arthurk-tekton-example-4pxbh] {&amp;#34;level&amp;#34;:&amp;#34;warn&amp;#34;,&amp;#34;ts&amp;#34;:1587899369.4305482,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:152&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Unexpected error: creating symlink: symlink /tekton/home/.ssh /root/.ssh: file exists&amp;#34;}&#xA;[git-source-arthurk-tekton-example-4pxbh] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1587899369.4896345,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:133&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[run-test] PASS&#xA;[run-test] ok  &#x9;_/workspace/repo/src&#x9;0.004s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            We have successfully installed Tekton on a local Kubernetes&#xA;            cluster, defined a Task, and tested it by creating a TaskRun via&#xA;            YAML manifest as well as the Tekton CLI &lt;code&gt;tkn&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        In the next part we&amp;#39;re going to create a task that will&#xA;        use &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;Kaniko&lt;/a&gt; to build a Docker image&#xA;        for our application and then push it to DockerHub.&#xA;        We will then create a Pipeline that runs both of our tasks sequentially (run application tests, build and push).&#xA;        &lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Running Knative with Istio in a Kind Cluster</title>
    <updated>2020-04-19T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-19:/running-knative-with-istio-in-kind/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post I&amp;#39;m going to show how to run &lt;a href=&#34;https://knative.dev&#34;&gt;Knative&lt;/a&gt; with &lt;a href=&#34;https://istio.io&#34;&gt;Istio&lt;/a&gt;&#xA;        as a networking layer on a local &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; cluster.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        I&amp;#39;m assuming that kind and kubectl are installed. Installation instructions for kind are &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/#installation&#34;&gt;here&lt;/a&gt;&#xA;        and kubectl &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind --version&#xA;kind kind version 0.7.0&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl version --client&#xA;Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;15&amp;#34;, GitVersion:&amp;#34;v1.15.5&amp;#34;, GitCommit:&amp;#34;20c265fef0741dd71a66480e35bd69f18351daea&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2019-10-15T19:16:51Z&amp;#34;, GoVersion:&amp;#34;go1.12.10&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Setup a kind cluster&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To get traffic into our cluster we need to create our kind cluster with a&#xA;        custom configuration that sets up a port forward from host to ingress&#xA;        controller.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        In this setup we&amp;#39;re going to use port &lt;code&gt;32000&lt;/code&gt;. Later we will configure&#xA;        the Istio ingress gateway to accept connections on this port.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;kind-config-istio.yml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;kind&lt;/span&gt;: Cluster&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;apiVersion&lt;/span&gt;: kind.sigs.k8s.io/v1alpha3&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;nodes&lt;/span&gt;:&#xA;- &lt;span style=&#34;color:#66d9ef&#34;&gt;role&lt;/span&gt;: control-plane&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;extraPortMappings&lt;/span&gt;:&#xA;  - &lt;span style=&#34;color:#66d9ef&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32000&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;hostPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To create the cluster with our custom configuration we use the &lt;code&gt;--config&lt;/code&gt; argument:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind create cluster --config kind-config-istio.yml&#xA;&#xA;Creating cluster &amp;#34;kind&amp;#34; ...&#xA; ✓ Ensuring node image (kindest/node:v1.17.0) 🖼&#xA; ✓ Preparing nodes 📦&#xA; ✓ Writing configuration 📜&#xA; ✓ Starting control-plane 🕹️&#xA; ✓ Installing CNI 🔌&#xA; ✓ Installing StorageClass 💾&#xA;Set kubectl context to &amp;#34;kind-kind&amp;#34;&#xA;You can now use your cluster with:&#xA;&#xA;kubectl cluster-info --context kind-kind&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Install Istio&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We&amp;#39;re going to install Istio via the &lt;a href=&#34;https://istio.io/docs/reference/commands/istioctl/&#34;&gt;istioctl&lt;/a&gt;&#xA;        command-line tool.&#xA;        The following command will download version istioctl v1.5.1 for macOS and extract it into the current directory:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -L https://github.com/istio/istio/releases/download/1.5.1/istioctl-1.5.1-osx.tar.gz | tar xvz -&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl version --remote=false&#xA;1.5.1&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Istio can be installed with different configuration profiles. In this example&#xA;        we are going to use the &lt;code&gt;default&lt;/code&gt; profile which will&#xA;        install the pilot, ingressgateway and prometheus.&#xA;        A list of all built-in&#xA;        configuration profiles and their differences can be found &lt;a href=&#34;https://istio.io/docs/setup/additional-setup/config-profiles/&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The following command will perform the installation:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl manifest apply --set profile=default&#xA;&#xA;Detected that your cluster does not support third party JWT authentication. Falling back to less secure first party JWT. See https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens for details.&#xA;- Applying manifest for component Base...&#xA;✔ Finished applying manifest for component Base.&#xA;- Applying manifest for component Pilot...&#xA;✔ Finished applying manifest for component Pilot.&#xA;- Applying manifest for component IngressGateways...&#xA;- Applying manifest for component AddonComponents...&#xA;✔ Finished applying manifest for component AddonComponents.&#xA;✔ Finished applying manifest for component IngressGateways.&#xA;&#xA;✔ Installation complete&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can check that the pods are running via kubectl:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods -n istio-system&#xA;&#xA;NAME                                    READY   STATUS    RESTARTS   AGE&#xA;istio-ingressgateway-5f54974979-crw9d   1/1     Running   0          21s&#xA;istiod-6548b95486-djvd6                 1/1     Running   0          6m57s&#xA;prometheus-6c88c4cb8-wtdtn              2/2     Running   0          21s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To verify the installation we can run the &lt;code&gt;verify-install&lt;/code&gt; command&#xA;        and pass in the manifest of the default configuration profile:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl manifest generate --set profile=default | ./istioctl verify-install -f -&#xA;...&#xA;Checked 25 crds&#xA;Checked 1 Istio Deployments&#xA;Istio is installed successfully&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The configuration profile&#xA;        will set the ingress type to &lt;code&gt;LoadBalancer&lt;/code&gt;, which&#xA;        is not working on a local cluster.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        For the ingress gateway to accept incoming connections we have to change&#xA;        the type from &lt;code&gt;LoadBalancer&lt;/code&gt; to &lt;code&gt;NodePort&lt;/code&gt; and change the assigned port&#xA;        to &lt;code&gt;32000&lt;/code&gt; (the port we forwarded during the cluster creation).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;patch-ingressgateway-nodeport.yaml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt;: NodePort&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;ports&lt;/span&gt;:&#xA;  - &lt;span style=&#34;color:#66d9ef&#34;&gt;name&lt;/span&gt;: http2&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32000&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;protocol&lt;/span&gt;: TCP&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We apply the file with &lt;code&gt;kubectl patch&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl patch service istio-ingressgateway -n istio-system --patch &amp;#34;$(cat patch-ingressgateway-nodeport.yaml)&amp;#34;&#xA;&#xA;service/istio-ingressgateway patched&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Istio is now set up and ready to accept connections.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Install Knative&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative consists of two components: &lt;a href=&#34;https://knative.dev/docs/serving/&#34;&gt;Serving&lt;/a&gt; and &lt;a href=&#34;https://knative.dev/docs/eventing/&#34;&gt;Eventing&lt;/a&gt;. In this example we&amp;#39;re going to install the Serving component.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        We start by applying the Kubernetes manifests for the CRDs, Core and Istio ingress controller:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/serving/releases/download/v0.14.0/serving-crds.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/serving/releases/download/v0.14.0/serving-core.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/net-istio/releases/download/v0.14.0/net-istio.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We check the pods via kubectl and wait until they have the status &lt;code&gt;Running&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods --namespace knative-serving&#xA;&#xA;NAME                                READY   STATUS    RESTARTS   AGE&#xA;activator-65fc4d666-2bj8r           2/2     Running   0          9m&#xA;autoscaler-74b4bb97bd-9rql4         2/2     Running   0          9m&#xA;controller-6b6978c965-rks25         2/2     Running   0          9m&#xA;istio-webhook-856d84fbf9-8nswp      2/2     Running   0          8m58s&#xA;networking-istio-6845f7cf59-6h25b   1/1     Running   0          8m58s&#xA;webhook-577576647-rw264             2/2     Running   0          9m&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative will create a custom URL for each service and for this to work&#xA;        it needs to have DNS configured. Since our cluster is running locally we need to&#xA;        use a wildcard DNS service (for example &lt;a href=&#34;https://nip.io&#34;&gt;nip.io&lt;/a&gt;).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        We patch the Knative config via kubectl and set the domain to &lt;code&gt;127.0.0.1.nip.io&lt;/code&gt;&#xA;        which will forward all requests to &lt;code&gt;127.0.0.1&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl patch configmap/config-domain \&#xA;  --namespace knative-serving \&#xA;  --type merge \&#xA;  --patch &amp;#39;{&amp;#34;data&amp;#34;:{&amp;#34;127.0.0.1.nip.io&amp;#34;:&amp;#34;&amp;#34;}}&amp;#39;&#xA;&#xA;configmap/config-domain patched&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative is now installed and ready to use.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating a test service&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To check that Knative is working correctly we deploy a test service&#xA;        that consists of an &lt;a href=&#34;https://github.com/jmalloc/echo-server&#34;&gt;echo-server&lt;/a&gt; which will return the request&#xA;        headers and body.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We start by creating a file named &lt;code&gt;knative-echoserver.yaml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;apiVersion&lt;/span&gt;: serving.knative.dev/v1&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;kind&lt;/span&gt;: Service&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;metadata&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;name&lt;/span&gt;: helloworld&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt;: default&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt;:&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;      &lt;span style=&#34;color:#66d9ef&#34;&gt;containers&lt;/span&gt;:&#xA;        - &lt;span style=&#34;color:#66d9ef&#34;&gt;image&lt;/span&gt;: jmalloc/echo-server&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We enable Istio sidecar injection for the default namespace&#xA;        and deploy the Knative service in it:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl label namespace default istio-injection=enabled&#xA;&#xA;namespace/default labeled&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f knative-echoserver.yaml&#xA;&#xA;service.serving.knative.dev/helloworld created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can check the deployment of the pods via kubectl:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods&#xA;&#xA;NAME                                           READY   STATUS    RESTARTS   AGE&#xA;helloworld-96c68-deployment-6744444b5f-6htld   3/3     Running   0          108s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        When all pods are running we can get the URL of the service and&#xA;        make an HTTP request to it:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get ksvc&#xA;&#xA;NAME         URL                                          LATESTCREATED      LATESTREADY        READY   REASON&#xA;helloworld   http://helloworld.default.127.0.0.1.nip.io   helloworld-96c68   helloworld-96c68   True&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl http://helloworld.default.127.0.0.1.nip.io&#xA;&#xA;Request served by helloworld-96c68-deployment-6744444b5f-6htld&#xA;&#xA;HTTP/1.1 GET /&#xA;&#xA;Host: helloworld.default.127.0.0.1.nip.io&#xA;X-Request-Id: 9e5bf3c9-0bc8-4551-9302-ea2eca5f6446&#xA;User-Agent: curl/7.64.1&#xA;Accept-Encoding: gzip&#xA;Forwarded: for=10.244.0.1;proto=http, for=127.0.0.1&#xA;X-B3-Traceid: d22e218318367687170ce339b13b0c91&#xA;X-Forwarded-For: 10.244.0.1, 127.0.0.1, 127.0.0.1&#xA;X-B3-Spanid: 0e3174748253699d&#xA;X-Forwarded-Proto: http&#xA;Accept: */*&#xA;K-Proxy-Request: activator&#xA;X-B3-Parentspanid: c39ad4d28b42b25f&#xA;X-B3-Sampled: 0&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            The response shows the pod which served the request (&lt;code&gt;helloworld-96c68-deployment-6744444b5f-6htld&lt;/code&gt;)&#xA;            and the tracing headers that Istio will add to every request.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        If we wait a few minutes we can see that Knative will scale down our&#xA;        service to zero replicas (no incoming requests). In this case&#xA;        we can make another request to the service and see it scale up again.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/running-knative-with-istio-in-kind/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Writing tar.gz files in Go</title>
    <updated>2020-04-12T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-12:/writing-tar-gz-files-in-go/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            In this blog post I&amp;#39;m going to explain how to use the Go &lt;code&gt;archive/tar&lt;/code&gt; and &lt;code&gt;compress/gzip&lt;/code&gt;&#xA;            packages to create a tar archive and compress it with gzip.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;Below is the full example code and after that there&amp;#39;s an explanation of the parts.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Full Code&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#f92672&#34;&gt;package&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;io&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;archive/tar&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;compress/gzip&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fmt&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;os&amp;#34;&lt;/span&gt;&#xA;)&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Files which to include in the tar.gz archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test/test.txt&amp;#34;&lt;/span&gt;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create output file&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Create&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output.tar.gz&amp;#34;&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error writing archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create the archive and write the output to the &amp;#34;out&amp;#34; Writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error creating archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Println&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Archive created successfully&amp;#34;&lt;/span&gt;)&#xA;}&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create new Writers for gzip and tar&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// These writers are chained. Writing to the tar writer will&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// write to the gzip writer which in turn will write to&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// the &amp;#34;buf&amp;#34; writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gzip&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Iterate over files and add them to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;range&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;&#x9;}&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Open the file which will be written into the archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Open&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Get FileInfo about our file providing file size, mode, etc.&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Stat&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create a tar Header from the FileInfo data&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;FileInfoHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt;())&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Use full path as name (FileInfoHeader only takes the basename)&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// If we don&amp;#39;t do this the directory strucuture would&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// not be preserved&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// https://golang.org/src/archive/tar/common.go?#L626&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Write file header to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;WriteHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Copy file content to tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Copy&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Explanation&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            In the main function we first declare &lt;code&gt;files&lt;/code&gt; as a string slice.&#xA;            It contains the paths of the files that will be included in the archive.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            For this example I&amp;#39;ve created two text files. I placed one of them in the same&#xA;            directory as the &lt;code&gt;main.go&lt;/code&gt; file and the other one in a subdirectory. The purpose&#xA;            of this is to test that the directory structure will be correctly restored after extraction.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We then create the output file with &lt;code&gt;&lt;a href=&#34;https://golang.org/pkg/os/#Create&#34;&gt;os.Create()&lt;/a&gt;&lt;/code&gt;&#xA;            and pass it to the &lt;code&gt;createArchive&lt;/code&gt; function along with our file paths.&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Files which to include in the tar.gz archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test/test.txt&amp;#34;&lt;/span&gt;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create output file&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Create&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output.tar.gz&amp;#34;&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error writing archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create the archive and write the output to the &amp;#34;out&amp;#34; Writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error creating archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Println&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Archive created successfully&amp;#34;&lt;/span&gt;)&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;&#xA;        The &lt;code&gt;createArchive&lt;/code&gt; function creates two Writer: The &lt;a href=&#34;https://golang.org/pkg/archive/tar/#NewWriter&#34;&gt;tar Writer&lt;/a&gt;&#xA;        and the &lt;a href=&#34;https://golang.org/pkg/compress/gzip/#NewWriter&#34;&gt;gzip Writer&lt;/a&gt;. Both implement the &lt;a href=&#34;https://golang.org/pkg/io/#Writer&#34;&gt;io.Writer&lt;/a&gt; interface.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        The Writers are chained which means that bytes written to the tar Writer &lt;code&gt;tw&lt;/code&gt;&#xA;        will simultaneously be written to the gzip Writer &lt;code&gt;gw&lt;/code&gt;.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        We will then iterate over the files in the &lt;code&gt;files&lt;/code&gt; slice and&#xA;        call the &lt;code&gt;addToArchive&lt;/code&gt; function for each of them with the filename and the tar Writer&#xA;        as arguments.&#xA;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create new Writers for gzip and tar&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// These writers are chained. Writing to the tar Writer will&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// write to the gzip writer which in turn will write to&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// the &amp;#34;buf&amp;#34; writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gzip&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Iterate over files and and add them to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;range&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;&#x9;}&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Inside the &lt;code&gt;addToArchive&lt;/code&gt; function we open the file and get&#xA;            a &lt;code&gt;&lt;a href=&#34;https://golang.org/pkg/os/#FileInfo&#34;&gt;FileInfo&lt;/a&gt;&lt;/code&gt;.&#xA;            The FileInfo contains information such as the file name, size or mode which is necessary for the next step.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Open the file which will be written into the archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Open&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Get FileInfo about our file providing file size, mode, etc.&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Stat&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;&#xA;        Each file in a tar archive has a &lt;a href=&#34;https://golang.org/pkg/archive/tar/#Header&#34;&gt;header&lt;/a&gt; containing metadata about the file&#xA;        followed by the file content. In this step we create the header by&#xA;        calling &lt;a href=&#34;https://golang.org/pkg/archive/tar/#FileInfoHeader&#34;&gt;FileInfoHeader&lt;/a&gt; which will take our FileInfo &lt;code&gt;info&lt;/code&gt;&#xA;        and generate a valid tar Header from it.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        The os.FileInfo &lt;code&gt;info&lt;/code&gt; only stores the base name of the file. For example if we pass in &lt;code&gt;test/test.txt&lt;/code&gt; it&#xA;        will only store the filename &lt;code&gt;test.txt&lt;/code&gt;. This is a problem when creating the tar archive as it would omit&#xA;        the directory structure of our files.&#xA;        To fix this we have to set &lt;code&gt;header.Name&lt;/code&gt; to the full file path.&#xA;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create a tar Header from the FileInfo data&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;FileInfoHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt;())&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Use full path as name (FileInfoHeader only takes the basename)&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// If we don&amp;#39;t do this the directory strucuture would&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// not be preserved&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// https://golang.org/src/archive/tar/common.go?#L626&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Now we can write the header and the file content to the Writer.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Write file header to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;WriteHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Copy file content to tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Copy&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;h2&gt;Run the program&lt;/h2&gt;&#xA;    &lt;p&gt;We can now run our program and check that the files can be extracted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; go run main.go&#xA;Archive created successfully&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; tar xzfv output.tar.gz&#xA;x example.txt&#xA;x test/test.txt&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; exa --tree&#xA;.&#xA;├── example.txt&#xA;├── output.tar.gz&#xA;└── test&#xA;   └── test.txt&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Both files have been extracted successfully.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/writing-tar-gz-files-in-go/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Notes about Google CloudSQL for Postgres</title>
    <updated>2020-04-05T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-05:/notes-about-google-cloudsql-for-postgres/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            Here are a few things that I&amp;#39;ve learned about Google CloudSQL for Postgres&#xA;            during the last 2 years in which I&amp;#39;ve been using it.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Price&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;It&amp;#39;s usually the most expensive part of the project accounting for around 80% of the total cost. The parts that need to be paid for are CPU Cores, RAM, Disk Storage and Network Internet Egress. Automated backups and HA are optional and cost extra.&lt;/li&gt;&#xA;            &lt;li&gt;There&amp;#39;s a &lt;a href=&#34;https://cloud.google.com/products/calculator&#34;&gt;pricing calculator&lt;/a&gt; available but I haven&amp;#39;t been able to replicate the price on the invoice (at least in my case the total on the invoice was &lt;em&gt;lower&lt;/em&gt; than what the calculator showed). I suggest to just try it for one or two months and see for yourself. Make sure to set a budget in GCP so the cost doesn&amp;#39;t go too high.&lt;/li&gt;&#xA;            &lt;li&gt;Read-only replicas need to have at least the same hardware as the master instance. This means each read-replica will double the cost.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Replication&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;As mentioned above a read-replica needs to have at least the same hardware (cores, memory, storage) as the master instance. It can have better hardware.&lt;/li&gt;&#xA;            &lt;li&gt;External replication is not supported. You can create read-only replicas in CloudSQL but it&amp;#39;s not possible to use streaming replication to an external Postgres instance.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;HA (High Availability)&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Failover will take place after the master instance is unresponsive for 1 minute. In total it takes 2-3 minutes for connections to be re-established.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Backups&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Deleting the instance will delete all of its backups too. Always make sure to have an additional backup job running that will export the data to another location.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Performance&lt;/h2&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Network throughput (MB/s) depends on the number of CPU cores. More CPU Cores = More throughput. 1 CPU core has 250 MB/s throughput and the maximum is 2000 MB/s which is reached at 8 cores.&lt;/li&gt;&#xA;            &lt;li&gt;Disk throughput and IOPS depend on the disk size. The minimum size is 10 GB which has 4.8 MB/s of read/write throughput and 300 IOPS (read/write). The maximum is 800 MB/s read and 400 MB/s write throughput with 15,000 IOPS (read/write) which is reached at 500 GB disk size.&lt;/li&gt;&#xA;            &lt;li&gt;The network latency from a GKE instance to CloudSQL is around 3ms. There is no difference in latency between using a private and public ip.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Maintenance&lt;/h2&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;The maintenance downtime is 1-2 minutes and occurs during a selected time window.&lt;/li&gt;&#xA;            &lt;li&gt;Maintenance notifications were recently added. Only e-mail notifications are supported.&lt;/li&gt;&#xA;            &lt;li&gt;Upgrading Postgres to a new major version is only possible by dumping the data and then importing it after the upgrade. For our 128 GB database it took around 40 minutes to export and 5 hours to import (pg_restore). This is not including the time it took to download the export from Cloud Storage.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/notes-about-google-cloudsql-for-postgres/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Managing Helm Charts with Helmfile</title>
    <updated>2020-03-29T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-29:/managing-helm-charts-with-helmfile/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            In this blog post I&amp;#39;m going to show how &lt;a href=&#34;https://github.com/roboll/helmfile&#34;&gt;Helmfile&lt;/a&gt; makes it easier&#xA;            to manage Helm charts and environments.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this I&amp;#39;m going to walk through an example where at the beginning&#xA;            we install helm charts over the CLI using the &lt;code&gt;helm&lt;/code&gt; command,&#xA;            and then refactor the code in steps to use the &lt;code&gt;helmfile&lt;/code&gt; command instead.&#xA;        &lt;/p&gt;&#xA;        &lt;h2&gt;Setup&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Our setup consists of 2 applications (backend and frontend) and Prometheus&#xA;            for metrics. We have helm charts for:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Backend (custom chart)&lt;/li&gt;&#xA;            &lt;li&gt;Frontend (custom chart)&lt;/li&gt;&#xA;            &lt;li&gt;Prometheus (chart from the &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus&#34;&gt;helm stable repo&lt;/a&gt;)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            which are deployed into these environments:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Development&lt;/li&gt;&#xA;            &lt;li&gt;Staging&lt;/li&gt;&#xA;            &lt;li&gt;Production&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            The files are organized in this directory structure:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;.&#xA;└── charts&#xA;   ├── backend&#xA;   │  ├── Chart.yaml&#xA;   │  ├── templates&#xA;   │  └── values-development.yaml&#xA;   │  └── values-staging.yaml&#xA;   │  └── values-production.yaml&#xA;   │  └── secrets-development.yaml&#xA;   │  └── secrets-staging.yaml&#xA;   │  └── secrets-production.yaml&#xA;   └── frontend&#xA;   │  ├── Chart.yaml&#xA;   │  ├── templates&#xA;   │  └── values-development.yaml&#xA;   │  └── values-staging.yaml&#xA;   │  └── values-production.yaml&#xA;   │  └── secrets-development.yaml&#xA;   │  └── secrets-staging.yaml&#xA;   │  └── secrets-production.yaml&#xA;   └── prometheus&#xA;      └── values-development.yaml&#xA;      └── values-staging.yaml&#xA;      └── values-production.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            Each values-development.yaml, values-staging.yaml, values-production.yaml file&#xA;            contains values that are specific to that environment.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            For example the development environment only needs to deploy 1 replica of&#xA;            the backend while the staging and production environments need 3 replicas.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We use &lt;a href=&#34;https://github.com/fstech/helm-secrets&#34;&gt;helm-secrets&lt;/a&gt; to manage secrets.&#xA;            Each secrets file is encrypted and has to be manually decrypted before deploying the chart.&#xA;            After the deployment is done the decrypted file has to be deleted.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Installation and Upgrades&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            With the above setup we use the following commands to deploy&#xA;            (install/upgrade) the backend chart in the staging environment:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helm secrets dec ./charts/backend/secrets-backend.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helm upgrade --install --atomic --cleanup-on-fail -f ./charts/backend/values-staging.yaml -f ./charts/backend/secrets-staging.yaml backend ./charts/backend&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; rm ./charts/backend/secrets-backend.yaml.dec&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            We use the &lt;code&gt;helm upgrade&lt;/code&gt; command with the &lt;code&gt;--install&lt;/code&gt; flag to&#xA;            be able to install and upgrade charts with the same command.&#xA;&#xA;            We also use the &lt;code&gt;--atomic&lt;/code&gt; and &lt;code&gt;--cleanup-on-fail&lt;/code&gt;&#xA;            flags to rollback changes in case a chart upgrade fails.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To deploy the other charts we have to repeat the same commands (for the prometheus&#xA;            chart we can leave out the part that handles secrets).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Now the problem is that it&amp;#39;s hard to remember the exact commands to&#xA;            run when deploying a chart (especially when the upgrades are not very frequent).&#xA;&#xA;            When multiple people are responsible for deployments it&amp;#39;s also difficult to&#xA;            make sure the same commands are used. If, for example, the secrets were not&#xA;            decrypted beforehand it will lead to encrypted values being deployed and probably&#xA;            crash the application.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Writing Bash Scripts&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To fix the issues mentioned above we can write bash scripts that execute&#xA;            the exact commands needed for a deployment.&#xA;&#xA;            We create one script per environment in each chart directory which leads to the&#xA;            following directory tree for the backend chart:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;.&#xA;└── charts&#xA;   ├── backend&#xA;      ├── Chart.yaml&#xA;      ├── templates/&#xA;      └── values-development.yaml&#xA;      └── values-staging.yaml&#xA;      └── values-production.yaml&#xA;      └── secrets-development.yaml&#xA;      └── secrets-staging.yaml&#xA;      └── secrets-production.yaml&#xA;      └── deploy-development.sh&#xA;      └── deploy-staging.sh&#xA;      └── deploy-production.sh&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            When we want to deploy the backend chart in the staging environment&#xA;            we can run:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./charts/backend/deploy-staging.sh&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            This works fine for small environments like in the example above, but&#xA;            for larger environments with 15 or 20 charts it will lead to a lot of&#xA;            similar-looking bash scripts with large amounts of code duplication.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Provisioning a new environment would mean that a new deploy script&#xA;            has to be created in each chart directory. If we have 15 charts&#xA;            that means we have to copy one of the existing deploy scripts 15 times&#xA;            and search/replace the contents to match the new environment name.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To avoid duplicating the same code over and over again we could consolidate&#xA;            all of our small deploy scripts into one large deploy script. But this comes&#xA;            with a cost: We have to spend time maintaining it, fixing bugs and possibly&#xA;            extend it to handle new environments.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            At this point Helmfile comes in handy. Instead of writing our custom deploy&#xA;            script we can declare our environments in a YAML file and let it handle the&#xA;            deployment logic for us.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Using a Helmfile&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Using the backend chart as an example we can write the following content&#xA;            into a &lt;code&gt;helmfile.yaml&lt;/code&gt; file to manage the staging deployment:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-staging.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-staging.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can deploy the chart by running:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            In the background Helmfile will run the same &lt;code&gt;helm upgrade --install ...&lt;/code&gt; command as before.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note that there&amp;#39;s no need to manually decrypt secrets anymore as Helmfile has built-in&#xA;            support for helm-secrets.&#xA;            This means that any file that is listed under &lt;code&gt;secrets:&lt;/code&gt; will automatically be decrypted&#xA;            and after the deployment is finished the decrypted file will automatically be removed.&#xA;        &lt;/p&gt;&#xA;&#xA;&#xA;        &lt;h2&gt;Environments&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            The above example uses the &lt;code&gt;values-staging.yaml&lt;/code&gt; file as chart&#xA;            values. To be able to use multiple environments we can list them under&#xA;            the &lt;code&gt;environments:&lt;/code&gt; key at the beginning of the helmfile&#xA;            and then use the environment name as a variable in the release definition.&#xA;            The file would now look like this:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-{{ .Environment.Name }}.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            When deploying the chart we now have to use the &lt;code&gt;--environment/-e&lt;/code&gt; option when&#xA;            executing the helmfile command:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging sync&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now easily create new environments by listing them under&#xA;            &lt;code&gt;environments&lt;/code&gt; instead of duplicating our bash scripts.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Templates&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            After adding all of our helm charts into the helmfile the file content&#xA;            would look like this:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;- name: frontend&#xA;  chart: charts/frontend&#xA;  values:&#xA;  - charts/frontend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/frontend/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;- name: prometheus&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;  values:&#xA;  - charts/prometheus/values-{{ .Environment.Name }}.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The same pattern (for values and secrets) is repeated for each release. While in the&#xA;            example above we only have 3 releases the pattern will continue for future additions and&#xA;            eventually lead to much duplicated code.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can avoid copy/pasting the release definitions by using Helmfile templates.&#xA;            A template is defined at the top of the file and then referenced in the release by using&#xA;            &lt;a href=&#34;https://confluence.atlassian.com/bitbucket/yaml-anchors-960154027.html&#34;&gt;YAML anchors&lt;/a&gt;.&#xA;            This is our helmfile after using templates:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;templates:&#xA;  default: &amp;amp;default&#xA;    chart: charts/{{`{{ .Release.Name }}`}}&#xA;    missingFileHandler: Warn&#xA;    values:&#xA;    - charts/{{`{{ .Release.Name }}`}}/values-{{ .Environment.Name }}.yaml&#xA;    secrets:&#xA;    - charts/{{`{{ .Release.Name }}`}}/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;releases:&#xA;- name: backend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: frontend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: prometheus&#xA;  &amp;lt;&amp;lt;: *default&#xA;  # override the defaults since it&amp;#39;s a remote chart&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            We have removed much of the duplicated code from our helmfile and can&#xA;            now easily add new environments and releases.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Helm Defaults&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We&amp;#39;ve previously used the the &lt;code&gt;--atomic&lt;/code&gt; and &lt;code&gt;--cleanup-on-fail&lt;/code&gt;&#xA;            options when deploying charts. To do the same when using Helmfile we just&#xA;            have to specify them under &lt;code&gt;helmDefaults&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;helmDefaults:&#xA;  atomic: true&#xA;  cleanupOnFail: true&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Running Helmfile Commands&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Here are a few examples of helmfile commands for common operations.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To install or upgrade all charts in an environment (using staging as an example)&#xA;            we run:&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            If we just want to sync (meaning to install/upgrade) a single chart we can use selectors. This command&#xA;            will sync the backend chart in the staging environment with our local values:&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging -l name=backend sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            To show the changes an operation would perform on a cluster without&#xA;            actually applying them we can run the following command (requires the&#xA;            &lt;a href=&#34;https://github.com/databus23/helm-diff&#34;&gt;helm-diff&lt;/a&gt; plugin):&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging -l name=prometheus diff&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Full Example Code&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            This is the final content of our helmfile.yaml file:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;helmDefaults:&#xA;  atomic: true&#xA;  cleanupOnFail: true&#xA;&#xA;templates:&#xA;  default: &amp;amp;default&#xA;    chart: charts/{{`{{ .Release.Name }}`}}&#xA;    missingFileHandler: Warn&#xA;    values:&#xA;    - charts/{{`{{ .Release.Name }}`}}/values-{{ .Environment.Name }}.yaml&#xA;    secrets:&#xA;    - charts/{{`{{ .Release.Name }}`}}/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;releases:&#xA;- name: backend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: frontend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: prometheus&#xA;  &amp;lt;&amp;lt;: *default&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;        The directory structure did not change and is the same as described at the top of the post.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/managing-helm-charts-with-helmfile/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Setting up Vim for YAML editing</title>
    <updated>2020-03-21T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-21:/setting-up-vim-for-yaml/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post I&amp;#39;m going to show how to set up Vim for easier YAML editing.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;full-example.png&#34; alt=&#34;Screenshot of Vim&#34;/&gt;&#xA;&#xA;        &lt;p&gt;You can scroll down to the end for a summary of all installed plugins and config file changes.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Syntax Highlighting&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s not much to do here. VIM has YAML syntax highlighting built-in and it&amp;#39;s great.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s one thing I want to mention though. A few years back YAML highlighting in Vim&#xA;            was very slow, and there was often a noticeable lag when opening large files.&#xA;            The workaround was to use the &lt;a href=&#34;https://github.com/stephpy/vim-yaml&#34;&gt;vim-yaml&lt;/a&gt; plugin for&#xA;            fast syntax highlighting.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Not sure if it&amp;#39;s still worth installing I decided to make a performance benchmark.&#xA;        I loaded up a &lt;a href=&#34;https://github.com/istio/istio/blob/master/manifests/base/files/gen-istio-cluster.yaml&#34;&gt;large YAML file&lt;/a&gt; (6100 lines) and compared the time:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# default syntax highlighting&lt;/span&gt;&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; vim gen-istio-cluster.yaml --startuptime default.log&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; tail -1 default.log&#xA;&lt;span class=&#34;out&#34;&gt;055.563&lt;/span&gt;&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# vim-yaml plugin&lt;/span&gt;&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; vim gen-istio-cluster.yaml --startuptime vimyaml.log&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; tail -1 vimyaml.log&#xA;&lt;span class=&#34;out&#34;&gt;060.320&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            As we can see the default syntax highlighting is just as fast as the plugin and there&amp;#39;s&#xA;            no need to install a separate plugin to fix the slow syntax highlighting anymore.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Indentation&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Indentation probably the most annoying part about editing YAML files.&#xA;            Large documents with deeply nested blocks are often hard to track and&#xA;            errors are easily made.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            YAML documents are required to have a 2 space indentation. However, Vim does not set this&#xA;            by default but it&amp;#39;s an easy fix by putting the following line in the vim config:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;autocmd FileType yaml setlocal ts=2 sts=2 sw=2 expandtab&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can also setup Indentation guides.&#xA;            Indentation guides are thin vertical lines at each indentation level and useful&#xA;            to help line up nested YAML blocks.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can display those lines by using the &lt;a href=&#34;https://github.com/Yggdroot/indentLine&#34;&gt;indentLine plugin&lt;/a&gt;.&#xA;            I&amp;#39;ve modified the indentation character to display a thinner line (default is &amp;#34;¦&amp;#34;):&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;let g:indentLine_char = &amp;#39;⦙&amp;#39;&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The result should look like this:&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;indentlines.png&#34; alt=&#34;Screenshot of Vim showing the indentLine plugin&#34;/&gt;&#xA;&#xA;        &lt;h2&gt;Folding&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            With folding we can hide parts of the file that are not relevant to our current task.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Vim has built-in support for folding based on the indentation level but the default&#xA;            folding rules make it hard to tell what is folded. This is because the folding starts&#xA;            on the line &lt;em&gt;following&lt;/em&gt; the start of a block. To change this we can install&#xA;            the &lt;a href=&#34;https://github.com/pedrohdz/vim-yaml-folds&#34;&gt;vim-yaml-folds&lt;/a&gt; plugin.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Here&amp;#39;s a side-by-side comparison of the default folding (left) compared to vim-yaml-folds (right):&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;folding-compare.png&#34; alt=&#34;comparison of default folding with vim-yaml-folds&#34;/&gt;&#xA;        &lt;p&gt;&#xA;            To work with folding we need to remember a few keyboard commands. Vimcasts has&#xA;            a great episode on this &lt;a href=&#34;http://vimcasts.org/episodes/how-to-fold/&#34;&gt;here&lt;/a&gt;.&#xA;            Most of the time I use the following commands:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;za: Toggle current fold&lt;/li&gt;&#xA;            &lt;li&gt;zR: Expand all folds&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            After the plugin is installed and folding is enabled the default settings will fold all&#xA;            blocks by default. To start with unfolded content we can set:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;set foldlevelstart=20&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s also a plugin called &lt;a href=&#34;https://www.vim.org/scripts/script.php?script_id=4021&#34;&gt;restore_view&lt;/a&gt;&#xA;            which will save the folds for each file. But be aware that this plugin will create an&#xA;            extra file with folding information for each opened document.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Linting&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Linting will analyze the code and show any potential errors while we&amp;#39;re writing it which helps&#xA;            us catch formatting or syntax errors early on.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this in Vim we can use &lt;a href=&#34;https://github.com/dense-analysis/ale&#34;&gt;ALE&lt;/a&gt;,&#xA;            an asynchronous linting framework that has support for many languages and tools including YAML.&#xA;            To enable YAML linting in ALE we have to install &lt;a href=&#34;https://github.com/adrienverge/yamllint&#34;&gt;yamllint&lt;/a&gt;,&#xA;            a Python-based YAML linter.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Installation instructions are &lt;a href=&#34;https://yamllint.readthedocs.io/en/stable/quickstart.html#installing-yamllint&#34;&gt;here&lt;/a&gt;.&#xA;            On macOS we can install it with Homebrew:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; brew install yamllint&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The default configuration is fairly strict and shows errors in document style such as&#xA;            line length, trailing spaces or comment indentation.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can modify the configuration to be less strict.&#xA;            Yamllint already comes with a &lt;a href=&#34;https://github.com/adrienverge/yamllint/blob/master/yamllint/conf/relaxed.yaml&#34;&gt;relaxed&lt;/a&gt;&#xA;            version of the default config that is a good starting point.&#xA;            The only additional thing I&amp;#39;ve decided to disable is line length checking.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this we open up &lt;code&gt;~/.config/yamllint/config&lt;/code&gt; and&#xA;            paste the following:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;extends: relaxed&#xA;&#xA;rules:&#xA;  line-length: disable&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;I&amp;#39;ve modified the ALE configuration to change the message format, error symbols and only lint&#xA;    when the file is saved:&lt;/p&gt;&#xA;&lt;pre&gt;let g:ale_echo_msg_format = &amp;#39;[%linter%] %s [%severity%]&amp;#39;&#xA;let g:ale_sign_error = &amp;#39;✘&amp;#39;&#xA;let g:ale_sign_warning = &amp;#39;⚠&amp;#39;&#xA;let g:ale_lint_on_text_changed = &amp;#39;never&amp;#39;&#xA;&lt;/pre&gt;&#xA;    &lt;p&gt;&#xA;        We can see the errors and warnings on the left side:&#xA;    &lt;/p&gt;&#xA;        &lt;img src=&#34;full-example.png&#34; alt=&#34;Screenshot of Vim&#34;/&gt;&#xA;&#xA;    &lt;h2&gt;Summary&lt;/h2&gt;&#xA;    &lt;p&gt;&#xA;        Here&amp;#39;s a summary of the plugins, applications and config modifications:&#xA;    &lt;/p&gt;&#xA;    &lt;h3&gt;Vim Plugins&lt;/h3&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/Yggdroot/indentLine&#34;&gt;indentLine&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/pedrohdz/vim-yaml-folds&#34;&gt;vim-yaml-folds&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/dense-analysis/ale&#34;&gt;ALE&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;h3&gt;Applicatins&lt;/h3&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/adrienverge/yamllint&#34;&gt;yamllint&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;h3&gt;Config&lt;/h3&gt;&#xA;    &lt;p&gt;In &lt;code&gt;~/.vimrc&lt;/code&gt; or &lt;code&gt;~/.config/nvim/init.vim&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;autocmd FileType yaml setlocal ts=2 sts=2 sw=2 expandtab&#xA;&#xA;set foldlevelstart=20&#xA;&#xA;let g:ale_echo_msg_format = &amp;#39;[%linter%] %s [%severity%]&amp;#39;&#xA;let g:ale_sign_error = &amp;#39;✘&amp;#39;&#xA;let g:ale_sign_warning = &amp;#39;⚠&amp;#39;&#xA;let g:ale_lint_on_text_changed = &amp;#39;never&amp;#39;&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;In &lt;code&gt;~/.config/yamllint/config&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;extends: relaxed&#xA;&#xA;rules:&#xA;  line-length: disable&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/setting-up-vim-for-yaml/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Private Helm Repo with GCS and GitHub Actions</title>
    <updated>2020-03-08T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-08:/private-helm-repo-with-gcs-and-github-actions/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;In this blog post I&amp;#39;m going to show how to setup a private Helm chart repository on&#xA;Google Cloud Storage (GCS) and use GitHub Actions to automatically push charts on new commits.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Setting up the GCS Bucket&lt;/h2&gt;&#xA;        &lt;p&gt;The first step is to create a GCS bucket that will hold our charts. We can do this over the CLI with the gcloud-sdk or over the Web UI. I&amp;#39;m going to use the CLI for the following examples.&lt;/p&gt;&lt;p&gt;To make it easier to handle access permissions we use the &lt;code&gt;-b on&lt;/code&gt; flag to enable uniform bucket-level access. It let&amp;#39;s us manage permissions on a bucket-level rather than on an object-level:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil mb -b on gs://my-chart-repo-arthurk&#xA;&lt;span class=&#34;out&#34;&gt;Creating gs://my-chart-repo-arthurk/...&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For Helm to be able to push charts to this bucket we need a Cloud IAM service account (with key) with &lt;em&gt;Storage Object Admin&lt;/em&gt; permissions:&lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud iam service-accounts create my-chart-repo-svc-acc&#xA;&lt;span class=&#34;out&#34;&gt;Created service account [my-chart-repo-svc-acc].&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud iam service-accounts keys create service-account.json --iam-account=my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com&#xA;&lt;span class=&#34;out&#34;&gt;created key [123123123] of type [json] as [service-account.json] for [my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com]&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil iam ch serviceAccount:my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com:roles/storage.objectAdmin gs://my-chart-repo-arthurk&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;When referring to the service account we have to use the email (not the name) which has the format &lt;code&gt;SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Setting up GitHub Actions&lt;/h2&gt;&#xA;        &lt;p&gt;In this step we&amp;#39;re going to setup GitHub Actions to detect charts that have changed and add them to our&#xA;            helm repo.&lt;/p&gt;&#xA;        &lt;p&gt;We start by creating the &lt;code&gt;.github/workflows/helm-ci.yml&lt;/code&gt; file and add:&lt;/p&gt;&#xA;        &lt;pre&gt;name: Helm Charts&#xA;on: [push]&#xA;&#xA;jobs:&#xA;  release:&#xA;    name: Release&#xA;    runs-on: ubuntu-latest&#xA;    steps:&#xA;      - name: Checkout&#xA;        uses: actions/checkout@v2&#xA;        with:&#xA;          fetch-depth: 2&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;By default the checkout action will clone the repo with a detached HEAD. To later compare files that have changed between the current HEAD and the previous commit we have to pass &lt;code&gt;fetch-depth: 2&lt;/code&gt; to the action.&lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            After pushing the code we can open GitHub Actions in the browser and check the workflow. It should look like this:&#xA;        &lt;/p&gt;&#xA;&#xA;            &lt;img src=&#34;gh-actions-1.png&#34; alt=&#34;GitHub Actions checkout step finished successfully&#34;/&gt;&#xA;&#xA;        &lt;h2&gt;Installing Helm and helm-gcs&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            The next step in the CI pipeline is to install Helm and the &lt;a href=&#34;https://github.com/hayorov/helm-gcs&#34;&gt;helm-gcs plugin&lt;/a&gt;. We add the following step to our workflow:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;- name: Install helm and plugins&#xA;  run: ./scripts/install.sh&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;and then create the &lt;code&gt;scripts/install.sh&lt;/code&gt; file with the following content:&lt;/p&gt;&#xA;        &lt;pre&gt;#!/usr/bin/env bash&#xA;&#xA;set -o errexit&#xA;&#xA;HELM_VERSION=3.1.1&#xA;HELM_GCS_VERSION=0.3.1&#xA;&#xA;echo &amp;#34;Installing Helm...&amp;#34;&#xA;wget -q https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz&#xA;tar -zxf helm-v${HELM_VERSION}-linux-amd64.tar.gz&#xA;sudo mv linux-amd64/helm /usr/local/bin/helm&#xA;helm version&#xA;&#xA;echo &amp;#34;Installing helm-gcs plugin...&amp;#34;&#xA;helm plugin install https://github.com/hayorov/helm-gcs --version ${HELM_GCS_VERSION}&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Set &lt;code&gt;chmod u+x scripts/install.sh&lt;/code&gt; and push the file. We can check GitHub Actions to make sure everything installed correctly:&lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;gh-actions-2.png&#34; alt=&#34;GitHub Actions showing that helm and helm-gcs have been installed successfully&#34;/&gt;&#xA;&#xA;        &lt;p&gt;This shows us that Helm 3.1.1 and helm-gcs 0.3.0 have been successfully installed&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Initializing the helm repository&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;We can now initialize the helm repository. For this to work we need to add our previously created&#xA;            service account key to GitHub. To do this we navigate to the repository and click on&#xA;            &lt;strong&gt;&amp;#34;Settings&amp;#34; → &amp;#34;Secrets&amp;#34; → &amp;#34;Add a new secret&amp;#34;&lt;/strong&gt;. There we set the name to be&#xA;            &lt;code&gt;GCLOUD_SERVICE_ACCOUNT_KEY&lt;/code&gt; and as a value add the content of the service-account.json file.&#xA;            After saving the secret it should look like this:&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;gh-actions-3.png&#34; alt=&#34;GitHub showing that a secret has been added to the project&#34;/&gt;&#xA;        &lt;p&gt;We can now modify the workflow to pass the secret as an environment variable to our next shell script:&lt;/p&gt;&#xA;&lt;pre&gt;- name: Release charts&#xA;  run: ./scripts/release.sh&#xA;  env:&#xA;    GCLOUD_SERVICE_ACCOUNT_KEY: ${{ secrets.GCLOUD_SERVICE_ACCOUNT_KEY }}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;In the release.sh script we save the service account to a file and point the&#xA;            &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt; environment variable to it. This is needed for helm-gcs&#xA;            plugin to authenticate. Afterwards we initialize the GCS repo which will create an&#xA;            empty &lt;code&gt;index.yaml&lt;/code&gt; file in the GCS bucket. Finally we can add the repo to helm&#xA;            so it can access its packages.&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;#!/usr/bin/env bash&#xA;&#xA;set -o errexit&#xA;set -o nounset&#xA;set -o pipefail&#xA;&#xA;GCS_BUCKET_NAME=&amp;#34;gs://my-chart-repo-arthurk&amp;#34;&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# setup service account for helm-gcs plugin&lt;/span&gt;&#xA;echo &amp;#34;${GCLOUD_SERVICE_ACCOUNT_KEY}&amp;#34; &amp;gt; svc-acc.json&#xA;export GOOGLE_APPLICATION_CREDENTIALS=svc-acc.json&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# initializing helm repo&lt;/span&gt;&#xA;&lt;span class=&#34;out&#34;&gt;# (only needed on first run but will do nothing if already exists)&lt;/span&gt;&#xA;echo &amp;#34;Initializing helm repo&amp;#34;&#xA;helm gcs init ${GCS_BUCKET_NAME}&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# add gcs bucket as helm repo&lt;/span&gt;&#xA;echo &amp;#34;Adding gcs bucket repo ${GCS_BUCKET_NAME}&amp;#34;&#xA;helm repo add private ${GCS_BUCKET_NAME}&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;Before committing the file make sure to mark it as executable with &lt;code&gt;chmod u+x scripts/release.sh&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Packaging and Pushing changed Charts&lt;/h2&gt;&#xA;        &lt;p&gt;In the final step of our CI script we need to identify which charts have changed and then&#xA;            package and push them to the helm repo. We do this by running &lt;code&gt;git diff&lt;/code&gt; on the previous&#xA;            revision with the following arguments:&lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&lt;code&gt;--find-renames&lt;/code&gt; detect if a file has been renamed&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;--diff-filter=d&lt;/code&gt; will ignore deleted files (we can&amp;#39;t package/push a deleted chart)&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;--name-only&lt;/code&gt; only print the name of the changed file&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;cut -d &amp;#39;/&amp;#39; -f 2 | uniq&lt;/code&gt; we only need unique directory names of files that have changed&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;We add the following content to the release.sh file:&lt;/p&gt;&#xA;&lt;pre&gt;prev_rev=$(git rev-parse HEAD^)&#xA;echo &amp;#34;Identifying changed charts since git rev ${prev_rev}&amp;#34;&#xA;&#xA;changed_charts=()&#xA;readarray -t changed_charts &amp;lt;&amp;lt;&amp;lt; &amp;#34;$(git diff --find-renames --diff-filter=d --name-only &amp;#34;$prev_rev&amp;#34; -- charts | cut -d &amp;#39;/&amp;#39; -f 2 | uniq)&amp;#34;&#xA;&#xA;if [[ -n &amp;#34;${changed_charts[*]}&amp;#34; ]]; then&#xA;    for chart in &amp;#34;${changed_charts[@]}&amp;#34;; do&#xA;        echo &amp;#34;Packaging chart &amp;#39;$chart&amp;#39;...&amp;#34;&#xA;        chart_file=$(helm package &amp;#34;charts/$chart&amp;#34; | awk &amp;#39;{print $NF}&amp;#39;)&#xA;&#xA;        echo &amp;#34;Pushing $chart_file...&amp;#34;&#xA;        helm gcs push &amp;#34;$chart_file&amp;#34; private&#xA;    done&#xA;else&#xA;    echo &amp;#34;No chart changes detected&amp;#34;&#xA;fi&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Commit and push the changes. After the CI run has finished, the GCS&#xA;            bucket will be initialized and have an &lt;code&gt;index.yaml&lt;/code&gt; file in it.&#xA;        This file is an index of all the helm charts in the repo. As we currently have no charts indexed it has&#xA;        the following content:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil cat gs://my-chart-repo-arthurk/index.yaml&#xA;&lt;span class=&#34;out&#34;&gt;apiVersion: v1&#xA;entries: {}&#xA;generated: &amp;#34;2020-03-08T06:51:49.496564824Z&amp;#34;&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Releasing our first chart&lt;/h2&gt;&#xA;        &lt;p&gt;We can now create and add the first chart to our helm repository. We do this by creating a &lt;em&gt;chart/&lt;/em&gt;&#xA;            directory and running &lt;code&gt;helm create&lt;/code&gt; to create an example chart:&#xA;&#xA;&lt;/p&gt;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; mkdir charts&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm create charts/foo&#xA;&lt;span class=&#34;out&#34;&gt;Creating charts/foo&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Add, commit and push all new files, then check GitHub Actions. It will show us that the chart&#xA;            was successfully packaged and pushed to the repo:&lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;gh-actions-4.png&#34; alt=&#34;GitHub Actions showing that the chart has been successfully added to the repo&#34;/&gt;&#xA;&#xA;        &lt;p&gt;Note that it&amp;#39;s not possible to push the same chart version to the same repo. The push will fail.&#xA;        We need to always make sure to increase the &lt;code&gt;version&lt;/code&gt; value in the &lt;code&gt;Chart.yaml&lt;/code&gt; file when releasing a new chart.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Trying it out&lt;/h2&gt;&#xA;        &lt;p&gt;To try out our private helm repo we can add it to helm on our client machine and list the repo contents:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm plugin install https://github.com/hayorov/helm-gcs&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud auth application-default login&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm repo add private-repo gs://my-chart-repo-arthurk&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm repo update&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm search repo private-repo -l&#xA;&lt;span class=&#34;out&#34;&gt;NAME            &#x9;CHART VERSION&#x9;APP VERSION&#x9;DESCRIPTION&#xA;private-repo/foo&#x9;0.1.0        &#x9;1.16.0     &#x9;A Helm chart for Kubernetes&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;As you can see the chart was successfully added to the registry. It can now be used as any other chart, for example by installing it with &lt;code&gt;helm install private-repo/foo --version 0.1.0&lt;/code&gt;.&lt;/p&gt;&#xA;        &lt;p&gt;The source code for all examples is available &lt;a href=&#34;https://github.com/arthurk/private-gcs-helm&#34;&gt;in this GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/private-helm-repo-with-gcs-and-github-actions/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Writing Reusable Helm Charts</title>
    <updated>2020-03-01T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-01:/writing-reusable-helm-charts/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm charts&lt;/a&gt; make it possible to easily package &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; manifests, version them and share them with&#xA;            other developers. To use a Helm chart across projects with different requirements it needs to be &lt;em&gt;reusable&lt;/em&gt;, meaning&#xA;            that common parts of the Kubernetes manifests can be changed in a values file without having to re-write the templates.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Let&amp;#39;s say we are looking into deploying Prometheus via Helm into our Kubernetes cluster. We search&#xA;            around and find a chart that is stable, well documented and actively maintained. It looks like a good choice.&#xA;            But there are a few options that you need to change in order to fit our requirements. Normally this could be done&#xA;            by creating a &lt;code&gt;values.yaml&lt;/code&gt; file and overriding the default settings. However, the chart that is available&#xA;            is not reusable enough and the options that we need to change are not available.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In such a case the only option for us is to copy the whole chart and modify it to fit the requirements even if the modification&#xA;            is only 1 or 2 lines of code. After copying the chart we also have to maintain it and keep it up to date with the upstream&#xA;            branch. It would&amp;#39;ve saved us a lot of time and work if the chart added a few options to make it reusable&#xA;            for projects that have different requirements.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In the next sections I&amp;#39;m going to go over the templates from the default Helm chart template (that is created when running&#xA;            &lt;code&gt;helm create&lt;/code&gt;) and explain what makes them reusable (and what can be improved).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Ingress&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;An Ingress allows external users to access Kubernetes Services. It provides a reverse-proxy,&#xA;        configurable traffic routing and TLS termination. There are several Ingress controllers available&#xA;        such as &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;nginx&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes/ingress-gce&#34;&gt;GCE&lt;/a&gt;, &lt;a href=&#34;https://github.com/containous/traefik&#34;&gt;Traefik&lt;/a&gt;, &lt;a href=&#34;https://github.com/haproxytech/kubernetes-ingress/&#34;&gt;HAProxy&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34;&gt;and more&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;For a reusable Ingress template we need to consider the following requirements:&lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Using the Ingress should be optional. Not every developer wants to expose their service to external users&lt;/li&gt;&#xA;            &lt;li&gt;It should be possible to choose an ingress controller such as nginx or GCE&lt;/li&gt;&#xA;            &lt;li&gt;Traffic routing should be configurable&lt;/li&gt;&#xA;            &lt;li&gt;TLS should be optional&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;a href=&#34;https://gist.github.com/arthurk/d872e92fabfca4f2e6af84662da10106&#34;&gt;default Ingress template&lt;/a&gt; meets all of&#xA;        our requirements and is a great example of a reusable template. The custom annotations are a very important part.&#xA;        A typical usage example of would look like this:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;ingress:&#xA;  enabled: true&#xA;  annotations:&#xA;    kubernetes.io/ingress.class: nginx&#xA;    nginx.ingress.kubernetes.io/server-snippet: |&#xA;      add_header X-Frame-Options &amp;#34;DENY&amp;#34;;&#xA;      proxy_set_header X-Frame-Options &amp;#34;DENY&amp;#34;;&#xA;    certmanager.k8s.io/cluster-issuer: letsencrypt&#xA;    certmanager.k8s.io/acme-challenge-type: dns01&#xA;    certmanager.k8s.io/acme-dns01-provider: cloudflare&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;We have the ingress enabled and use nginx as a controller. We specify a custom&#xA;            &lt;code&gt;server-snippet&lt;/code&gt; (used by the nginx-ingress to inject custom code into the server config) that adds a custom header&#xA;        (&lt;code&gt;X-Frame-Options&lt;/code&gt;). We use annotations to signal &lt;a href=&#34;cert-manager.readthedocs.io/&#34;&gt;cert-manager&lt;/a&gt; to&#xA;        provision a SSL certificate for this host.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Other ingress controllers such as &lt;a href=&#34;https://github.com/kubernetes/ingress-gce&#34;&gt;GCE&lt;/a&gt; also make use of annotations&#xA;        to integrate with Google Cloud services. In this example we assign an external static IP and provision an SSL certificate&#xA;        (with &lt;a href=&#34;http://github.com/GoogleCloudPlatform/gke-managed-certs&#34;&gt;gke-managed-certs&lt;/a&gt;):&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;ingress:&#xA;  enabled: true&#xA;  annotations:&#xA;    kubernetes.io/ingress.class: gce&#xA;    kubernetes.io/ingress.global-static-ip-name: my-external-ip&#xA;    kubernetes.io/ingress.allow-http: &amp;#39;false&amp;#39;&#xA;    networking.gke.io/managed-certificates: example-certificate&#xA;  hosts:&#xA;    - host: example.org&#xA;      paths:&#xA;        - &amp;#34;/*&amp;#34;&#xA;  tls:&#xA;    - hosts:&#xA;        - example.org&#xA;      secretName: &amp;#34;example-org-tls&amp;#34;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Service&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        A Service is an abstraction for a grouping of pods. It selects pods based on labels and&#xA;        allows network access to them. There are several Service types that Kubernetes supports such as&#xA;        ClusterIP, LoadBalancer or NodePort.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The requirements for a Service template in a reusable Helm chart are:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;          &lt;li&gt;It should be possible to pick a Service type. Not everyone wants to run an application behind a Load Balancer&lt;/li&gt;&#xA;          &lt;li&gt;It should be possible to add annotations&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            These are fairly simple requirements and the &lt;a href=&#34;https://gist.github.com/arthurk/e7bec72e9e7f4ea8785656f582846421&#34;&gt;default Service template&lt;/a&gt; meets all of them. A usage example looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;service:&#xA;  type: ClusterIP&#xA;  port: 80&#xA;  annotations:&#xA;    prometheus.io/scrape: &amp;#34;true&amp;#34;&#xA;    prometheus.io/port: &amp;#34;4000&amp;#34;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The service has a &lt;code&gt;ClusterIP&lt;/code&gt; Service type. In environments where higher availability is required&#xA;        this could be changed to a &lt;code&gt;LoadBalancer&lt;/code&gt;. The annotations are used by the Prometheus Helm chart:&#xA;        The prometheus server looks for all services in a cluster that have the &lt;code&gt;prometheus.io/scrape: &amp;#34;true&amp;#34;&lt;/code&gt;&#xA;        annotation and automatically scrapes them every minute.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Deployment&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        A Deployment is an abstraction for pods. It runs multiple replicas of an application and keeps them&#xA;        in the desired state. If an application fails or becomes unresponsive it will be replaced automatically.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;In a reusable Deployment template we should be able to:&lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Set the number of replicas&lt;/strong&gt;: Depending on the environment we should be able to adjust&#xA;                this value. A test environment doesn&amp;#39;t need to run as many replicas as a production environment.&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add Pod annotations&lt;/strong&gt;: Applications such as &lt;a href=&#34;https://linkerd.io/&#34;&gt;Linkerd&lt;/a&gt; use&#xA;                annotations to identify Pods into which to inject a sidecar container (&lt;code&gt;linkerd.io/inject: enabled&lt;/code&gt;)&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Pull the Docker image from a custom (private) registry&lt;/strong&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Modify arguments and environment variables&lt;/strong&gt;: As an example&#xA;                we should be able to pass &lt;code&gt;--log.level=debug&lt;/code&gt; to a container to see debug logs in case we have&#xA;                to identify problems with our application. Environment variables such as &lt;code&gt;MIX_ENV=prod&lt;/code&gt; often&#xA;                tell the application in which environment it is running and which configuration it should load&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add custom ConfigMaps and Secrets&lt;/strong&gt;: It should be possible to load application-specific&#xA;                configuration files or secrets that were added externally (for example SSL certificates for a database connection or API keys)&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add Liveness and Readiness probes&lt;/strong&gt; to check if the container is started and alive&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Configure container resource limits and requests&lt;/strong&gt;: In test or staging environments we should be able to&#xA;                disable it or set it to a low value&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Run Sidecar Containers&lt;/strong&gt;: If the application requires a database connection but the database is on CloudSQL it&#xA;                is often recommended to run &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloudsql-proxy/&#34;&gt;cloudsql-proxy&lt;/a&gt; as a&#xA;                sidecar container to establish a secure connection to the database&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Allow to set Affinity and Tolerations&lt;/strong&gt;: To optimize the performance of the application we should be able&#xA;                to run it on the same machine as certain other applications or have it scheduled on a specific node pool&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;Unlike the Ingress and Service templates, the &lt;a href=&#34;https://gist.github.com/arthurk/5f833ec5b264b84b6e1bedbd8eac69ea&#34;&gt;default template&lt;/a&gt; doesn&amp;#39;t meet the requirements from above. Specifically we need to allow to:&#xA;&#xA;&lt;/p&gt;&lt;ul&gt;&#xA;    &lt;li&gt;Add Pod annotations so that other applications such as Linkerd know where to inject sidecar containers&lt;/li&gt;&#xA;    &lt;li&gt;Replace &lt;code&gt;appVersion&lt;/code&gt; with &lt;code&gt;image.tag&lt;/code&gt;. This allows to change the docker image tag&#xA;        without having to re-package the chart with a different &lt;code&gt;appVersion&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;extraArgs&lt;/code&gt; to allow additional arguments to be passed into the container&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;env&lt;/code&gt; to allow additional environment variables to be passed into the container&lt;/li&gt;&#xA;    &lt;li&gt;Replace the default livenessProbe/readinessProbe with a block that allows us to set all values (the default&#xA;        template only allows &lt;code&gt;httpGet&lt;/code&gt; probes on &lt;code&gt;/&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;extraVolumes&lt;/code&gt; and &lt;code&gt;extraVolumeMounts&lt;/code&gt; to allow mounting of custom ConfigMaps and Secrets&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;sidecarContainers&lt;/code&gt; to allow to inject additional containers into the pod&#xA;        (such as &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloudsql-proxy/&#34;&gt;cloudsql-proxy&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The modified template code looks as follows (&lt;span class=&#34;diff_add&#34;&gt;green&lt;/span&gt; text marks added and changed code):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: {{ include &amp;#34;mychart.fullname&amp;#34; . }}&#xA;  labels:&#xA;    {{- include &amp;#34;mychart.labels&amp;#34; . | nindent 4 }}&#xA;spec:&#xA;  replicas: {{ .Values.replicaCount }}&#xA;  selector:&#xA;    matchLabels:&#xA;      {{- include &amp;#34;mychart.selectorLabels&amp;#34; . | nindent 6 }}&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        {{- include &amp;#34;mychart.selectorLabels&amp;#34; . | nindent 8 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;      annotations:&#xA;      {{- if .Values.podAnnotations }}&#xA;        {{- toYaml .Values.podAnnotations | nindent 8 }}&#xA;      {{- end }}&lt;/span&gt;&#xA;    spec:&#xA;    {{- with .Values.imagePullSecrets }}&#xA;      imagePullSecrets:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;      serviceAccountName: {{ include &amp;#34;mychart.serviceAccountName&amp;#34; . }}&#xA;      securityContext:&#xA;        {{- toYaml .Values.podSecurityContext | nindent 8 }}&#xA;      containers:&#xA;        - name: {{ .Chart.Name }}&#xA;          securityContext:&#xA;            {{- toYaml .Values.securityContext | nindent 12 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          image: &amp;#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}&amp;#34;&lt;/span&gt;&#xA;          imagePullPolicy: {{ .Values.image.pullPolicy }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          args:&#xA;          {{- range $key, $value := .Values.extraArgs }}&#xA;            - --{{ $key }}={{ $value }}&#xA;          {{- end }}&#xA;          {{- if .Values.env }}&#xA;          env:&#xA;            {{ toYaml .Values.env | nindent 12}}&#xA;          {{- end }}&lt;/span&gt;&#xA;          ports:&#xA;            - name: http&#xA;              containerPort: 80&#xA;              protocol: TCP&lt;span class=&#34;diff_add&#34;&gt;&#xA;          {{- with .Values.livenessProbe }}&#xA;          livenessProbe:&#xA;            {{- toYaml . | nindent 12 }}&#xA;          {{- end }}&#xA;          {{- with .Values.readinessProbe }}&#xA;          readinessProbe:&#xA;            {{- toYaml . | nindent 12 }}&#xA;          {{- end }}&lt;/span&gt;&#xA;          resources:&#xA;            {{- toYaml .Values.resources | nindent 12 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          volumeMounts:&#xA;          {{- if .Values.extraVolumeMounts }}&#xA;          {{ toYaml .Values.extraVolumeMounts | nindent 12 }}&#xA;          {{- end }}&#xA;       {{- if .Values.sidecarContainers }}&#xA;       {{- toYaml .Values.sidecarContainers | nindent 8 }}&#xA;       {{- end }}&#xA;      volumes:&#xA;      {{- if .Values.extraVolumes }}&#xA;      {{ toYaml .Values.extraVolumes | nindent 8}}&#xA;      {{- end }}&lt;/span&gt;&#xA;      {{- with .Values.nodeSelector }}&#xA;      nodeSelector:&#xA;        {{- toYaml . | nindent 8 }}&#xA;      {{- end }}&#xA;    {{- with .Values.affinity }}&#xA;      affinity:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;    {{- with .Values.tolerations }}&#xA;      tolerations:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;    &lt;p&gt;&#xA;    The default helm chart template is a great starting point for building reusable helm charts. The Ingress and Service&#xA;    templates are perfect examples. The Deployment template is lacking a few options to be reusable enough but can&#xA;    easily be modified and improved.&#xA;    &lt;/p&gt;&#xA;&#xA;    &lt;p&gt;&#xA;    For good examples of reusable Helm charts I recommend checking the&#xA;    &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable&#34;&gt;helm/charts stable repo&lt;/a&gt;.&#xA;    Charts such as &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus&#34;&gt;Prometheus&lt;/a&gt;, &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/grafana&#34;&gt;Grafana&lt;/a&gt; or &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/nginx-ingress&#34;&gt;nginx-ingress&lt;/a&gt; are actively maintained and constantly improved. They are good references to look at when writing a new Helm chart.&#xA;    &lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/writing-reusable-helm-charts/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Pre-Installed Daemons on Google Compute Engine</title>
    <updated>2020-02-23T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-23:/pre-installed-daemons-on-google-compute-engine/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;I found out that Google Compute Engine instances will come with the &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Google Guest Environment&lt;/a&gt; pre-installed which runs daemons in the background. This is unlike AWS EC2 instances which don&amp;#39;t install any daemons (but come with &lt;a href=&#34;https://github.com/aws/aws-cli&#34;&gt;aws-cli&lt;/a&gt; pre-installed). We can see the following output when listing the running processes on a new Debian GCE intance:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ ps ax | grep google&#xA;&#xA;418 ?        Ssl    2:52 /usr/bin/google_osconfig_agent&#xA;526 ?        Ss     2:17 /usr/bin/python3 /usr/bin/google_network_daemon&#xA;528 ?        Ss     3:32 /usr/bin/python3 /usr/bin/google_accounts_daemon&#xA;529 ?        Ss     1:14 /usr/bin/python3 /usr/bin/google_clock_skew_daemon&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can check all installed google packages:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ apt list --installed | grep google&#xA;&#xA;gce-disk-expand&#xA;google-cloud-packages-archive-keyring&#xA;google-cloud-sdk&#xA;google-compute-engine-oslogin&#xA;google-compute-engine&#xA;google-osconfig-agent&#xA;python-google-compute-engine&#xA;python3-google-compute-engine&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and systemd services:&lt;/p&gt;&#xA;&lt;pre&gt;$ systemctl list-unit-files | grep google&#xA;&#xA;google-accounts-daemon.service         enabled&#xA;google-clock-skew-daemon.service       enabled&#xA;google-instance-setup.service          enabled&#xA;google-network-daemon.service          enabled&#xA;google-osconfig-agent.service          enabled&#xA;google-shutdown-scripts.service        enabled&#xA;google-startup-scripts.service         enabled&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;These packages and services are part of the Google &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Linux Guest Environment&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/managing-instance-access&#34;&gt;OS Login Guest Environment&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The GCP docs have some information on the &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Guest Environment&lt;/a&gt; but it lacks details on the specifics of each daemon/script. A better source is the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/compute-image-packages/tree/master/packages/python-google-compute-engine&#34;&gt;GitHub repo&lt;/a&gt; where we can find a good explanation for each daemon and script:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-network-daemon:&lt;/strong&gt; handles network setup for multiple network interfaces on boot and integrates network load balancing with forwarding rule changes into the guest&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-accounts-daemon:&lt;/strong&gt; daemon to setup and manage user accounts, and to enable SSH key based authentication&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-clock-skew-daemon:&lt;/strong&gt; daemon to keep the system clock in sync after VM start and stop events&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-instance-setup:&lt;/strong&gt; scripts to execute VM configuration scripts during boot&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-startup-scripts/google-shutdown-scripts:&lt;/strong&gt; run user-provided scripts at VM startup and shutdown&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The remaining daemon is the agent for the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/guest-oslogin&#34;&gt;OS Login Guest Environment&lt;/a&gt;. It manages access control when using the &lt;a href=&#34;https://cloud.google.com/compute/docs/oslogin/&#34;&gt;OS Login&lt;/a&gt; feature by linking linux user accounts to Google accounts (which can then be managed with Cloud IAM). This feature is disabled by default and I&amp;#39;m not sure why the package is installed and the daemon is running.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Uninstall&lt;/h2&gt;&#xA;&lt;p&gt;If all that&amp;#39;s needed is a simple VM instance without Google Cloud integration, all daemons and scripts can be uninstalled by removing the packages:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ apt-get remove python-google-compute-engine python3-google-compute-engine \&#xA;                 google-osconfig-agent google-compute-engine-oslogin&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;I think it&amp;#39;s good to at least remove the &lt;code&gt;google-osconfig-agent&lt;/code&gt; package and get rid of the &lt;code&gt;google_osconfig_agent&lt;/code&gt; daemon running in the background. The package can be re-installed before enabling OS Login.&lt;/p&gt;&#xA;&lt;p&gt;Each daemon can also be disabled separately:&lt;/p&gt;&#xA;&lt;pre&gt;$ systemctl disable google-accounts-daemon.service&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/pre-installed-daemons-on-google-compute-engine/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Generating Ethereum Addresses in Python</title>
    <updated>2020-02-16T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-16:/generating-ethereum-addresses-in-python/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            I&amp;#39;ve been wondering how long it would take to generate all Ethereum private keys with addresses on my laptop.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            I know there is &lt;a href=&#34;https://bitcointalk.org/index.php?topic=7769.msg1010711#msg1010711&#34;&gt;not enough energy in our star system&lt;/a&gt;&#xA;            to do this in a reasonable timeframe, even on an imaginative computer that would use the absolute minimum of energy possible.&#xA;            This was more of a learning experience for me to get to know more about SHA-3 and KECCAK hashes,&#xA;            ECDSA curves, Public Keys and Ethereum addresses.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Due to its slow interpreter, Python is usually not a good choice when it comes to writing performant applications.&#xA;            The exception being Python modules which use an interface that calls C/C++ code. These modules are usually very fast,&#xA;            popular examples are &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&#34;https://numpy.org/&#34;&gt;Numpy&lt;/a&gt;.&#xA;            To generate Ethereum addresses we can use the following two Python modules which are both C based and have a good performance:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;https://github.com/ofek/coincurve/&#34;&gt;coincurve&lt;/a&gt;: Cross-platform Python CFFI bindings for libsecp256k1&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;https://github.com/tiran/pysha3&#34;&gt;pysha3&lt;/a&gt;: SHA-3 wrapper for Python (with support for keccak)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            Generating Ethereum addresses is a 3-step process:&#xA;        &lt;/p&gt;&#xA;        &lt;ol&gt;&#xA;            &lt;li&gt;Generate a private key&lt;/li&gt;&#xA;            &lt;li&gt;Derive the public key from the private key&lt;/li&gt;&#xA;            &lt;li&gt;Derive the Ethereum address from the public key&lt;/li&gt;&#xA;        &lt;/ol&gt;&#xA;        &lt;p&gt;&#xA;            Note that public keys and Ethereum addresses are not the same. Addresses are hashes of public keys.&#xA;            It&amp;#39;s not possible to send funds to a public key.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 1: Generate a private key&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Ethereum private keys are based on &lt;a href=&#34;https://keccak.team/keccak.html&#34;&gt;KECCAK-256 hashes&lt;/a&gt;. To generate such a hash we&#xA;            use the &lt;code&gt;keccak_256&lt;/code&gt; function from the pysha3 module on a random 32 byte seed:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;import secrets&#xA;from sha3 import keccak_256&#xA;&#xA;private_key = keccak_256(secrets.token_bytes(32)).digest()&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note that a KECCAK hash is not the same as a SHA-3 hash.&#xA;            KECCAK won a competition to become the SHA-3 standard but was slightly modified&#xA;            before it became standardized. Some SHA3 libraries such as pysha3 include&#xA;            the legacy KECCAK algorithm while others, such as the &lt;a href=&#34;https://docs.python.org/3.7/library/hashlib.html&#34;&gt;Python hashlib module&lt;/a&gt;,&#xA;            only implement the official SHA-3 standard.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 2: Derive the public key from the private key&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To get our public key we need to sign our private key with an Elliptic Curve Digital Signature Algorithm (ECDSA).&#xA;            Ethereum uses the &lt;a href=&#34;https://en.bitcoin.it/wiki/Secp256k1&#34;&gt;secp256k1 curve ECDSA&lt;/a&gt;. Coincurve uses this as a default&#xA;            so we don&amp;#39;t need to explicitly specify it when calling the function:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;from coincurve import PublicKey&#xA;&#xA;public_key = PublicKey.from_valid_secret(private_key).format(compressed=False)[1:]&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The &lt;a href=&#34;https://ethereum.github.io/yellowpaper/paper.pdf&#34;&gt;Ethereum Yellow Paper&lt;/a&gt; states that the public key has to be a byte array of size 64.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            By default coincurve uses the compressed format for public keys (libsecp256k1 was developed for Bitcoin where compressed keys are commonly used) which is 33 bytes in size.&#xA;            Uncompressed keys are 65 bytes in size. Additionally all public keys are&#xA;            prepended with a single byte to indicate if they are compressed or uncompressed.&#xA;            This means we first need to get the uncompressed 65 byte key (&lt;code&gt;compressed=False&lt;/code&gt;)&#xA;            and then strip the first byte (&lt;code&gt;[1:]&lt;/code&gt;) to get our 64 byte Ethereum public key.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 3: Derive the Ethereum address from the public key&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now generate our Ethereum address:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;addr = keccak_256(public_key).digest()[-20:]&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;As specified in the &lt;a href=&#34;https://ethereum.github.io/yellowpaper/paper.pdf&#34;&gt;Yellow Paper&lt;/a&gt; we take the right most 20 bytes of the 32 byte KECCAK hash of the corresponding ECDSA public key.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Full Example&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            This is the full example code from the above steps. It generates a random private key, derives the address&#xA;            and prints them in hex format:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;from secrets import token_bytes&#xA;from coincurve import PublicKey&#xA;from sha3 import keccak_256&#xA;&#xA;private_key = keccak_256(token_bytes(32)).digest()&#xA;public_key = PublicKey.from_valid_secret(private_key).format(compressed=False)[1:]&#xA;addr = keccak_256(public_key).digest()[-20:]&#xA;&#xA;print(&amp;#39;private_key:&amp;#39;, private_key.hex())&#xA;print(&amp;#39;eth addr: 0x&amp;#39; + addr.hex())&#xA;&#xA;### Output ###&#xA;# private_key: 7bf19806aa6d5b31d7b7ea9e833c202e51ff8ee6311df6a036f0261f216f09ef&#xA;# eth addr: 0x3db763bbbb1ac900eb2eb8b106218f85f9f64a13&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            I used the Python &lt;code&gt;timeit&lt;/code&gt; module to do &lt;a href=&#34;https://gist.github.com/arthurk/fbc876951379e2b0c889ea71b5167b4e&#34;&gt;a quick benchmark&lt;/a&gt; with the above code.&#xA;            The result is that my laptop can generate 18k addresses per second on a single cpu core.&#xA;            Using all 4 cpu cores that&amp;#39;s 72k addresses per second, ~6.2 billion (6.220.800.000) addresses per day or around&#xA;            two trillion (2.270.592.000.000) addresses per year.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Ethereum&amp;#39;s address space is 2^160. This means that by using this method it would take my laptop&#xA;            643665439999999976814879449351716864 (six hundred and forty-three decillion ...) years&#xA;            to generate all Ethereum private keys with addresses.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/generating-ethereum-addresses-in-python/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Validating Helm Chart Values with JSON Schemas</title>
    <updated>2020-02-08T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-08:/validate-helm-chart-values-with-json-schemas/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;Helm v3 &lt;a href=&#34;https://github.com/helm/helm/pull/5350&#34;&gt;added support&lt;/a&gt; to validate values in a chart&amp;#39;s &lt;code&gt;values.yaml&lt;/code&gt; file with &lt;a href=&#34;https://json-schema.org/&#34;&gt;JSON schemas&lt;/a&gt;. It allows us to do:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;Requirement checks. Example: An &lt;code&gt;API_KEY&lt;/code&gt; environment variable is set&lt;/li&gt;&#xA;    &lt;li&gt;Type validation. Example: The image tag is a string such as &lt;code&gt;&amp;#34;1.5&amp;#34;&lt;/code&gt; and not the number &lt;code&gt;1.5&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Range validation. Example: The value for a CPU utilization percentage key is between 1 and 100&lt;/li&gt;&#xA;    &lt;li&gt;Constraint Validation. Example: The &lt;code&gt;pullPolicy&lt;/code&gt; is &lt;code&gt;IfNotPresent&lt;/code&gt;, &lt;code&gt;Always&lt;/code&gt;, or &lt;code&gt;Never&lt;/code&gt;; A URL has the format &lt;code&gt;http(s)://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In this post I&amp;#39;m going to show how to create a JSON schema and use it to validate a chart&amp;#39;s &lt;code&gt;values.yaml&lt;/code&gt; file. After that I&amp;#39;m going to show how to automatically generate a schema from an existing values file.&lt;/p&gt;&lt;p&gt;&#xA;&#xA;&lt;/p&gt;&lt;h2&gt;Example&lt;/h2&gt;&#xA;&lt;p&gt;For this example I&amp;#39;m using the chart that is created when running &lt;code&gt;helm create mychart&lt;/code&gt;. We&amp;#39;ll create a JSON schema that will validate that the following conditions are met:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;code&gt;image.repository&lt;/code&gt; is a valid docker image name&lt;/li&gt;&#xA;    &lt;li&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt; is &lt;code&gt;IfNotPresent&lt;/code&gt;, &lt;code&gt;Always&lt;/code&gt; or &lt;code&gt;Never&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;The relevant part in the &lt;code&gt;values.yaml&lt;/code&gt; file is:&#xA;&#xA;&lt;pre&gt;image:&#xA;  repository: my-docker-image&#xA;  pullPolicy: IfNotPresent&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The JSON schema needs to be in a file named &lt;code&gt;values.schema.json&lt;/code&gt;. It has to be located in the same directory as the &lt;code&gt;values.yaml&lt;/code&gt; file. To match the requirements from above the file needs to have the following content:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;{&#xA;  &amp;#34;$schema&amp;#34;: &amp;#34;http://json-schema.org/schema#&amp;#34;,&#xA;  &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;,&#xA;  &amp;#34;required&amp;#34;: [&#xA;    &amp;#34;image&amp;#34;&#xA;  ],&#xA;  &amp;#34;properties&amp;#34;: {&#xA;    &amp;#34;image&amp;#34;: {&#xA;      &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;,&#xA;      &amp;#34;required&amp;#34;: [&#xA;        &amp;#34;repository&amp;#34;,&#xA;        &amp;#34;pullPolicy&amp;#34;&#xA;      ],&#xA;      &amp;#34;properties&amp;#34;: {&#xA;        &amp;#34;repository&amp;#34;: {&#xA;          &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,&#xA;          &amp;#34;pattern&amp;#34;: &amp;#34;^[a-z0-9-_]+$&amp;#34;&#xA;        },&#xA;        &amp;#34;pullPolicy&amp;#34;: {&#xA;          &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,&#xA;          &amp;#34;pattern&amp;#34;: &amp;#34;^(Always|Never|IfNotPresent)$&amp;#34;&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;Note that putting a key in the &lt;code&gt;required&lt;/code&gt; array does not mean that it has a value. In YAML if a key doesn&amp;#39;t have a value it will be set to an empty string. To make sure the value was set, a regex for the &lt;code&gt;pattern&lt;/code&gt; key has to be added that matches a non-empty string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To demonstrate that the validation is working I&amp;#39;m leaving the &lt;code&gt;repo&lt;/code&gt; empty and set &lt;code&gt;pullPolicy&lt;/code&gt; to an invalid value. Running lint shows the following output:&lt;/p&gt;&#xA;&lt;pre&gt;$ helm lint .&#xA;&#xA;==&amp;gt; Linting .&#xA;[ERROR] values.yaml: - image.repository: Invalid type. Expected: string, given: null&#xA;- image.pullPolicy: Does not match pattern &amp;#39;^(Always|Never|IfNotPresent)$&amp;#39;&#xA;&#xA;[ERROR] templates/: values don&amp;#39;t meet the specifications of the schema(s) in the following chart(s):&#xA;mychart:&#xA;- image.repository: Invalid type. Expected: string, given: null&#xA;- image.pullPolicy: Does not match pattern &amp;#39;^(Always|Never|IfNotPresent)$&amp;#39;&#xA;&#xA;Error: 1 chart(s) linted, 1 chart(s) failed&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The schema is automatically validated when running the following commands:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;helm install&lt;/li&gt;&#xA;    &lt;li&gt;helm upgrade&lt;/li&gt;&#xA;    &lt;li&gt;helm lint&lt;/li&gt;&#xA;    &lt;li&gt;helm template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The YAML values and the JSON schema need to be kept in sync manually. Helm will not check if keys from the YAML values file are missing from the schema. It will only validate fields that are specified in the schema.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating a JSON Schema for existing YAML values&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can infer a schema from existing YAML values and use it as a starting point when writing a new schema. The steps are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;    &lt;li&gt;Convert your values YAML file to JSON on &lt;a href=&#34;https://www.json2yaml.com/&#34;&gt;https://www.json2yaml.com/&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Paste the JSON on &lt;a href=&#34;https://www.jsonschema.net&#34;&gt;https://www.jsonschema.net/&lt;/a&gt; and click on &amp;#34;Infer Schema&amp;#34;&lt;/li&gt;&#xA;    &lt;li&gt;Paste the schema into the &lt;code&gt;values.schema.json&lt;/code&gt; file&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;We can run &lt;code&gt;helm lint&lt;/code&gt; to make sure the schema has been generated correctly:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ helm lint mychart/&#xA;&#xA;==&amp;gt; Linting .&#xA;&#xA;1 chart(s) linted, 0 chart(s) failed&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The inferred schema will mark all keys as required and set their type. A regex can be added to keys to make sure they have a value set. The &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;default&lt;/code&gt; and &lt;code&gt;examples&lt;/code&gt; fields are not necessary for validating helm charts and can be removed.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/validate-helm-chart-values-with-json-schemas/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
  <entry>
    <title>Extracting Data from Invoices with Google AutoML Natural Language</title>
    <updated>2020-02-02T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-02:/automl-invoice-data-extraction/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;    &lt;p&gt;In this tutorial I will show how to use &lt;a href=&#34;https://cloud.google.com/natural-language/automl/docs/&#34;&gt;Google AutoML Natural Language&lt;/a&gt; to setup a machine learning model that will automatically extract the total from invoices.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Why?&lt;/h2&gt;&#xA;    &lt;p&gt;Manually extracting data from invoices and entering them into an accounting system is time-consuming and tedious work.&lt;/p&gt;&#xA;    &lt;p&gt;To automate this there are template-based systems like &lt;a href=&#34;https://github.com/invoice-x/invoice2data&#34;&gt;invoice2data&lt;/a&gt; available. They extract the data using predefined extraction rules (regular expressions):&lt;/p&gt;&#xA;    &lt;pre&gt;Invoice Total: \$(\d+.\d{2})&lt;/pre&gt;&#xA;    &lt;p&gt;With such a system there&amp;#39;s still manual work required. YAML templates with extraction rules need to be written for each supplier and then maintained as invoice structures change over time. In the example above the supplier could decide to change &lt;code&gt;Invoice Total&lt;/code&gt; to &lt;code&gt;Total&lt;/code&gt; on the next invoice. The extraction would fail and the rule would have to be adjusted.&lt;/p&gt;&#xA;    &lt;p&gt;A better solution is to use a machine learning model that can extract the information without writing extraction rules. In this post I&amp;#39;m going to show how to setup such a model.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Steps To Do&lt;/h2&gt;&#xA;    &lt;p&gt;To build our invoice data extraction ML model we have to do the following steps:&lt;/p&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;Collect the training documents&lt;/li&gt;&#xA;        &lt;li&gt;Upload the documents to Google Cloud Storage&lt;/li&gt;&#xA;        &lt;li&gt;Create a CSV and JSONL file for AutoML to import the uploaded documents&lt;/li&gt;&#xA;        &lt;li&gt;Import the documents&lt;/li&gt;&#xA;        &lt;li&gt;Annotate the documents&lt;/li&gt;&#xA;        &lt;li&gt;Build/Train the model&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;p&gt;After that we can manually test the model by uploading invoices and checking how well it&amp;#39;s able to extract the data.&lt;/p&gt;&#xA;    &lt;p&gt;I&amp;#39;m using Google AutoML Natural Language which is part of Google Cloud Platform and a GCP account is required. Regarding cost there are two things to note: Google will give $300 free credit for new GCP accounts and $25 promotional credit for developers using AutoML for the first time.&lt;/p&gt;&#xA;    &lt;p&gt;Note that for the &lt;code&gt;gsutil&lt;/code&gt; commands below I installed the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud SDK&lt;/a&gt; but it&amp;#39;s not necessary to do so, the GCS operations from below can also be done over the Web UI.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Training Documents&lt;/h2&gt;&#xA;    &lt;p&gt;The first step is to collect training documents that are structured in the same way as the documents we want the model to handle later on. I was able to collect 150 invoices from different consultants. All invoices are text based PDF files with either 1 or 2 pages and the total written somewhere in the bottom right (single page invoices) or top-right (two-page invoices).&lt;/p&gt;&#xA;    &lt;p&gt;This is almost the minimum amount of training documents that is required by AutoML to work. Feel free to add more. The more training documents the better the model&amp;#39;s performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It&amp;#39;s important to use documents that have a similar structure. AutoML will use those documents to build the model by trying out different algorithms to find patterns. If those documents are structured differently it won&amp;#39;t be able to find any patterns and the model will have a poor performance.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Uploading Documents to GCS&lt;/h2&gt;&#xA;&lt;p&gt;All documents need to be stored in a Google Cloud Storage (GCS) bucket. AutoML doesn&amp;#39;t support other document sources. There are three important restrictions when creating the bucket:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Location Type&lt;/em&gt; has to be &lt;code&gt;Region&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Location&lt;/em&gt; has to be &lt;code&gt;us-central1 (Iowa)&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Storage Class&lt;/em&gt; has to be &lt;code&gt;Standard&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The bucket can be created by running:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil mb -l us-central1 gs://automl-nlp-example&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have all my training documents in a folder called &lt;code&gt;invoices/&lt;/code&gt; and named with the same pattern: &lt;code&gt;invoice-X&lt;/code&gt; where X is the number of the invoice (1 to 150). I recommend using the same pattern for all the invoice files as we will need to iterate over them later on. We can upload the folder with the invoices by running:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;gsutil -m cp -r invoices/ gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Importing the documents&lt;/h2&gt;&#xA;&lt;p&gt;For AutoML to be able to import the training documents we need to create a CSV file. The CSV file contains a link to a &lt;a href=&#34;http://jsonlines.org/&#34;&gt;JSONL&lt;/a&gt; document and the JSONL file then contains links to the actual invoice PDF files. The files are imported in the following way:&lt;/p&gt;&#xA;&lt;p&gt;AutoML -&amp;gt; CSV file → JSONL file → PDF file(s)&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating the CSV file&lt;/h2&gt;&#xA;&lt;p&gt;Creating the CSV file is simple and requires only one line:&lt;/p&gt;&lt;p&gt;&#xA;&lt;/p&gt;&lt;pre&gt;,gs://automl-nlp-example/data.jsonl&lt;/pre&gt;&#xA;&lt;p&gt;It&amp;#39;s important to have the comma at the beginning of the line which will make AutoML randomly assign the documents to different sets (TRAIN, VALIDATION, TEST). It will use:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;80% of the PDF documents for training the model&lt;/li&gt;&#xA;    &lt;li&gt;10% for validating the results during training&lt;/li&gt;&#xA;    &lt;li&gt;10% for verifying the model&amp;#39;s results after it has been trained&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We can upload the CSV file to GCS:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil cp data.csv gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Creating the JSONL File&lt;/h2&gt;&#xA;&lt;p&gt;The JSONL file contains links to the invoice PDF documents. Each line links to one PDF document and needs to have the following structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;{&amp;#34;document&amp;#34;: {&amp;#34;input_config&amp;#34;: {&amp;#34;gcs_source&amp;#34;: {&amp;#34;input_uris&amp;#34;: [ &amp;#34;gs://automl-nlp-example/invoice-1.pdf&amp;#34; ]}}}}&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We need to repeat this line for all documents and change the value of &lt;code&gt;input_uris&lt;/code&gt; to use the actual filename of the PDF file. I used a small Python script to generate the file (this is easy since my documents follow the same filename pattern) and upload it to GCS:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;python3 -c &amp;#39;for x in range(1, 151): print(&amp;#34;&amp;#34;&amp;#34;{&amp;#34;document&amp;#34;: {&amp;#34;input_config&amp;#34;: {&amp;#34;gcs_source&amp;#34;: {&amp;#34;input_uris&amp;#34;: [ &amp;#34;gs://automl-nlp-example/invoice-%s.pdf&amp;#34; ]}}}}&amp;#34;&amp;#34;&amp;#34; % x)&amp;#39; &amp;gt; data.jsonl&#xA;&#xA;gsutil cp data.jsonl gs://automl-nlp-example/&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now we should have the following files in the GCS bucket:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;The &lt;code&gt;invoice/&lt;/code&gt; directory contains all invoices as PDF files&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;code&gt;data.csv&lt;/code&gt; file contains a link to the JSONL file&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;code&gt;data.jsonl&lt;/code&gt; file contains links to the PDF files&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Creating the AutoML Dataset&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can start creating the dataset in the GCP console. Go to &lt;em&gt;Natual Language&lt;/em&gt; and then &lt;em&gt;AutoML Entity Extraction&lt;/em&gt; to create the dataset. The location has to be &lt;em&gt;Global&lt;/em&gt; and the model objective has to be &lt;em&gt;Entity Extraction&lt;/em&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;create-new-dataset.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The CSV file can be imported from the GCS bucket at the bottom of the screen:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;import-csv.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case the import process took &lt;em&gt;13 minutes&lt;/em&gt; to finish.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;import-done.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I created a &lt;code&gt;totalPrice&lt;/code&gt; label. We can do this in the bottom by clicking on &amp;#34;&lt;em&gt;Add New Label&lt;/em&gt;&amp;#34;. We will use that label in the next step to annotate the entity we want to extract.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Annotating the Documents&lt;/h2&gt;&#xA;&lt;p&gt;Annotating the documents is the most time-consuming part. We need to go through the following number of invoice documents in each set and mark the total in them:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;100 documents in the Training set&lt;/li&gt;&#xA;    &lt;li&gt;10 documents in the Validation set&lt;/li&gt;&#xA;    &lt;li&gt;10 documents in the Test set&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;This is the minimum number of annotations to make the model train. Feel free to annotate more documents if there&amp;#39;s time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next we click on a document and navigate to the &lt;em&gt;Structured Text&lt;/em&gt; view which. This will show the content of the PDF file and make it easy to annotate by simply selecting the text with the mouse and picking a label in the overlay popup. In the example below I would annotate the total: &lt;code&gt;5,032.50&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;annotating.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using this view AutoML will use the PDFs annotation&amp;#39;s position during training and learn to distinguish between entities based on the position of the annotation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After all the documents have been annotated we can switch to the &amp;#34;TRAIN&amp;#34; tab and start the training:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;pre-start-training.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case the training process took &lt;em&gt;2 hours and 18 minutes&lt;/em&gt; to finish.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Manual Testing&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;After we trained the model we can try it out by uploading an invoice to GCS:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil cp testing-invoice.pdf gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then selecting it in the &amp;#34;TEST &amp;amp; USE&amp;#34; tab and clicking on the &amp;#34;PREDICT&amp;#34; button:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;test-and-use.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The prediction will only take a second and shows the result in a PDF view:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;prediction-results.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the example above we can see that the &lt;code&gt;totalPrice&lt;/code&gt; was successfully extracted. Feel free to try it out with other invoices. Multiple documents can be submitted by using the &lt;a href=&#34;https://cloud.google.com/natural-language/automl/docs/predict#batch_prediction&#34;&gt;Batch prediction REST API&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I found Google AutoML Natural Language easy to use. Most of the tasks (except creating the CSV and JSONL files) can be done in the Web UI and the whole process doesn&amp;#39;t require any coding experience. I only had a very small training dataset available but the results are good enough. In my manual tests I was able to extract the total in around 80% of the cases. I&amp;#39;m sure this could be improved by making a higher quality training dataset. The downside is that the price for AutoML is high. It cost me around $25 to train, test and deploy this model.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/automl-invoice-data-extraction/" rel="alternate"></link>
    <summary type="html"></summary>
  </entry>
</feed>
