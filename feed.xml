<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>Arthur Koziel</title>
  <id>https://www.arthurkoziel.com/</id>
  <updated>2021-05-12T14:36:47+08:00</updated>
  <subtitle>Recent Blog Posts</subtitle>
  <link href="https://www.arthurkoziel.com/"></link>
  <entry>
    <title>Running virt-manager and libvirt on macOS</title>
    <updated>2021-05-12T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2021-05-12:/running-virt-manager-and-libvirt-on-macos/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;I’ve previously written about &lt;a href=&#34;https://www.arthurkoziel.com/qemu-ubuntu-20-04/&#34;&gt;using QEMU on macOS to create an Ubuntu VM&lt;/a&gt; via CLI. In this blog post I’m going to describe how to install &lt;a href=&#34;https://libvirt.org/&#34;&gt;libvirt&lt;/a&gt; and &lt;a href=&#34;https://virt-manager.org/&#34;&gt;virt-manager&lt;/a&gt; on macOS to create an &lt;a href=&#34;https://ubuntu.com/&#34;&gt;Ubuntu&lt;/a&gt; VM via &lt;a href=&#34;https://www.qemu.org/&#34;&gt;QEMU&lt;/a&gt; from the virt-manager GUI.&lt;/p&gt;&#xA;&lt;p&gt;What’s described in this blog post was more of an experiment to see if it would work. Running libvirt locally is very slow and not usable due to the missing support for the HVF &lt;a href=&#34;https://developer.apple.com/documentation/hypervisor&#34;&gt;Hypervisor.Framework&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;If you’re only interested in running virt-manager on macOS (and connect to remote machines) you can skip the steps after the virt-manager installation.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/running-virt-manager-and-libvirt-on-macos/img/ubuntu-desktop.png&#34; alt=&#34;Ubuntu Desktop&#34;/&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;&#xA;&lt;p&gt;Libvirt uses a hypervisor (such as QEMU) to run a VM and provides an API to manage it. API Clients are available for many languages such as Python, Go or Rust.&lt;/p&gt;&#xA;&lt;p&gt;Virt-manager is a Python application that provides a GUI to manage VMs though the libvirt API.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Libvirt is available in Homebrew and the installation can be done with a single command:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;brew install libvirt&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Virt-manager is not available in Homebrew but there’s a &lt;a href=&#34;https://github.com/jeffreywildman/homebrew-virt-manager&#34;&gt;custom formula&lt;/a&gt; available that makes it convenient to install it. However, this formula is outdated and fails to run on macOS Catalina and Big Sur.&lt;/p&gt;&#xA;&lt;p&gt;To fix this I’ve &lt;a href=&#34;https://github.com/arthurk/homebrew-virt-manager&#34;&gt;created a fork&lt;/a&gt; with updated dependencies. It can be installed via a custom tap:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;brew tap arthurk/homebrew-virt-manager&#xA;brew install virt-manager virt-viewer&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The installation might take a few minutes due to many dependencies.&lt;/p&gt;&#xA;&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;&#xA;&lt;p&gt;When the installation has finished we can test it by starting the libvirtd daemon:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;brew services start libvirt&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and then virt-manager with a connection to it:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;virt-manager -c &amp;#34;qemu:///session&amp;#34; --no-fork&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The original Homebrew formula had a custom patch applied that made &lt;code&gt;--no-fork&lt;/code&gt; the default behaviour (launching it in the foreground). To make it easier to maintain the formula I’ve removed the patch. If the process should run in the foreground the &lt;code&gt;--no-fork&lt;/code&gt; argument needs to be specified or else it will run in the background.&lt;/p&gt;&#xA;&lt;p&gt;Note that the virt-manager window will be hidden after starting. It will show up in the Dock (the icon is a rocket) and clicking on it will bring it into the foreground.&lt;/p&gt;&#xA;&lt;h2 id=&#34;creating-a-vm&#34;&gt;Creating a VM&lt;/h2&gt;&#xA;&lt;p&gt;As an example I’m creating a Ubuntu 20.10 VM.&lt;/p&gt;&#xA;&lt;p&gt;Virt-manager assumes that &lt;a href=&#34;https://www.spice-space.org/&#34;&gt;SPICE&lt;/a&gt; is available and will add it to the default settings. However, it’s is not supported on macOS and therefore we need to change a the default values to remove all SPICE related settings:&lt;/p&gt;&#xA;&lt;p&gt;Make sure to check the customize box before starting the VM:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/running-virt-manager-and-libvirt-on-macos/img/customize.png&#34; alt=&#34;virt-manager window before creating the VM, showing the customize checkbox&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;In the customization window we need to remove all hardware related to SPICE (Right-Click -&amp;gt; Remove Hardware):&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Channel spice&lt;/li&gt;&#xA;&lt;li&gt;USB Redirector 1&lt;/li&gt;&#xA;&lt;li&gt;USB Redirector 2&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/running-virt-manager-and-libvirt-on-macos/img/settings-remove-hardware.png&#34; alt=&#34;virt-manager settings with annotation about which hardware to remove&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;In the &lt;strong&gt;Display&lt;/strong&gt; section set the type to VNC server:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/running-virt-manager-and-libvirt-on-macos/img/settings-display.png&#34; alt=&#34;virt-manager settings showing the display section&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;In the &lt;strong&gt;Video&lt;/strong&gt; section set the model to Virtio:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/running-virt-manager-and-libvirt-on-macos/img/settings-video.png&#34; alt=&#34;virt-manager settings showing the video section&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;Apply these changes and click on &lt;em&gt;Begin Installation&lt;/em&gt; in the top left corner. It will open a window that boots into the Ubuntu installer. From there on we can follow the installer. The default settings are usually fine.&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;As mentioned above this was an experiment. The VM runs very slow due to the missing support for HVF (Hypervisor.Framework). The focus of libvirt is on KVM/Linux hosts. I wouldn’t use it on a daily basis on macOS and instead stick to the QEMU CLI with HVF set as the accelerator. You can check my &lt;a href=&#34;https://www.arthurkoziel.com/qemu-ubuntu-20-04/&#34;&gt;previous blog post&lt;/a&gt; on how to do that.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/running-virt-manager-and-libvirt-on-macos/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Tutorial: Tekton Triggers with GitHub integration</title>
    <updated>2021-05-01T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2021-05-01:/tutorial-tekton-triggers-with-github-integration/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;In a &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/&#34;&gt;previous blog post&lt;/a&gt; we’ve used &lt;a href=&#34;https://github.com/tektoncd/pipeline&#34;&gt;Tekton Pipelines&lt;/a&gt; to set up a simple pipeline that runs tests, builds a docker image and pushes it to a registry.&lt;/p&gt;&#xA;&lt;p&gt;In this blog post we’re going to take a look at &lt;a href=&#34;https://github.com/tektoncd/triggers&#34;&gt;Tekton Triggers&lt;/a&gt; and integrate it with GitHub. We’re going to setup a GitHub webhook that will automatically run our pipeline when a GitHub PR is opened or new commits are pushed to an existing PR branch.&lt;/p&gt;&#xA;&lt;p&gt;All code examples in this blog post are available in a &lt;a href=&#34;https://github.com/arthurk/tekton-triggers-example&#34;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#creating-a-pipeline&#34;&gt;Creating a Pipeline&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#creating-resources-for-tekton-triggers&#34;&gt;Creating Resources for Tekton Triggers&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#eventlistener&#34;&gt;EventListener&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#github-interceptor&#34;&gt;GitHub Interceptor&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cel-interceptor&#34;&gt;CEL Interceptor&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#triggerbinding&#34;&gt;TriggerBinding&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#triggertemplate&#34;&gt;TriggerTemplate&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#ingress&#34;&gt;Ingress&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#adding-the-webhook-to-github&#34;&gt;Adding the webhook to Github&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#creating-a-pr-and-testing-our-trigger&#34;&gt;Creating a PR and testing our trigger&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;&#xA;&lt;p&gt;For this tutorial we need a Kubernetes cluster with an ingress-controller installed that can give us an external IP.&lt;/p&gt;&#xA;&lt;p&gt;We also need a GitHub repository where we can add the webhook.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Tekton Triggers requires Tekton Pipelines to be installed. We also need to install the core interceptors (GitHub, GitLab, BitBucket, and CEL) manifests as we’ll use them later on.&lt;/p&gt;&#xA;&lt;p&gt;By default all resources will be installed in the &lt;code&gt;tekton-pipelines&lt;/code&gt; namespace.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# Tekton Pipelines&#xA;kubectl apply -f https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.23.0/release.yaml&#xA;&#xA;# Tekton Triggers + Interceptors&#xA;kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/previous/v0.13.0/release.yaml&#xA;kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/previous/v0.13.0/interceptors.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Configure RBAC for our Tekton Triggers service account:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/arthurk/tekton-triggers-example/master/01-rbac.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;creating-a-pipeline&#34;&gt;Creating a Pipeline&lt;/h2&gt;&#xA;&lt;p&gt;We’re going to use a simple pipeline that clones a Git repo with a Go application in it and runs the tests. The example application can be found at &lt;a href=&#34;https://github.com/arthurk/go-example-app&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-triggers-example/blob/master/02-pipeline.yaml&#34;&gt;pipeline.yaml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Pipeline&#xA;metadata:&#xA;  name: github-pr-pipeline&#xA;spec:&#xA;  resources:&#xA;    - name: source&#xA;      type: git&#xA;  tasks:&#xA;    - name: test&#xA;      taskRef:&#xA;        name: test&#xA;      resources:&#xA;        inputs:&#xA;          - name: source&#xA;            resource: source&#xA;---&#xA;apiVersion: tekton.dev/v1beta1&#xA;kind: Task&#xA;metadata:&#xA;  name: test&#xA;spec:&#xA;  resources:&#xA;    inputs:&#xA;      - name: source&#xA;        type: git&#xA;  steps:&#xA;    - name: run-test&#xA;      image: golang:1.16.3-alpine3.13&#xA;      workingDir: /workspace/source&#xA;      command: [&amp;#34;go&amp;#34;]&#xA;      args: [&amp;#34;test&amp;#34;]&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;creating-resources-for-tekton-triggers&#34;&gt;Creating Resources for Tekton Triggers&lt;/h2&gt;&#xA;&lt;p&gt;For our project we need to create the following resources:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;EventListener&lt;/code&gt;: A Kubernetes Service that listens for incoming HTTP requests and executes a Trigger.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Trigger&lt;/code&gt;: Decides what to do with the received event. Sets a TriggerBinding, TriggerTemplate and Interceptor to run.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;TriggerBinding&lt;/code&gt;: Specifies the data to be extracted from the request and saved as parameters. This data will be passed to the TriggerTemplate.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;TriggerTemplate&lt;/code&gt;: A template of a resource (TaskRun/PipelineRun) to be created when an event is received.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Interceptor&lt;/code&gt;: Processes an event to do custom validation or filtering&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;eventlistener&#34;&gt;EventListener&lt;/h3&gt;&#xA;&lt;p&gt;An &lt;code&gt;EventListener&lt;/code&gt; processes an incoming request and executes a &lt;code&gt;Trigger&lt;/code&gt;. Our EventListener looks like this:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-triggers-example/blob/master/03-eventlistener.yaml&#34;&gt;eventlistener.yaml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: triggers.tekton.dev/v1alpha1&#xA;kind: EventListener&#xA;metadata:&#xA;  name: github-pr&#xA;spec:&#xA;  serviceAccountName: tekton-triggers-example-sa&#xA;  triggers:&#xA;    - name: github-listener&#xA;      interceptors:&#xA;        - ref:&#xA;            name: &amp;#34;github&amp;#34;&#xA;          params:&#xA;            - name: &amp;#34;secretRef&amp;#34;&#xA;              value:&#xA;                secretName: github-interceptor-secret&#xA;                secretKey: secretToken&#xA;            - name: &amp;#34;eventTypes&amp;#34;&#xA;              value: [&amp;#34;pull_request&amp;#34;]&#xA;        - ref:&#xA;            name: &amp;#34;cel&amp;#34;&#xA;          params:&#xA;            - name: &amp;#34;filter&amp;#34;&#xA;              value: &amp;#34;body.action in [&amp;#39;opened&amp;#39;, &amp;#39;synchronize&amp;#39;, &amp;#39;reopened&amp;#39;]&amp;#34;&#xA;      bindings:&#xA;        - ref: github-pr-binding&#xA;      template:&#xA;        ref: github-pr-pipeline-template&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After receiving the incoming request it will execute the &lt;code&gt;github-listener&lt;/code&gt; Trigger. In this case the Trigger is embedded into the EventListener resource rather than specified in a separate resource.&lt;/p&gt;&#xA;&lt;p&gt;Our github-listener trigger will then make use of &lt;code&gt;Interceptor&lt;/code&gt;’s. An Interceptor let’s us validate or modify incoming requests before they trigger a pipeline run. We embed the Interceptor resource rather than putting it into a separate resource manifest.&lt;/p&gt;&#xA;&lt;h3 id=&#34;github-interceptor&#34;&gt;GitHub Interceptor&lt;/h3&gt;&#xA;&lt;p&gt;The first interceptor we’re running is called &lt;code&gt;github&lt;/code&gt;. It’s part of the core interceptors that we installed above. It makes sure that the request:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;has a valid format for GitHub webhooks&lt;/li&gt;&#xA;&lt;li&gt;matches a pre-defined secret (that we’ll set later)&lt;/li&gt;&#xA;&lt;li&gt;matches the &lt;code&gt;pull_request&lt;/code&gt; event type&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The github interceptor requires a secret token. This token is set when creating the webhook in GitHub and will be validated by the github interceptor when the request arrives:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-triggers-example/blob/master/04-secret.yaml&#34;&gt;secret.yaml&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Secret&#xA;metadata:&#xA;  name: github-interceptor-secret&#xA;type: Opaque&#xA;stringData:&#xA;  secretToken: &amp;#34;1234567&amp;#34;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;cel-interceptor&#34;&gt;CEL Interceptor&lt;/h3&gt;&#xA;&lt;p&gt;The second interceptor we’re using is called &lt;code&gt;cel&lt;/code&gt; and is also included in the core interceptor manifests that we installed above. Interceptors are executed in the order they’re specified. The cel interceptor will run after the github interceptor.&lt;/p&gt;&#xA;&lt;p&gt;It let’s us specify a &lt;a href=&#34;https://github.com/google/cel-go&#34;&gt;CEL&lt;/a&gt; filter expression that will be applied to requests.&lt;/p&gt;&#xA;&lt;p&gt;We’ll apply this filter expression because GitHub &lt;code&gt;pull_request&lt;/code&gt; events are sent for every action performed on a pull request (&lt;code&gt;assigned&lt;/code&gt;, &lt;code&gt;unassigned&lt;/code&gt;, &lt;code&gt;converted_to_draft&lt;/code&gt;, …).&lt;/p&gt;&#xA;&lt;p&gt;For this tutorial we only need to know when a PR was &lt;code&gt;opened&lt;/code&gt;, &lt;code&gt;reopened&lt;/code&gt; or &lt;code&gt;synchronized&lt;/code&gt; (commits pushed to the PR branch). The CEL filter checks the webhook request &lt;code&gt;body.action&lt;/code&gt; and filters our any events that don’t match those actions.&lt;/p&gt;&#xA;&lt;h3 id=&#34;triggerbinding&#34;&gt;TriggerBinding&lt;/h3&gt;&#xA;&lt;p&gt;After the event listener is done validating and modifying the incoming request, we need to extract values from it and bind them to variables that we can later use in our Pipeline. This is what a &lt;code&gt;TriggerBinding&lt;/code&gt; is used for.&lt;/p&gt;&#xA;&lt;p&gt;Our TriggerBinding looks like this:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-triggers-example/blob/master/05-triggerbinding.yaml&#34;&gt;triggerbinding.yaml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: triggers.tekton.dev/v1alpha1&#xA;kind: TriggerBinding&#xA;metadata:&#xA;  name: github-pr-binding&#xA;spec:&#xA;  params:&#xA;    - name: gitrepositoryurl&#xA;      value: $(body.repository.clone_url)&#xA;    - name: gitrevision&#xA;      value: $(body.pull_request.head.sha)&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;We’re only interested in the followinw two fields which are necessary to clone the repo at a specific revision:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;gitrepositoryurl: the url of the repo to clone (https://…)&lt;/li&gt;&#xA;&lt;li&gt;gitrevision: the commit SHA to check out&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The parameters are passed to a &lt;code&gt;TriggerTemplate&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;triggertemplate&#34;&gt;TriggerTemplate&lt;/h3&gt;&#xA;&lt;p&gt;A TriggerTemplate is responsible for dynamically generating a resource. In our case it’s a &lt;code&gt;PipelineRun&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The TriggerTemplate receives the two variables from the previously created TriggerBinding and makes them available under &lt;code&gt;spec.resourcetemplates&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-triggers-example/blob/master/06-triggertemplate.yaml&#34;&gt;triggertemplate.yaml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: triggers.tekton.dev/v1alpha1&#xA;kind: TriggerTemplate&#xA;metadata:&#xA;  name: github-pr-pipeline-template&#xA;spec:&#xA;  params:&#xA;    - name: gitrevision&#xA;      description: The git revision (SHA)&#xA;      default: master&#xA;    - name: gitrepositoryurl&#xA;      description: The git repository url (&amp;#34;https://github.com/foo/bar.git&amp;#34;)&#xA;  resourcetemplates:&#xA;    - apiVersion: tekton.dev/v1beta1&#xA;      kind: PipelineRun&#xA;      metadata:&#xA;        generateName: github-pr-pipeline-run-&#xA;      spec:&#xA;        pipelineRef:&#xA;          name: github-pr-pipeline&#xA;        resources:&#xA;          - name: source&#xA;            resourceSpec:&#xA;              type: git&#xA;              params:&#xA;                - name: revision&#xA;                  value: $(tt.params.gitrevision)&#xA;                - name: url&#xA;                  value: $(tt.params.gitrepositoryurl)&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Note that to access variables from a triggertemplate inside a resourcetemplate they need to be prefixed with &lt;code&gt;$tt&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;ingress&#34;&gt;Ingress&lt;/h3&gt;&#xA;&lt;p&gt;For GitHub to be able to send a request to our event listener we need to expose it by creating an Ingress resource and pointing it to our event listener service:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-triggers-example/blob/master/07-ingress.yaml&#34;&gt;ingress.yaml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: networking.k8s.io/v1&#xA;kind: Ingress&#xA;metadata:&#xA;  name: ingress-resource&#xA;  annotations:&#xA;    kubernetes.io/ingress.class: nginx&#xA;    nginx.ingress.kubernetes.io/ssl-redirect: &amp;#34;false&amp;#34;&#xA;spec:&#xA;  rules:&#xA;    - http:&#xA;        paths:&#xA;          - path: /hooks&#xA;            pathType: Exact&#xA;            backend:&#xA;              service:&#xA;                name: el-github-pr&#xA;                port:&#xA;                  number: 8080&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;An EventListener will create a service with the &lt;code&gt;el-&lt;/code&gt; prefix followed by the name of the event-listener. Our event-listener is named &lt;code&gt;github-pr&lt;/code&gt;, so the EventListener Service is named &lt;code&gt;el-github-pr&lt;/code&gt;. EventListener services will always use port 8080.&lt;/p&gt;&#xA;&lt;p&gt;Make sure to note the external IP address of your ingress. In this example it’s &lt;code&gt;123.123.1.1&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl get ingress&#xA;NAME               CLASS    HOSTS   ADDRESS        PORTS   AGE&#xA;ingress-resource   &amp;lt;none&amp;gt;   *       123.123.1.1    80      26d&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;We can now send GitHub webhook requests to &lt;code&gt;http://123.123.1.1/hooks&lt;/code&gt; and automatically trigger a PipelineRun.&lt;/p&gt;&#xA;&lt;h2 id=&#34;adding-the-webhook-to-github&#34;&gt;Adding the webhook to Github&lt;/h2&gt;&#xA;&lt;p&gt;In your GitHub repo go to &lt;code&gt;Settings -&amp;gt; Webhooks&lt;/code&gt; and click &lt;code&gt;Add Webhook&lt;/code&gt;. The fields we need to set are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Payload URL: Your external IP Address from the Ingress with &lt;code&gt;/hooks&lt;/code&gt; path&lt;/li&gt;&#xA;&lt;li&gt;Content type: &lt;code&gt;application/json&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Secret: &lt;code&gt;1234567&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Under events select &lt;code&gt;Let me select individual events&lt;/code&gt;. Uncheck &lt;code&gt;Pushes&lt;/code&gt; and check &lt;code&gt;Pull requests&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/tutorial-tekton-triggers-with-github-integration/add-webhook.png&#34; alt=&#34;Picture of GitHub Webhook settings&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;After saving the webhook, GitHub will send a ping event. It will be filtered out by our Interceptor which only allows &lt;code&gt;pull_request&lt;/code&gt; events, but we can check the EventListener Pod logs to verify it:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl get pods | grep el-github-pr&#xA;el-github-pr-7df7f66d57-kqd6k     1/1     Running   0          13s&#xA;&#xA;kubectl logs el-github-pr-7df7f66d57-kqd6k&#xA;...&#xA;{&#xA;  &amp;#34;level&amp;#34;: &amp;#34;info&amp;#34;,&#xA;  &amp;#34;ts&amp;#34;: &amp;#34;2021-05-01T08:48:59.461Z&amp;#34;,&#xA;  &amp;#34;logger&amp;#34;: &amp;#34;eventlistener&amp;#34;,&#xA;  &amp;#34;caller&amp;#34;: &amp;#34;sink/sink.go:240&amp;#34;,&#xA;  &amp;#34;msg&amp;#34;: &amp;#34;interceptor stopped trigger processing: rpc error: code = FailedPrecondition desc = event type ping is not allowed&amp;#34;,&#xA;  &amp;#34;knative.dev/controller&amp;#34;: &amp;#34;eventlistener&amp;#34;,&#xA;  &amp;#34;/triggers-eventid&amp;#34;: &amp;#34;f43a88c2-a462-47c9-a6cd-f98de681da40&amp;#34;,&#xA;  &amp;#34;/trigger&amp;#34;: &amp;#34;github-listener&amp;#34;&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;creating-a-pr-and-testing-our-trigger&#34;&gt;Creating a PR and testing our trigger&lt;/h2&gt;&#xA;&lt;p&gt;Let’s test that everything works by creating a PR, either manually or using the &lt;a href=&#34;https://github.com/cli/cli/&#34;&gt;GitHub CLI&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;git switch -c webhook-test&#xA;git commit --allow-empty -m &amp;#34;trigger webhook&amp;#34;&#xA;gh pr create -f&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And checking for a PipelineRun to get created:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl get pr&#xA;NAME                           SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME&#xA;github-pr-pipeline-run-qfvsx   True        Succeeded   73s         44s&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;We can see that everything worked. The pipeline was triggered and the run succeeded.&lt;/p&gt;&#xA;&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;In this tutorial we used Tekton Pipelines and Tekton Triggers to automatically trigger a pipeline run when a GitHub PR is created or commits have been pushed to its branch.&lt;/p&gt;&#xA;&lt;p&gt;We setup a GitHub webhook that will send a Pull Request event to our Kubernetes Ingress controller where it is passed on to the EventListener Service. The service will use Interceptors to validate and filter the webhook payload. The validated payload is passed to the TriggerBinding which will extract data from the webhook request and map it to parameters. Lastly the TriggerTemplate will populate a PipelineRun manifest with those parameters and create the resource, which will then be picked up by Tekton Pipelines and processed.&lt;/p&gt;&#xA;&lt;p&gt;To improve our pipeline futher we could send the status of the pipeline (pending, running, failed) back to github, which will then display it in the web UI. There’s a Task for that in the &lt;a href=&#34;https://github.com/tektoncd/catalog/tree/main/task/github-set-status/0.2&#34;&gt;Tekton Catalog&lt;/a&gt; which is easy to integrate. Or maybe we could &lt;a href=&#34;https://github.com/tektoncd/catalog/tree/main/task/github-add-comment&#34;&gt;add a comment&lt;/a&gt; to the PR with more detailed information.&lt;/p&gt;&#xA;&lt;p&gt;Tekton Triggers is still in alpha phase and things change very frequently. You can keep up with the development of the project by checking the the &lt;a href=&#34;https://github.com/tektoncd/triggers/&#34;&gt;Triggers&lt;/a&gt; repo for code changes and the &lt;a href=&#34;https://github.com/tektoncd/community&#34;&gt;Community&lt;/a&gt; repo for feature proposals and discussion around them.&lt;/p&gt;&#xA;&lt;p&gt;If you spot any mistakes in this blog post please let me know via email.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/tutorial-tekton-triggers-with-github-integration/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Tutorial: Encrypting Kubernetes Secrets with Sealed Secrets</title>
    <updated>2021-01-12T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2021-01-12:/encrypting-k8s-secrets-with-sealed-secrets/</id>
    <content type="html">&#xA;        &#xA;&#x9;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bitnami-labs/sealed-secrets/&#34;&gt;Sealed Secrets&lt;/a&gt; is a solution to store encrypted &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; secrets in version control.&lt;/p&gt;&#xA;&lt;p&gt;In this blog post we’ll learn how to install and use it.&lt;/p&gt;&#xA;&lt;h2 id=&#34;comparison-with-helm-secrets-and-sops&#34;&gt;Comparison with helm-secrets and sops&lt;/h2&gt;&#xA;&lt;p&gt;A popular alternative to Sealed Secrets is &lt;a href=&#34;https://github.com/zendesk/helm-secrets&#34;&gt;helm-secrets&lt;/a&gt; which uses &lt;a href=&#34;https://github.com/mozilla/sops&#34;&gt;sops&lt;/a&gt; as a backend.&lt;/p&gt;&#xA;&lt;p&gt;The main difference is:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sealed Secrets decrypts the secret &lt;em&gt;server-side&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Helm-secrets decrypts the secret &lt;em&gt;client-side&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Client-side decryption with helm-secrets can be a security risk since the client (such as a CI/CD system) needs to have access to the encryption key to perform the deployment.&lt;/p&gt;&#xA;&lt;p&gt;With Sealed Secrets and server-side decryption we can avoid this security risk. The encryption key only exists in the Kubernetes cluster and is never exposed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;installation-via-helm-chart&#34;&gt;Installation via Helm chart&lt;/h2&gt;&#xA;&lt;p&gt;Sealed Secrets consists of two components:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Client-side CLI tool to encrypt secrets and create sealed secrets&lt;/li&gt;&#xA;&lt;li&gt;Server-side controller used to decrypt sealed secrets and create secrets&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;To install the controller in our Kubernetes cluster we’ll use the official Helm chart from the &lt;a href=&#34;https://github.com/bitnami-labs/sealed-secrets/tree/master/helm/sealed-secrets&#34;&gt;sealed-secrets repository&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Add the repository and install it to the &lt;code&gt;kube-system&lt;/code&gt; namespace:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets&#xA;&#xA;helm install sealed-secrets --namespace kube-system --version 1.13.2 sealed-secrets/sealed-secrets&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;cli-tool-installation&#34;&gt;CLI tool installation&lt;/h2&gt;&#xA;&lt;p&gt;Secrets are encrypted client-side using the &lt;code&gt;kubeseal&lt;/code&gt; CLI tool.&lt;/p&gt;&#xA;&lt;p&gt;For macOS, we can use the &lt;a href=&#34;https://formulae.brew.sh/formula/kubeseal&#34;&gt;Homebrew formula&lt;/a&gt;. For Linux, we can download the binary from the &lt;a href=&#34;https://github.com/bitnami-labs/sealed-secrets/releases&#34;&gt;GitHub release page&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# macos&#xA;brew install kubeseal&#xA;&#xA;# linux&#xA;wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.13.1/kubeseal-linux-amd64 -O kubeseal&#xA;sudo install -m 755 kubeseal /usr/local/bin/kubeseal&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The &lt;code&gt;kubeseal&lt;/code&gt; CLI uses the current &lt;code&gt;kubectl&lt;/code&gt; context to &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/&#34;&gt;access the cluster&lt;/a&gt;. Before continuing make sure that &lt;code&gt;kubectl&lt;/code&gt; is connected to the cluster where Sealed Secrets should be installed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;creating-a-sealed-secret&#34;&gt;Creating a sealed secret&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;code&gt;kubeseal&lt;/code&gt; CLI takes a Kubernetes &lt;code&gt;Secret&lt;/code&gt; manifest as an input, encrypts it and outputs a &lt;code&gt;SealedSecret&lt;/code&gt; manifest.&lt;/p&gt;&#xA;&lt;p&gt;In this tutorial we’ll use this secret manifest as an input:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Secret&#xA;metadata:&#xA;  creationTimestamp: null&#xA;  name: my-secret&#xA;data:&#xA;  password: YmFy&#xA;  username: Zm9v&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Store the manifest in a file named &lt;code&gt;secret.yaml&lt;/code&gt; and encrypt it:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cat secret.yaml | kubeseal \&#xA;    --controller-namespace kube-system \&#xA;    --controller-name sealed-secrets \&#xA;    --format yaml \&#xA;    &amp;gt; sealed-secret.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The content of the &lt;code&gt;sealed-secret.yaml&lt;/code&gt; file should look like this:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: bitnami.com/v1alpha1&#xA;kind: SealedSecret&#xA;metadata:&#xA;  creationTimestamp: null&#xA;  name: my-secret&#xA;  namespace: default&#xA;spec:&#xA;  encryptedData:&#xA;    password: AgA...&#xA;    username: AgA...&#xA;  template:&#xA;    metadata:&#xA;      creationTimestamp: null&#xA;      name: my-secret&#xA;      namespace: default&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;We should now have the secret in &lt;code&gt;secret.yaml&lt;/code&gt; and the sealed secret in &lt;code&gt;sealed-secret.yaml&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: It’s not a good practice to store the unencrypted secret in a file. This is only for demonstration purposes in this tutorial.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;To deploy the sealed secret we apply the manifest with kubectl:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl apply -f sealed-secret.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The controller in the cluster will notice that a &lt;code&gt;SealedSecret&lt;/code&gt; resource has been created, decrypt it and create a decrypted &lt;code&gt;Secret&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Let’s fetch the secret to make sure that the controller has successfully unsealed it:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl get secret my-secret -o yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The data should contain our base64 encoded username and password:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;data:&#xA;  password: YmFy&#xA;  username: Zm9v&#xA;...&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Everything went well. The secret has been successfully unsealed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;updating-a-sealed-secret&#34;&gt;Updating a sealed secret&lt;/h2&gt;&#xA;&lt;p&gt;To update a value in a sealed secret, we have to create a new &lt;code&gt;Secret&lt;/code&gt; manifest locally and merge it into an existing &lt;code&gt;SealedSecret&lt;/code&gt; with the &lt;code&gt;--merge-into&lt;/code&gt; option.&lt;/p&gt;&#xA;&lt;p&gt;In the example below we update the value of the password key (&lt;code&gt;--from-file=password&lt;/code&gt;) to &lt;code&gt;my new password&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;echo -n &amp;#34;my new password&amp;#34; \&#xA;    | kubectl create secret generic xxx --dry-run=client --from-file=password=/dev/stdin -o json \&#xA;    | kubeseal --controller-namespace=kube-system --controller-name=sealed-secrets --format yaml --merge-into sealed-secret.yaml&#xA;&#xA;kubectl apply -f sealed-secret.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The local secret is temporary and the name (&lt;code&gt;xxx&lt;/code&gt; in our case) doesn’t matter. The name of the sealed secret will stay the same.&lt;/p&gt;&#xA;&lt;h2 id=&#34;adding-a-new-value-to-a-sealed-secret&#34;&gt;Adding a new value to a sealed secret&lt;/h2&gt;&#xA;&lt;p&gt;The difference between updating a value and adding a new value is the name of the key. If a key named &lt;code&gt;password&lt;/code&gt; already exists, it will update it. If it doesn’t exist, it will add it.&lt;/p&gt;&#xA;&lt;p&gt;For example to add a new &lt;code&gt;api_key&lt;/code&gt; key (&lt;code&gt;--from-file=api_key&lt;/code&gt;) into our secret we run:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;echo -n &amp;#34;my secret api key&amp;#34; \&#xA;    | kubectl create secret generic xxx --dry-run=client --from-file=api_key=/dev/stdin -o json \&#xA;    | kubeseal --controller-namespace=kube-system --controller-name=sealed-secrets --format yaml --merge-into sealed-secret.yaml&#xA;&#xA;kubectl apply -f sealed-secret.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;deleting-a-value-from-a-sealed-secret&#34;&gt;Deleting a value from a sealed secret&lt;/h2&gt;&#xA;&lt;p&gt;To delete a key from the sealed secret we have to remove it from the YAML file:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# BSD sed (macOS)&#xA;sed -i &amp;#39;&amp;#39; &amp;#39;/api_key:/d&amp;#39; sealed-secret.yaml&#xA;&#xA;# GNU sed&#xA;sed -i &amp;#39;/api_key:/d&amp;#39; sealed-secret.yaml&#xA;&#xA;kubectl apply -f sealed-secret.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After applying the file, the controller will update the &lt;code&gt;Secret&lt;/code&gt; automatically and remove the &lt;code&gt;api_key&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;delete-the-sealed-secret&#34;&gt;Delete the sealed secret&lt;/h2&gt;&#xA;&lt;p&gt;To delete the secret, we use kubectl to delete the resource:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl delete -f sealed-secret.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This will delete the &lt;code&gt;SealedSecret&lt;/code&gt; resource from the cluster as well as the corresponding &lt;code&gt;Secret&lt;/code&gt; resource.&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;Sealed Secrets is a secure way to manage Kubernetes secrets in version control. The encryption key is stored and secrets are decrypted in the cluster. The client doesn’t have access to the encryption key.&lt;/p&gt;&#xA;&lt;p&gt;The client uses the &lt;code&gt;kubeseal&lt;/code&gt; CLI tool to generate &lt;code&gt;SealedSecret&lt;/code&gt; manifests that hold encrypted data. After applying the file the server-side controller will recognize a new sealed secret resource and decrypt it to create a &lt;code&gt;Secret&lt;/code&gt; resource.&lt;/p&gt;&#xA;&lt;p&gt;Overall I’d recommend to use Sealed Secrets for improved security.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/encrypting-k8s-secrets-with-sealed-secrets/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Setting up Argo CD with Helm</title>
    <updated>2021-01-05T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2021-01-05:/setting-up-argocd-with-helm/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;In this blog post we’re going to setup &lt;a href=&#34;https://argoproj.github.io/argo-cd/&#34;&gt;Argo CD&lt;/a&gt; on a Kubernetes cluster. We’ll install it with Helm, create an application to use the &lt;a href=&#34;https://argoproj.github.io/argo-cd/operator-manual/declarative-setup/#app-of-apps&#34;&gt;app-of-apps&lt;/a&gt; pattern and set everything up so that Argo CD can update itself.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/setting-up-argocd-with-helm/images/1-argo-app-details.png&#34; alt=&#34;A picture of the Argo CD web ui showing the details view of an application&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;All files mentioned in this blog post are available in a &lt;a href=&#34;https://github.com/arthurk/argocd-example-install/&#34;&gt;Git repository on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-argo-cd&#34;&gt;What is Argo CD?&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://argoproj.github.io/argo-cd/&#34;&gt;Argo CD&lt;/a&gt; is a &lt;a href=&#34;https://www.gitops.tech/&#34;&gt;GitOps&lt;/a&gt; tool to automatically synchronize the cluster to the desired state defined in a Git repository. Each workload is defined declarative through a resource manifest in a YAML file. Argo CD checks if the state defined in the Git repository matches what is running on the cluster and synchronizes it if changes were detected.&lt;/p&gt;&#xA;&lt;p&gt;For example, instead of running CLI commands to update resources with &lt;code&gt;kubectl apply&lt;/code&gt; or &lt;code&gt;helm upgrade&lt;/code&gt; we would update an &lt;code&gt;Application&lt;/code&gt; manifest inside our Git repository. Argo CD periodically checks the repository for changes. It will recognize that the application manifest has changed and automatically synchronize the resources on the cluster.&lt;/p&gt;&#xA;&lt;p&gt;With this workflow security is improved too. A connection to the cluster, either from the developers laptop or from a CI/CD system, is no longer needed as changes are pulled from the Git repository by a Kubernetes Operator running inside the cluster.&lt;/p&gt;&#xA;&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;&#xA;&lt;p&gt;To follow this tutorial you’ll need:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A Kubernetes cluster and kubectl (1.19.1)&lt;/li&gt;&#xA;&lt;li&gt;Helm (3.4.2)&lt;/li&gt;&#xA;&lt;li&gt;A public git repository&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The version numbers behind the tools are the ones I’ve used to write this tutorial.&lt;/p&gt;&#xA;&lt;h2 id=&#34;creating-a-helm-chart&#34;&gt;Creating a Helm chart&lt;/h2&gt;&#xA;&lt;p&gt;We’ll use Helm to install Argo CD with the chart from &lt;a href=&#34;https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd&#34;&gt;argoproj/argo-helm&lt;/a&gt;. Our setup needs to set custom values and we’ll create our own Helm “umbrella” chart that pulls in the original Argo CD chart as a dependency.&lt;/p&gt;&#xA;&lt;p&gt;Using this approach we can also bundle extra resources with the chart in the future. For example, we can install credentials that are used to authenticate with private Git or Helm repositories by placing them in the chart’s &lt;code&gt;template&lt;/code&gt; directory.&lt;/p&gt;&#xA;&lt;p&gt;To create the chart we make a directory and place two files in it:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;mkdir -p charts/argo-cd&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;charts/argo-cd/Chart.yaml&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v2&#xA;name: argo-cd&#xA;version: 1.0.0&#xA;dependencies:&#xA;  - name: argo-cd&#xA;    version: 2.11.0&#xA;    repository: https://argoproj.github.io/argo-helm&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;charts/argo-cd/values.yaml&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;argo-cd:&#xA;  installCRDs: false&#xA;  global:&#xA;    image:&#xA;      tag: v1.8.1&#xA;  dex:&#xA;    enabled: false&#xA;  server:&#xA;    extraArgs:&#xA;      - --insecure&#xA;    config:&#xA;      repositories: |&#xA;        - type: helm&#xA;          name: stable&#xA;          url: https://charts.helm.sh/stable&#xA;        - type: helm&#xA;          name: argo-cd&#xA;          url: https://argoproj.github.io/argo-helm&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;We set the following chart values:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;installCRDs&lt;/code&gt; is set to &lt;code&gt;false&lt;/code&gt;. This is required when using Helm v3 to avoid warnings about nonexistant webhooks&lt;/li&gt;&#xA;&lt;li&gt;The Helm chart defaults to Argo CD version &lt;code&gt;1.7.6&lt;/code&gt;. To use the latest version we bump &lt;code&gt;global.image.tag&lt;/code&gt; to &lt;code&gt;1.8.1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;We disable the &lt;code&gt;dex&lt;/code&gt; component that is used for integration with external auth providers&lt;/li&gt;&#xA;&lt;li&gt;We start the server with the &lt;code&gt;--insecure&lt;/code&gt; flag to serve the Web UI over http (This is assuming we’re using a local k8s server without TLS setup)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Before we install the chart, we need to generate a &lt;code&gt;Chart.lock&lt;/code&gt; file. We do this so that our dependency (the original &lt;code&gt;argo-cd&lt;/code&gt; chart) can be rebuilt. This is important later on when we let Argo CD manage this chart to avoid errors.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;helm repo add argo-cd https://argoproj.github.io/argo-helm&#xA;helm dep update charts/argo-cd/&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This will generate two files:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Chart.lock&lt;/li&gt;&#xA;&lt;li&gt;charts/argo-cd-2.11.0.tgz&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The &lt;code&gt;tgz&lt;/code&gt; file is the downloaded dependency and not required in our Git repository. Argo CD can download the dependencies by itself. We exclude it by creating a &lt;code&gt;.gitignore&lt;/code&gt; file in the chart directory:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;echo &amp;#34;charts/&amp;#34; &amp;gt; charts/argo-cd/.gitignore&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The chart is ready and we can push it to our Git repository:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;git add charts/argo-cd&#xA;git commit -m &amp;#39;add argo-cd chart&amp;#39;&#xA;git push&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;installing-the-argo-cd-helm-chart&#34;&gt;Installing the Argo CD Helm chart&lt;/h2&gt;&#xA;&lt;p&gt;Later on we’ll let Argo CD manage itself so that we can perform updates by modifying files inside our Git repository. But for the initial bootstrap we have to install it manually:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;helm install argo-cd charts/argo-cd/&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There will be warnings about deprecated CRDs:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;These warnings can safely be ignored for now. There’s a &lt;a href=&#34;https://github.com/argoproj/argo-helm/pull/514&#34;&gt;PR&lt;/a&gt; to fix this issue in future chart versions.&lt;/p&gt;&#xA;&lt;h2 id=&#34;accessing-the-web-ui&#34;&gt;Accessing the Web UI&lt;/h2&gt;&#xA;&lt;p&gt;The Helm chart doesn’t install an Ingress by default, so to access the Argo CD Web UI we have to port-forward to the service:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl port-forward svc/argo-cd-argocd-server 8080:443&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;We can then visit &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; to access it.&lt;/p&gt;&#xA;&lt;p&gt;The default username is “admin”. The password is auto-generated and defaults to the pod name of the Argo CD server pod. We can get it with:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl get pods -l app.kubernetes.io/name=argocd-server -o name | cut -d&amp;#39;/&amp;#39; -f 2&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After logging in for the first time we’ll see the following screen:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/setting-up-argocd-with-helm/images/2-argo-new-install.png&#34; alt=&#34;A picture of the Argo CD Web UI after logging in for the first time&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;Applications can be added through the Web UI but since we want to manage everything declaratively we’ll write &lt;code&gt;Application&lt;/code&gt; manifests.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-root-app&#34;&gt;The root app&lt;/h2&gt;&#xA;&lt;p&gt;To add an application to Argo CD we need to add an &lt;code&gt;Application&lt;/code&gt; resource to Kubernetes. It specifies the Git repository and the file path under which to find the manifests.&lt;/p&gt;&#xA;&lt;p&gt;For example, if we wanted to deploy &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt;, we would write an &lt;code&gt;Application&lt;/code&gt; manifest for it. It would specify what Helm chart to use and the values to set. We would then apply it with &lt;code&gt;kubectl&lt;/code&gt; and wait for the resource to be created. This process is repeated for other applications we want to add.&lt;/p&gt;&#xA;&lt;p&gt;But applying the manifests with &lt;code&gt;kubectl&lt;/code&gt; is a manual step that’s error prone and can be insecure. We also need to repeat it for every application, not only when adding applications but also when updating them.&lt;/p&gt;&#xA;&lt;p&gt;With Argo CD there is a way to automate this by creating an application that implements the &lt;a href=&#34;https://argoproj.github.io/argo-cd/operator-manual/declarative-setup/#app-of-apps&#34;&gt;app of apps&lt;/a&gt; pattern. We can call this the “root” application.&lt;/p&gt;&#xA;&lt;p&gt;The root application has one task: it generates &lt;code&gt;Application&lt;/code&gt; manifests for other applications. Argo CD will watch the root application and synchronize any applications that it generates.&lt;/p&gt;&#xA;&lt;p&gt;With this setup, we only have to add one application manually rather than all applications.&lt;/p&gt;&#xA;&lt;h2 id=&#34;creating-the-root-app-chart&#34;&gt;Creating the root app chart&lt;/h2&gt;&#xA;&lt;p&gt;For the root app we’ll create a Helm chart that has &lt;code&gt;Application&lt;/code&gt; manifests as templates. We create it in an &lt;code&gt;apps&lt;/code&gt; directory and put a &lt;code&gt;Chart.yaml&lt;/code&gt; file and an empty &lt;code&gt;values.yaml&lt;/code&gt; file in it:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;mkdir -p apps/templates&#xA;touch apps/values.yaml&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;apps/Chart.yaml&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v2&#xA;name: root&#xA;version: 1.0.0&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;We create the &lt;code&gt;Application&lt;/code&gt; manifest for our root application in &lt;code&gt;apps/templates/root.yaml&lt;/code&gt;. This allows us to do any updates to the root application itself through Argo CD:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: argoproj.io/v1alpha1&#xA;kind: Application&#xA;metadata:&#xA;  name: root&#xA;  finalizers:&#xA;  - resources-finalizer.argocd.argoproj.io&#xA;spec:&#xA;  destination:&#xA;    server: https://kubernetes.default.svc&#xA;    namespace: default&#xA;  project: default&#xA;  source:&#xA;    path: apps/&#xA;    repoURL: https://github.com/arthurk/argocd-example-install.git&#xA;    targetRevision: HEAD&#xA;  syncPolicy:&#xA;    automated:&#xA;      prune: true&#xA;      selfHeal: true&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The above &lt;code&gt;Application&lt;/code&gt; watches the Helm chart under &lt;code&gt;apps/&lt;/code&gt; (our root application) and synchronizes it if changes were detected.&lt;/p&gt;&#xA;&lt;p&gt;How does Argo CD know our application is a Helm chart? It looks for a &lt;code&gt;Chart.yaml&lt;/code&gt; file under &lt;code&gt;path&lt;/code&gt; in the Git repository. If present, it will check the &lt;code&gt;apiVersion&lt;/code&gt; inside it. For &lt;code&gt;apiVersion: v1&lt;/code&gt; it uses Helm 2, for &lt;code&gt;apiVersion: v2&lt;/code&gt; it uses Helm 3 to render the chart.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Argo CD will not use &lt;code&gt;helm install&lt;/code&gt; to install charts. It will render the chart with &lt;code&gt;helm template&lt;/code&gt; and then apply the output with &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;To deploy our root application we need to push the files to our Git repository and apply the manifest:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;git add apps&#xA;git ci -m &amp;#39;add root app&amp;#39;&#xA;git push&#xA;&#xA;helm template apps/ | kubectl apply -f -&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;In the Web UI we can now see that the root application was created:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/setting-up-argocd-with-helm/images/3-argo-root-app-created.png&#34; alt=&#34;Argo CD Web UI showing root application&#34;/&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;letting-argo-cd-manage-itself&#34;&gt;Letting Argo CD manage itself&lt;/h2&gt;&#xA;&lt;p&gt;We previously installed Argo CD with &lt;code&gt;helm install&lt;/code&gt; which means that updates would require us to run &lt;code&gt;helm upgrade&lt;/code&gt;. To avoid doing this we can create an Application resource for Argo CD and let it manage itself.&lt;/p&gt;&#xA;&lt;p&gt;With this approach any updates to our Argo CD deployment can be made by modifying files in our Git repository rather than running manual commands.&lt;/p&gt;&#xA;&lt;p&gt;We put the application manifest in &lt;code&gt;apps/templates/argo-cd.yaml&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: argoproj.io/v1alpha1&#xA;kind: Application&#xA;metadata:&#xA;  name: argo-cd&#xA;  namespace: default&#xA;  finalizers:&#xA;  - resources-finalizer.argocd.argoproj.io&#xA;spec:&#xA;  destination:&#xA;    server: https://kubernetes.default.svc&#xA;    namespace: default&#xA;  project: default&#xA;  source:&#xA;    path: charts/argo-cd&#xA;    repoURL: https://github.com/arthurk/argocd-example-install.git&#xA;    targetRevision: HEAD&#xA;  syncPolicy:&#xA;    automated:&#xA;      prune: true&#xA;      selfHeal: true&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then push the file to our Git repository:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;git add apps/templates/argo-cd.yaml&#xA;git ci -m &amp;#39;add argo-cd application&amp;#39;&#xA;git push&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;In the Web UI we should now see the root application being &lt;code&gt;OutOfSync&lt;/code&gt; and &lt;code&gt;Syncing&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;If it doesn’t show the application immediately, click the “Refresh” button on the root application. By default it checks for changes in the Git repository every 3 minutes.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/setting-up-argocd-with-helm/images/4-argo-app-created.png&#34; alt=&#34;Argo CD Web UI overview after the Argo CD application has been created&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;Once the Argo CD application is synced, we can delete it from Helm. It can now manage itself.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kubectl delete secret -l owner=helm,name=argo-cd&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;example-installing-prometheus&#34;&gt;Example: Installing Prometheus&lt;/h2&gt;&#xA;&lt;p&gt;To demonstrate how to deploy a Helm chart with Argo CD, we’ll add &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; on our cluster.&lt;/p&gt;&#xA;&lt;p&gt;We add the application manifest in &lt;code&gt;apps/templates/prometheus.yaml&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: argoproj.io/v1alpha1&#xA;kind: Application&#xA;metadata:&#xA;  name: prometheus&#xA;  namespace: default&#xA;  finalizers:&#xA;  - resources-finalizer.argocd.argoproj.io&#xA;spec:&#xA;  destination:&#xA;    server: https://kubernetes.default.svc&#xA;    namespace: default&#xA;  project: default&#xA;  source:&#xA;    chart: prometheus&#xA;    helm:&#xA;      values: |&#xA;        pushgateway:&#xA;          enabled: false&#xA;    repoURL: https://prometheus-community.github.io/helm-charts&#xA;    targetRevision: 13.0.2&#xA;  syncPolicy:&#xA;    automated:&#xA;      prune: true&#xA;      selfHeal: true&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;To deploy the application we push the manifest to our Git repository:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;git add apps/templates/prometheus.yaml&#xA;git ci -m &amp;#39;add prometheus&amp;#39;&#xA;git push&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Compared to our Argo CD chart from above the differences are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;We’re using &lt;code&gt;chart&lt;/code&gt; instead of &lt;code&gt;path&lt;/code&gt; to install a Helm chart from a Helm repository&lt;/li&gt;&#xA;&lt;li&gt;The &lt;code&gt;targetRevision&lt;/code&gt; is the chart version&lt;/li&gt;&#xA;&lt;li&gt;The &lt;code&gt;repoURL&lt;/code&gt; is set to the &lt;a href=&#34;https://github.com/prometheus-community/helm-charts/&#34;&gt;prometheus-community&lt;/a&gt; Helm chart repository&lt;/li&gt;&#xA;&lt;li&gt;We’re overriding the chart’s default values to disable the pushgateway&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Prometheus should show up in the Web UI after the next refresh.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/setting-up-argocd-with-helm/images/5-prometheus.png&#34; alt=&#34;Argo CD Web UI showing application overview after prometheus application has been added&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;To uninstall Prometheus we have to delete the previously added &lt;code&gt;prometheus.yaml&lt;/code&gt; file:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;git rm apps/templates/prometheus.yaml&#xA;git ci -m &amp;#39;remove prometheus&amp;#39;&#xA;git push&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The application will be removed from the cluster after the next refresh.&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;In this tutorial we’ve installed Argo CD from a Helm chart and set it up so that it can manage itself. Updates to Argo CD can be done by modifying the manifest inside the Git repository.&lt;/p&gt;&#xA;&lt;p&gt;We’ve created a “root” application that uses the &lt;a href=&#34;https://argoproj.github.io/argo-cd/operator-manual/declarative-setup/#app-of-apps&#34;&gt;app-of-apps&lt;/a&gt; pattern to manage our applications in a declarative way.&lt;/p&gt;&#xA;&lt;p&gt;To show how we can install applications with Argo CD, we’ve added (and then removed) Prometheus from our cluster.&lt;/p&gt;&#xA;&lt;p&gt;More details about Argo CD can be found on the &lt;a href=&#34;https://argoproj.github.io/argo-cd/&#34;&gt;project page&lt;/a&gt; and the &lt;a href=&#34;https://github.com/argoproj/argo-cd/&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/setting-up-argocd-with-helm/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Run QEMU on macOS 11.0 Big Sur</title>
    <updated>2020-12-18T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-12-18:/qemu-on-macos-big-sur/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;Around 3 months ago I wrote a blog post about how to create a &lt;a href=&#34;https://www.arthurkoziel.com/qemu-ubuntu-20-04/&#34;&gt;QEMU Ubuntu 20.04 VM on macOS&lt;/a&gt;. If you follow the instructions with the newly released macOS 11.0, QEMU 5.1 will fail with the following error:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;qemu-system-x86_64: Error: HV_ERROR&#xA;fish: &amp;#39;qemu-system-x86_64 \&#xA;    -machi…&amp;#39; terminated by signal SIGABRT (Abort)&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This error occurs because Apple has made changes to the hypervisor entitlements. Entitlements are key-value pairs that grant an executable permission to use a service or technology. In this case the QEMU binary is missing the entitlement to create and manage virtual machines.&lt;/p&gt;&#xA;&lt;p&gt;To be more specific, the &lt;code&gt;com.apple.vm.hypervisor&lt;/code&gt; entitlement (used in macOS 10.15) has been deprecated and &lt;a href=&#34;https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_security_hypervisor&#34;&gt;replaced&lt;/a&gt; by &lt;code&gt;com.apple.security.hypervisor&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;To fix the issue all we have to do is add the entitlement to the &lt;code&gt;qemu-system-x86_64&lt;/code&gt; binary. First create an xml file named &lt;code&gt;entitlements.xml&lt;/code&gt; with this content:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt;&#xA;&amp;lt;!DOCTYPE plist PUBLIC &amp;#34;-//Apple//DTD PLIST 1.0//EN&amp;#34; &amp;#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd&amp;#34;&amp;gt;&#xA;&amp;lt;plist version=&amp;#34;1.0&amp;#34;&amp;gt;&#xA;&amp;lt;dict&amp;gt;&#xA;    &amp;lt;key&amp;gt;com.apple.security.hypervisor&amp;lt;/key&amp;gt;&#xA;    &amp;lt;true/&amp;gt;&#xA;&amp;lt;/dict&amp;gt;&#xA;&amp;lt;/plist&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then sign the qemu binary with it:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;codesign -s - --entitlements entitlements.xml --force /usr/local/bin/qemu-system-x86_64&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Now the &lt;code&gt;qemu-system-x86_64&lt;/code&gt; command should work and can be used to launch VMs.&lt;/p&gt;&#xA;&lt;p&gt;(Thanks to Tony for noticing this issue and contacting me)&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/qemu-on-macos-big-sur/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Convert Markdown to HTML with Pandoc</title>
    <updated>2020-11-29T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-11-29:/convert-md-to-html-pandoc/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;In this post I’ll describe how to use &lt;a href=&#34;&#34;&gt;Pandoc&lt;/a&gt; to convert Markdown to a full HTML page (including header/footer).&lt;/p&gt;&#xA;&lt;p&gt;The Pandoc version used for the examples below is 2.11.2.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-pandoc&#34;&gt;What is Pandoc?&lt;/h2&gt;&#xA;&lt;p&gt;Pandoc is an open-source document converter that is written in Haskell. It was initially released in 2006 and has been under active development since then.&lt;/p&gt;&#xA;&lt;p&gt;The goal of Pandoc is to convert a document from one markup format to another. It distinguishes between input formats and output formats. As of writing this it supports &lt;a href=&#34;https://pandoc.org/MANUAL.html#input-formats&#34;&gt;38 input formats&lt;/a&gt; and &lt;a href=&#34;https://pandoc.org/MANUAL.html#output-formats&#34;&gt;59 output formats&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;In this post we’ll use Markdown as an input format and HTML as an output format.&lt;/p&gt;&#xA;&lt;h2 id=&#34;preparing-the-html-template&#34;&gt;Preparing the HTML template&lt;/h2&gt;&#xA;&lt;p&gt;To generate a full HTML page we have to use Pandoc’s standalone mode which will use a &lt;a href=&#34;https://pandoc.org/MANUAL.html#templates&#34;&gt;template&lt;/a&gt; to add header and footer.&lt;/p&gt;&#xA;&lt;p&gt;Pandoc ships with a default template, if you wish to use that skip this section and omit the &lt;code&gt;--template&lt;/code&gt; argument.&lt;/p&gt;&#xA;&lt;p&gt;The template we’ll use is this (save it to &lt;code&gt;template.html&lt;/code&gt;):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;!doctype html&amp;gt;&#xA;&amp;lt;html lang=&amp;#34;en&amp;#34;&amp;gt;&#xA;  &amp;lt;head&amp;gt;&#xA;    &amp;lt;meta charset=&amp;#34;utf-8&amp;#34;&amp;gt;&#xA;    &amp;lt;meta name=&amp;#34;date&amp;#34; content=&amp;#39;$date-meta$&amp;#39;&amp;gt;&#xA;    &amp;lt;title&amp;gt;$title$&amp;lt;/title&amp;gt;&#xA;  &amp;lt;/head&amp;gt;&#xA;  &amp;lt;body&amp;gt;&#xA;    &amp;lt;p&amp;gt;Date: $date$&amp;lt;/p&amp;gt;&#xA;$body$&#xA;  &amp;lt;/body&amp;gt;&#xA;&amp;lt;/html&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Pandoc’s template variables can have &lt;a href=&#34;https://pandoc.org/MANUAL.html#interpolated-variables&#34;&gt;different formats&lt;/a&gt;, the one we’re using here start and end with a dollar sign:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;$date$&lt;/code&gt;: A date in a parsable format. See the &lt;a href=&#34;https://pandoc.org/MANUAL.html#variables-set-automatically&#34;&gt;date-meta docs&lt;/a&gt; for a list of recognized formats for the &lt;code&gt;date&lt;/code&gt; variable. We use this to show when the document was created&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;$date-meta$&lt;/code&gt;: The &lt;code&gt;date&lt;/code&gt; parsed to ISO 8601 format. This is automatically done&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;$title$&lt;/code&gt;: The document title&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;$body$&lt;/code&gt;: The document body in HTML (the converted Markdown)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We only need to set the &lt;code&gt;date&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; in the Markdown document via a metadata block.&lt;/p&gt;&#xA;&lt;h2 id=&#34;writing-the-markdown-file&#34;&gt;Writing the Markdown file&lt;/h2&gt;&#xA;&lt;p&gt;Create a Markdown file &lt;code&gt;doc.md&lt;/code&gt; with the following content:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---&#xA;title: My Document&#xA;date: September 22, 2020&#xA;---&#xA;&#xA;## Test&#xA;some text&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The beginning of the document is the metadata block with required &lt;code&gt;date&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; variables mentioned above.&lt;/p&gt;&#xA;&lt;p&gt;Several Markdown variants are &lt;a href=&#34;https://pandoc.org/MANUAL.html#Markdown-variants&#34;&gt;supported&lt;/a&gt; such as GitHub-Flavored markdown. This example uses &lt;a href=&#34;https://pandoc.org/MANUAL.html#pandocs-markdown&#34;&gt;Pandoc’s extended markdown&lt;/a&gt; which is the default input for files with the &lt;code&gt;md&lt;/code&gt; extension.&lt;/p&gt;&#xA;&lt;h2 id=&#34;converting-the-document&#34;&gt;Converting the document&lt;/h2&gt;&#xA;&lt;p&gt;Run the following command to generate the HTML page:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;pandoc --standalone --template template.html doc.md&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Pandoc will try to guess the input format from the file extension (&lt;code&gt;.md&lt;/code&gt; will use the Markdown input format) and output it to HTML (the default output format).&lt;/p&gt;&#xA;&lt;p&gt;The output will be printed to the terminal:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;!doctype html&amp;gt;&#xA;&amp;lt;html lang=&amp;#34;en&amp;#34;&amp;gt;&#xA;  &amp;lt;head&amp;gt;&#xA;    &amp;lt;meta charset=&amp;#34;utf-8&amp;#34;&amp;gt;&#xA;    &amp;lt;meta name=&amp;#34;date&amp;#34; content=&amp;#39;2020-09-22&amp;#39;&amp;gt;&#xA;    &amp;lt;title&amp;gt;My Document&amp;lt;/title&amp;gt;&#xA;  &amp;lt;/head&amp;gt;&#xA;  &amp;lt;body&amp;gt;&#xA;    &amp;lt;p&amp;gt;Date: September 22, 2020&amp;lt;/p&amp;gt;&#xA;&amp;lt;h2 id=&amp;#34;test&amp;#34;&amp;gt;Test&amp;lt;/h2&amp;gt;&#xA;&amp;lt;p&amp;gt;some text&amp;lt;/p&amp;gt;&#xA;  &amp;lt;/body&amp;gt;&#xA;&amp;lt;/html&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;To save the document to a file we can either redirect stdout or use the &lt;code&gt;-o&lt;/code&gt; argument:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;pandoc --standalone --template template.html doc.md -o doc.html&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;In this example we’ve converted Markdown to a standalone HTML page that is using a custom template.&lt;/p&gt;&#xA;&lt;p&gt;This was just a simple example of what Pandoc is capable to do. The standalone mode coupled with the bundled default templates makes it easy to generate a wide variety of outputs such as HTML presentations, Jupyter notebooks or PDF documents.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/convert-md-to-html-pandoc/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Smartyao LY092 Keyboard Review</title>
    <updated>2020-11-24T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-11-24:/keyboard-review-smartyao-ly092/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;This blog post is a review of the &lt;a href=&#34;http://www.smartyao.com/?_l=en&#34;&gt;Smartyao&lt;/a&gt; LY092 Split-Keyboard which I’ve been using for the past 7 months.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/keyboard.jpg&#34; title=&#34;Smartyao LY092 Keyboard&#34; alt=&#34;Smartyao L092 Keyboard&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;I bought this split-keyboard to improve ergonomics since it allows typing at shoulder width rather than having to hunch over the keyboard.&lt;/p&gt;&#xA;&lt;p&gt;This is my first mechanical keyboard in over 10 years and also my first split keyboard. I wasn’t sure if I’d like split keyboards and didn’t want to spend $350 on an ErgoDox, so I decided to try this one out.&lt;/p&gt;&#xA;&lt;p&gt;I bought it on &lt;a href=&#34;http://shop114100401.taobao.com/?spm=2013.1.1000126.2.MloYHY&amp;amp;v=1&#34;&gt;Taobao&lt;/a&gt; in China for 458 RMB (around $70 USD). There’s a rebranded version (Koolertron) available on &lt;a href=&#34;https://www.amazon.com/Koolertron-Programmable-Mechanical-Keyboard-Ergonomic/dp/B081GX5CY1&#34;&gt;Amazon&lt;/a&gt; although it costs more than double the original price.&lt;/p&gt;&#xA;&lt;h2 id=&#34;hardware&#34;&gt;Hardware&lt;/h2&gt;&#xA;&lt;p&gt;The keyboard has 43 keys on the left half and 45 keys on the right half. It can be ordered with either Cherry MX (Brown or Red) or Gateron (Blue or Black) switches. I’ve picked the Cherry MX Brown. Extra key caps with Numpad labels are included in the package and the existing keys can be swapped out.&lt;/p&gt;&#xA;&lt;p&gt;The letters and numbers on the key caps are engraved so there’s no need to worry about them wearing off. The letters are see-through which is nice with the background lighting. Each side of the keyboard has 2 Micro USB connections. The middle ones are for connecting the two halves with each other. The left side or right side can be connected to the computer USB port with the included Micro USB to USB adapter.&lt;/p&gt;&#xA;&lt;p&gt;The background light is white and can be turned on/off by pressing the top right key on the left side and top left key on the right side (each side has to be turned off separately). The key for turning the light on/off can be programmed in the configuration software.&lt;/p&gt;&#xA;&lt;h2 id=&#34;configuration-software&#34;&gt;Configuration Software&lt;/h2&gt;&#xA;&lt;p&gt;The keyboard is programmable which means that each key can be assigned a different value or alternatively use of one of the built-in functions. I’ve described some of those functions below. For a full reference of what the keyboard is capable to do check the &lt;a href=&#34;https://dme657285d3.pic11.websiteonline.cn/upload/Thumbkeyboard_User_Manual_V5.pdf&#34;&gt;User Manual&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;&#xA;&lt;p&gt;The software can be downloaded from the &lt;a href=&#34;http://www.smartyao.com/page91?_l=en&#34;&gt;manufacturer’s website&lt;/a&gt;. It’s only available for Windows but I was able to run it without problems on macOS via VirtualBox. I used the &lt;a href=&#34;https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/&#34;&gt;MSEdge on Win10 trial&lt;/a&gt; version of Windows 10.&lt;/p&gt;&#xA;&lt;p&gt;VirtualBox disables all USB connections by default. The keyboard needs to be connected via the menu before it can be used. It shows up under &lt;code&gt;Devices -&amp;gt; USB&lt;/code&gt; as &lt;code&gt;LingYao ShangHai Thumb Keyboard&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/keyboard-vbox-menu.png&#34; alt=&#34;VirtualBox USB menu showing the keyboard device&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;Each keyboard-side has to be programmed separately. It has to be put into a configuration mode by holding down the top-left key (for example &lt;code&gt;ESC&lt;/code&gt; on the left side) while connecting it to the USB port. The LEDs will start blinking to indicate that it is now in config mode. On launch the configuration software will automatically load the default profile.&lt;/p&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;&#xA;&lt;p&gt;The following picture shows the default view after loading the left side of the keyboard into the config software:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/overview-left.png&#34; title=&#34;Configuration Software&#34; alt=&#34;Overview of the left keyboard side in the configuration software&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;Each key can be programmed individually by clicking on it and selecting a new value with the virtual keyboard:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/virtual-keyboard.png&#34; title=&#34;Virtual Keyboard&#34; alt=&#34;Picture of the Virtual Keyboard in the editor&#34;/&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;functions&#34;&gt;Functions&lt;/h2&gt;&#xA;&lt;p&gt;Besides assigning a new char to a key there are also a few functions that can be used:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;3-Layer Keymap&lt;/strong&gt;: Use a different char for a key when &lt;code&gt;FN1&lt;/code&gt; or &lt;code&gt;FN2&lt;/code&gt; is pressed. For example &lt;code&gt;FN1+H&lt;/code&gt; can be mapped to &lt;code&gt;Left Arrow&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Hotkey&lt;/strong&gt;: Use one key press to send a shortcut such as &lt;code&gt;Ctrl+C&lt;/code&gt;, another key to send &lt;code&gt;Ctrl+V&lt;/code&gt;. This is a useful function to implement copy/paste/cut/undo/redo with a single key press&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Burst&lt;/strong&gt;: Send up to 3 chars with a single click. Useful to output a pair of braces &lt;code&gt;[]&lt;/code&gt; with a single press&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Mouse&lt;/strong&gt;: Control (move, click, …) the mouse with the keyboard. For example a key press can move the mouse by 200px to the left&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Round-Robin&lt;/strong&gt;: Output a different char on every key press. Up to 3 chars can be chosen&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Continue&lt;/strong&gt;: Simulates a key-hold when a key is pressed the first time and a key-release when it’s pressed again. Gaming-feature to make it easier to walk around without continuously holding down the key&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Jitter&lt;/strong&gt;: One key press to continuously output up to 3 chars. Like the Burst function but will continue doing so until the key is pressed again&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;There are also special functions to turn the Backlights ON/OFF, Switch Profiles and execute Macros.&lt;/p&gt;&#xA;&lt;h2 id=&#34;macros&#34;&gt;Macros&lt;/h2&gt;&#xA;&lt;p&gt;Macros are scripts that can be programmed to do a series of key presses or mouse movements with a short delay between them:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/macro-editor.png&#34; title=&#34;Macro Editor&#34; alt=&#34;Macro Editor window&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;Each keyboard can save 8 macros saved with each Macro being able to press up to 32 keys.&lt;/p&gt;&#xA;&lt;h2 id=&#34;twokeyclick&#34;&gt;TwoKeyClick&lt;/h2&gt;&#xA;&lt;p&gt;The TwoKeyClick function enables sending up to 6 characters when two keys are pressed simultaneously. For example pressing &lt;code&gt;A+S&lt;/code&gt; at the same time can send &lt;code&gt;Enter&lt;/code&gt; or pressing &lt;code&gt;J+K&lt;/code&gt; can send &lt;code&gt;ESC&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/twokeyclick-editor.png&#34; title=&#34;TwoKeyClick Editor&#34; alt=&#34;TwoKeyClick Editor window&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;During my testing I found this hard to trigger since the keys have to be pressed at exactly the same time. There’s a workaround by using the Burst Function to send two chars with a single key press and then using those same two chars to run the TwoKeyClick function.&lt;/p&gt;&#xA;&lt;h2 id=&#34;profiles&#34;&gt;Profiles&lt;/h2&gt;&#xA;&lt;p&gt;A profile is a mapping of keys. For example the &lt;code&gt;A&lt;/code&gt; key could output a different char (or run a function) in each profile. Each side of the keyboard has 4 profiles (Default Profile and Profile 1-3).&lt;/p&gt;&#xA;&lt;p&gt;The LEDs on the left side of the keyboard show which profile is currently active. On the right side of the keyboard the background light of the F11 and F12 keys shows the currently active profile.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/layer-2-active.jpg&#34; title=&#34;Keyboard Profile 1 is active&#34; alt=&#34;Keyboard LED when Profile 1 is active&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;I found this function useful to try out different keyboard layouts such as Dvorak, Colemak and Workman.&lt;/p&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;Overall I’m happy with the keyboard. I’ve been using it daily for the past 7 months. For a split keyboard it’s cheap (compared to ErgoDox) and built very well. I’m sure it will last a long time. The software is easy to use and allows each key to be assigned a different char (for example to use a different keyboard layout) or use one of the built-in functions to enable extra keys or shortcuts.&lt;/p&gt;&#xA;&lt;p&gt;Pros:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Good price for a split-keyboard&lt;/li&gt;&#xA;&lt;li&gt;Great build quality&lt;/li&gt;&#xA;&lt;li&gt;Easily programmable with many functions to choose from&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Cons:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Software is only available for Windows&lt;/li&gt;&#xA;&lt;li&gt;Each side has to be connected separately to be programmed&lt;/li&gt;&#xA;&lt;li&gt;Can’t assign a symbol such as &lt;code&gt;_&lt;/code&gt; or &lt;code&gt;{&lt;/code&gt; to a key (always requires holding down the shift key)&lt;/li&gt;&#xA;&lt;li&gt;Can’t switch to another profile temporarily when holding down a key&lt;/li&gt;&#xA;&lt;li&gt;Media Keys don’t work (play/pause/next/prev)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The cons are only minor issues for me and I’ll keep using the keyboard in the future.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/keyboard-review-smartyao-ly092/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Using QEMU to create a Ubuntu 20.04 Desktop VM on macOS</title>
    <updated>2020-09-20T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-09-20:/qemu-ubuntu-20-04/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;In this blog post we’re going to create a Ubuntu 20.04 VM using &lt;a href=&#34;https://www.qemu.org/&#34;&gt;QEMU&lt;/a&gt; on MacOS.&lt;/p&gt;&#xA;&lt;p&gt;Note for users on macOS 11.0: follow &lt;a href=&#34;https://www.arthurkoziel.com/qemu-on-macos-big-sur/&#34;&gt;this post&lt;/a&gt; first to get qemu to run.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/qemu-ubuntu-20-04/ubuntu-20-04-with-qemu.png&#34; alt=&#34;A picture of the Ubuntu 20.04 Desktop&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;QEMU is a hardware emulator which can make use of different accelerators when running VMs. The most popular accelerator is &lt;a href=&#34;https://www.linux-kvm.org/page/Main_Page&#34;&gt;KVM&lt;/a&gt; which is built into the Linux kernel and allows Linux hosts to run VMs with native performance.&lt;/p&gt;&#xA;&lt;p&gt;Using QEMU on macOS used to be very slow as no accelerator was available. This changed 2 years ago when the project &lt;a href=&#34;https://wiki.qemu.org/ChangeLog/2.12&#34;&gt;added support&lt;/a&gt; for the macOS native hypervisor with Hypervisor.framework (HVF) as an accelerator.&lt;/p&gt;&#xA;&lt;p&gt;Before we begin with the setup I assume that the &lt;a href=&#34;https://releases.ubuntu.com/20.04/&#34;&gt;Ubuntu 20.04 Desktop ISO&lt;/a&gt; has been downloaded in the current working directory.&lt;/p&gt;&#xA;&lt;h2 id=&#34;qemu-installation&#34;&gt;QEMU Installation&lt;/h2&gt;&#xA;&lt;p&gt;We can use Homebrew to install QEMU. The version we’re using in this tutorial is 5.1.0:&lt;/p&gt;&#xA;&lt;pre class=&#34;shell&#34;&gt;&lt;code&gt;$ brew install qemu&#xA;&#xA;qemu-system-x86_64 --version&#xA;QEMU emulator version 5.1.0&#xA;Copyright (c) 2003-2020 Fabrice Bellard and the QEMU Project developers&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;It will pull in a few dependencies (the package depends on 14 other packages) and the installation can take a few minutes.&lt;/p&gt;&#xA;&lt;h2 id=&#34;create-the-disk-image&#34;&gt;Create the disk image&lt;/h2&gt;&#xA;&lt;p&gt;Once the installation is done, we can create the disk image that we’re going to install Ubuntu on.&lt;/p&gt;&#xA;&lt;p&gt;We’re using the QCOW2 format to create a 20GB image. This can be resized later on if needed. The Ubuntu installation took around 5GB of space when I installed it.&lt;/p&gt;&#xA;&lt;pre class=&#34;shell&#34;&gt;&lt;code&gt;qemu-img create -f qcow2 ubuntu-20.04.1-desktop-amd64.qcow2 20G&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;boot-machine-with-ubuntu-iso-mounted&#34;&gt;Boot machine with Ubuntu ISO mounted&lt;/h2&gt;&#xA;&lt;p&gt;We can now boot up the machine with the Ubuntu ISO attached as a&lt;/p&gt;&#xA;&lt;p&gt;In this step we boot up the machine with the Ubuntu ISO mounted in the CD drive:&lt;/p&gt;&#xA;&lt;pre class=&#34;shell&#34;&gt;&lt;code&gt;qemu-system-x86_64 \&#xA;    -machine type=q35,accel=hvf \&#xA;    -smp 2 \&#xA;    -hda ubuntu-20.04.1-desktop-amd64.qcow2 \&#xA;    -cdrom ./ubuntu-20.04.1-desktop-amd64.iso \&#xA;    -m 4G \&#xA;    -vga virtio \&#xA;    -usb \&#xA;    -device usb-tablet \&#xA;    -display default,show-cursor=on&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The options are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;-machine&lt;/code&gt;: The emulated machine and the accelerator. q35 is the newest machine type and HVF is the macOS native hypervisor.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-smp&lt;/code&gt;: Number of CPUs to use&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-m&lt;/code&gt;: Amount of memory to use&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-hda&lt;/code&gt;: Disk drive (the one we created earlier)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-cdrom&lt;/code&gt;: The ISO image to put into the CD drive&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-vga&lt;/code&gt;: The graphic card to use. I found &lt;code&gt;virtio&lt;/code&gt; (based on &lt;a href=&#34;https://virgil3d.github.io/&#34;&gt;Virgil&lt;/a&gt; to have the best performance&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-usb&lt;/code&gt;: Enable USB host controller&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-device&lt;/code&gt; Adding a “usb-tablet” as an input device. I’m running this on a laptop and without this setting the mouse did not work.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-display&lt;/code&gt;: To show the mouse cursor (disabled by default)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;During testing I had problems with the Linux kernel as it would panic during the boot process. The issue was the &lt;code&gt;-cpu&lt;/code&gt; parameter. I fixed it by specifying the CPU architecture manually (see &lt;code&gt;qemu-system-x86_64 -cpu help&lt;/code&gt; for a list of all available architectures).&lt;/p&gt;&#xA;&lt;p&gt;My machine has an IvyBridge processor (Core i7):&lt;/p&gt;&#xA;&lt;pre class=&#34;shell&#34;&gt;&lt;code&gt;$ sysctl -n machdep.cpu.brand_string&#xA;&#xA;Intel(R) Core(TM) i7-3740QM CPU @ 2.70GHz&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And using &lt;code&gt;-cpu IvyBridge&lt;/code&gt; would fail. However when using &lt;code&gt;-cpu Nehalem&lt;/code&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Intel_CPU_microarchitectures&#34;&gt;also an i7 CPU&lt;/a&gt;) everything worked well.&lt;/p&gt;&#xA;&lt;p&gt;Now after the machine is booted up the Ubuntu installer will run. Follow the installation steps and don’t restart the VM at the end of the installation, instead shut it down by stopping the qemu process with CTRL-C on the host.&lt;/p&gt;&#xA;&lt;h2 id=&#34;boot-without-iso-mounted&#34;&gt;Boot without ISO mounted&lt;/h2&gt;&#xA;&lt;p&gt;When running the VM we don’t need the Ubuntu ISO mounted and can remove it by leaving out the &lt;code&gt;-cdrom&lt;/code&gt; option:&lt;/p&gt;&#xA;&lt;pre class=&#34;shell&#34;&gt;&lt;code&gt;qemu-system-x86_64 \&#xA;    -machine type=q35,accel=hvf \&#xA;    -smp 2 \&#xA;    -hda ubuntu-20.04.1-desktop-amd64.qcow2 \&#xA;    -m 4G \&#xA;    -vga virtio \&#xA;    -usb \&#xA;    -device usb-tablet \&#xA;    -display default,show-cursor=on&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;In my experience QEMU is faster, more responsive and uses less CPU/RAM than VirtualBox. I didn’t have to configure any display scaling for HiDPI screens as it worked out of the box. The only thing I’m missing are shared clipboards and drag-and-drop of files (which are available when installing the VirtualBox Guest Additions).&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/qemu-ubuntu-20-04/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Using the VBoxManage CLI to create a Ubuntu 20.04 VM</title>
    <updated>2020-09-11T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-09-11:/vboxmanage-cli-ubuntu-20-04/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;&#x9;&lt;p&gt;This tutorial describes the installation of Ubuntu 20.04 in VirtualBox by using the &lt;a href=&#34;https://www.virtualbox.org/manual/ch08.html&#34;&gt;VBoxManage CLI&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#x9;&lt;img src=&#34;https://www.arthurkoziel.com/vboxmanage-cli-ubuntu-20-04/ubuntu-desktop.png&#34; alt=&#34;Ubuntu 20.04 Desktop&#34;/&gt;&#xA;&#xA;&#x9;&lt;p&gt;I&amp;#39;m assuming that &lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;VirtualBox 6.1&lt;/a&gt; is installed, the VBoxManage CLI is ready to use and the &lt;a href=&#34;https://ubuntu.com/download/desktop&#34;&gt;Ubuntu 20.04 ISO&lt;/a&gt; has been downloaded.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I&amp;#39;ve tested this on an apple laptop with retina (HiDPI) display and included instructions to scale the display output. The scaling commands can be ignored for non-HiDPI displays.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Getting started&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We first create the VM and increase the hardware resources. The default settings use 1 CPU core, 128MB RAM and 8MB VRAM which is not enough. We&amp;#39;ll increase it to use 2 CPU cores, 4GB RAM and 128MB VRAM:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage createvm --name &amp;#34;Ubuntu 20.04&amp;#34; --ostype Ubuntu_64 --register&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage modifyvm &amp;#34;Ubuntu 20.04&amp;#34; --cpus 2 --memory 4096 --vram 128 --graphicscontroller vmsvga --usbohci on --mouse usbtablet&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In the above command I&amp;#39;ve also enabled USB and set the Graphics Controller to VMSVGA which is the default controller when using the GUI but set to the legacy VBoxVGA when using the CLI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To see all available options run &lt;code&gt;VBoxManage modifyvm&lt;/code&gt; which will output a list of all flags. The current values can be seen with &lt;code&gt;VBoxManage showvminfo &amp;#34;Ubuntu 20.04&amp;#34;&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Storage Devices&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Next we need to configure the storage devices. We create an empty 20GB hard drive that we can install Ubuntu on. A minimal Ubuntu installation takes about 5GB of space:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage createhd --filename ~/VirtualBox\ VMs/Ubuntu\ 20.04/Ubuntu\ 20.04.vdi --size 20480 --variant Standard&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storagectl &amp;#34;Ubuntu 20.04&amp;#34; --name &amp;#34;SATA Controller&amp;#34; --add sata --bootable on&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storageattach &amp;#34;Ubuntu 20.04&amp;#34; --storagectl &amp;#34;SATA Controller&amp;#34; --port 0 --device 0 --type hdd --medium ~/VirtualBox\ VMs/Ubuntu\ 20.04/Ubuntu\ 20.04.vdi &#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We also create a dvd drive with the Ubuntu ISO mounted in it. The VM will boot from this drive first when starting:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storagectl &amp;#34;Ubuntu 20.04&amp;#34; --name &amp;#34;IDE Controller&amp;#34; --add ide&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storageattach &amp;#34;Ubuntu 20.04&amp;#34; --storagectl &amp;#34;IDE Controller&amp;#34; --port 0 --device 0 --type dvddrive --medium ~/Downloads/ubuntu-20.04.1-desktop-amd64.iso&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Display output scaling&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The Ubuntu installer runs with a resolution of 800x600 and would look tiny on HiDPI screens if we wouldn&amp;#39;t scale up the display output. The following command will scale up the display output by a factor of 2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage setextradata &amp;#34;Ubuntu 20.04&amp;#34; GUI/ScaleFactor 2&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Start the VM&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Now we can start the VM which will load the Ubuntu GUI installer from the mounted ISO:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage startvm &amp;#34;Ubuntu 20.04&amp;#34;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After the installation is done the installer will prompt to remove the installation media. We can ignore this for now and press enter to reboot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When Ubuntu is booted up we install the VirtualBox Guest Additions to get the display drivers. This can be done by clicking on the menu bar and selecting &amp;#34;Devices&amp;#34;, then  &amp;#34;Insert Guest Additions CD Image&amp;#34;. The installer will start automatically.&lt;/p&gt; &#xA;&#xA;&lt;h2&gt;Finishing the installation&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;After the Guest Additions are installed we can let GNOME handle the display output scaling which has a better quality.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the Ubuntu terminal (right click on Desktop and select &amp;#34;Open in Terminal&amp;#34;) enter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; gsettings set org.gnome.desktop.interface scaling-factor 2&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can now shut down the VM to remove the installation media and disable the VirtualBox display scaling. On the Desktop click on the arrow in the top right corner and select &amp;#34;Power Off&amp;#34;. Then run the following command on the host:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storageattach &amp;#34;Ubuntu 20.04&amp;#34; --storagectl &amp;#34;IDE Controller&amp;#34; --port 0 --device 0 --type dvddrive --medium emptydrive&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage setextradata &amp;#34;Ubuntu 20.04&amp;#34; GUI/ScaleFactor 1&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With that done the Ubuntu VM is ready to use.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;&#xA;We created a Ubuntu 20.04 VM by using the CLI instead of the GUI wizard. Personally I find this useful since I sometimes forget to set values in the GUI (such as increasing the VRAM). The Ubuntu Desktop installer has to run in a GUI, to automate this we&amp;#39;d need to use the Ubuntu Server ISO and install the GNOME desktop manually.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/vboxmanage-cli-ubuntu-20-04/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Listing Images and Tags in GCR</title>
    <updated>2020-06-02T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-06-02:/listing-images-and-tags-in-gcr/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;&#xA;In this blog post I&amp;#39;m going to describe how to use the &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;GCR&lt;/a&gt; Docker Registry API to list all Docker images and tags in the registry. An installation of the Docker daemon is not needed (we&amp;#39;re going to use a service account).&#xA;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Authentication Flow&lt;/h2&gt;&#xA;&lt;p&gt;In the next sections we&amp;#39;ll walk through the authentication and authorization with the Google Auth Server and the GCR Docker registry.&lt;/p&gt;&#xA;&lt;p&gt;The whole process looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                 +-------------+ +-----+&#xA;| Client  |                                 | GoogleAuth  | | GCR |&#xA;+---------+                                 +-------------+ +-----+&#xA;     |                                             |           |&#xA;     | 1) Authenticate (needs Service Account)     |           |&#xA;     |--------------------------------------------&amp;gt;|           |&#xA;     |                                             |           |&#xA;     |                         OAuth2 Access Token |           |&#xA;     |&amp;lt;--------------------------------------------|           |&#xA;     |                                             |           |&#xA;     | 2) Get JWT token (needs Access Token)       |           |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                             |           |&#xA;     |                                             | JWT Token |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;     |                                             |           |&#xA;     | 3) Get Catalog/Tags (needs JWT token)       |           |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                             |           |&#xA;     |                                           Image Catalog |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;     |                                             |           |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We&amp;#39;ll later make this process shorter by using the service account key for authentication which allows us to skip steps 1 and 2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 3-step process shown above is longer but makes it possible to use more restricted permissions on the service account. This is not possible when using the service account key for authentication.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating the Service Account&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Before we go into step 1 we need to create a service account that is able to authenticate with the Google Authentication Server and get authorization to fetch the necessary information from the Docker registry.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To create the service account and a JSON keyfile we run the following command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# create service account&lt;/span&gt;&#xA;$ gcloud iam service-accounts create gcr-svc-acc&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# create JSON keyfile&lt;/span&gt;&#xA;$ gcloud iam service-accounts keys create gcr-svc-acc-keyfile.json --iam-account gcr-svc-acc@&amp;lt;your-project&amp;gt;.iam.gserviceaccount.com&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We should now have a file called &lt;code&gt;gcr-svc-acc-keyfile.json&lt;/code&gt; that we can later use to get the OAuth2 Access Token, but first we need to grant the service account permissions to access resources in the GCP project.&lt;/p&gt;&#xA;&lt;p&gt;In our use case we need to grant it the following roles:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;Project Browser&lt;/li&gt;&#xA;    &lt;li&gt;Storage Object Viewer on the GCS bucket for the container registry (the bucket is automatically created for each project and named &lt;code&gt;artifacts.&amp;lt;your-project&amp;gt;.appspot.com&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Note: If you don&amp;#39;t want to list all images in the registry (only the tags for an image) you don&amp;#39;t have to grant the Project Browser role.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# grant &amp;#34;Project Browser&amp;#34; role&lt;/span&gt;&#xA;$ gcloud projects add-iam-policy-binding &amp;lt;your-project&amp;gt; --member=&amp;#34;serviceAccount:gcr-svc-acc@&amp;lt;your-project&amp;gt;.iam.gserviceaccount.com&amp;#34; --role=&amp;#34;roles/browser&amp;#34;&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# grant &amp;#34;Storage Object&amp;#34; Viewer role for GCR bucket&lt;/span&gt;&#xA;$ gsutil iam ch serviceAccount:gcr-svc-acc@&amp;lt;your-project&amp;gt;.iam.gserviceaccount.com:objectViewer gs://artifacts.&amp;lt;your-project&amp;gt;.appspot.com/&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Next we need to enable the Cloud Resource Manager API. You can skip this if you only need the image tags.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ gcloud services enable cloudresourcemanager.googleapis.com&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We are now ready to use our service account to get an OAuth2 Access Token from the Google Authorization Server.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Step 1: Getting the OAuth2 Access Token&lt;/h2&gt;&#xA;&lt;pre&gt;+---------+                                 +-------------+&#xA;| Client  |                                 | GoogleAuth  |&#xA;+---------+                                 +-------------+&#xA;     |                                             |&#xA;     | 1) Authenticate (needs Service Account)     |&#xA;     |--------------------------------------------&amp;gt;|&#xA;     |                                             |&#xA;     |                         OAuth2 Access Token |&#xA;     |&amp;lt;--------------------------------------------|&#xA;     |                                             |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The OAuth2 Access Token is required to access data over Google APIs. In our case we want to access the images and tags from the GCR Docker Registry API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obtaining this token by directly communicating via HTTP/REST is &lt;a href=&#34;https://developers.google.com/identity/protocols/oauth2/service-account#authorizingrequests&#34;&gt;complicated&lt;/a&gt; but fortunately there are client libraries available which let us abstract the cryptography parts away.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I&amp;#39;m going to use &lt;a href=&#34;https://github.com/google/oauth2l&#34;&gt;oauth2l&lt;/a&gt; CLI tool.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When requesting the access token we need to pass in a scope that specifies the level of access we need. In our case that&amp;#39;s the &lt;code&gt;cloud-platform.read-only&lt;/code&gt; scope, or the &lt;code&gt;devstorage.read_only&lt;/code&gt; scope if you only want the image tags.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;TODO: restricted&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We run the following command to get the access token:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ export ACCESS_TOKEN=$(oauth2l fetch --credentials gcr-svc-acc-keyfile.json --scope cloud-platform.read-only --cache=&amp;#34;&amp;#34;)&#xA;&#xA;$ echo $ACCESS_TOKEN&#xA;ya29.c.Ko9BzQetXXzZ6mOZTg71LdmqabQx...&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With the OAuth2 Access Token saved we can continue with the Docker registry authentication.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Step 2: Docker Registry JWT token&lt;/h2&gt;&#xA;&lt;pre&gt;+---------+                                                 +-----+&#xA;| Client  |                                                 | GCR |&#xA;+---------+                                                 +-----+&#xA;     |                                                         |&#xA;     | 2) Get JWT token (needs OAuth2 Access Token)            |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                                         |&#xA;     |                                               JWT Token |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For communication with the Docker registry we need to obtain a JWT token that authorizes us to access the data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depending on which data we want to request from the Docker registry we have to use a different scope for the JWT token:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;code&gt;registry:catalog:*&lt;/code&gt; scope to get the list of images&lt;/li&gt;&#xA;    &lt;li&gt;&lt;code&gt;repository:&amp;lt;image_name&amp;gt;:pull&lt;/code&gt; scope to get the tags for an image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To get this token we have to make a request to the gcr.io token server with the username &lt;code&gt;_token&lt;/code&gt; and the previously obtained OAuth2 access token as the password:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# Get JWT token with &amp;#34;registry:catalog:*&amp;#34; scope&lt;/span&gt;&#xA;$ export JWT_TOKEN=$(curl -sSL &amp;#34;https://gcr.io/v2/token?service=gcr.io&amp;amp;scope=registry:catalog:*&amp;#34; -u _token:$ACCESS_TOKEN | jq --raw-output &amp;#39;.token&amp;#39;)&#xA;&#xA;$ echo $JWT_TOKEN&#xA;AJAD5v14fnm+N...&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With the JWT token we can use the &lt;a href=&#34;https://docs.docker.com/registry/spec/api/&#34;&gt;Docker Registry HTTP API&lt;/a&gt; to list all images in the registry.&#xA;&#xA;&lt;/p&gt;&lt;h2&gt;Step 3: Get the image catalog and tags&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                                 +-----+&#xA;| Client  |                                                 | GCR |&#xA;+---------+                                                 +-----+&#xA;     |                                                         |&#xA;     | 3) Get Catalog/Tags (needs JWT token)                   |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                                         |&#xA;     |                                      Image Catalog/Tags |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;     |                                                         |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To get a list of all docker images in the registry we use the &lt;code&gt;&lt;a href=&#34;https://docs.docker.com/registry/spec/api/#listing-repositories&#34;&gt;_catalog&lt;/a&gt;&lt;/code&gt; endpoint. In the HTTP request we have to pass the JWT token in the &lt;code&gt;Authorization&lt;/code&gt; header:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -H &amp;#34;Authorization: Bearer $JWT_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/_catalog&amp;#34; | jq&#xA;&#xA;{&#xA;  &amp;#34;repositories&amp;#34;: [&#xA;    &amp;#34;project/alpine&amp;#34;,&#xA;    &amp;#34;project/busybox&amp;#34;&#xA;  ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The registry has the &amp;#34;project/alpine&amp;#34; and &amp;#34;project/busybox&amp;#34; images in it (which I pushed there before writing this tutorial). The project name is always part of the image name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next we can pick an image and list the tags. For this we can use the &lt;code&gt;&lt;a href=&#34;https://docs.docker.com/registry/spec/api/#listing-image-tags&#34;&gt;&amp;lt;name&amp;gt;/tags/list&lt;/a&gt;&lt;/code&gt; endpoint. But before we can do this, we need a new JWT token.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Our previously created token (with the &lt;code&gt;registry:catalog:*&lt;/code&gt; scope) doesn&amp;#39;t have the permission to get this information. If we try to send a request it will respond with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -H &amp;#34;Authorization: Bearer $JWT_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/project/alpine/tags/list&amp;#34; | jq&#xA;&#xA;{&#xA;  &amp;#34;errors&amp;#34;: [&#xA;    {&#xA;      &amp;#34;code&amp;#34;: &amp;#34;UNAUTHORIZED&amp;#34;,&#xA;      &amp;#34;message&amp;#34;: &amp;#34;Requested repository does not match bearer token resource: project/alpine&amp;#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The scope we need is called &lt;code&gt;repository:&amp;lt;image_name&amp;gt;:pull&lt;/code&gt;. The &lt;code&gt;image_name&lt;/code&gt; includes the GCP project name. In the following example we&amp;#39;re going to generate a new token and list the tags for the &lt;code&gt;project/alpine&lt;/code&gt; image:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# request a new JWT token&lt;/span&gt;&#xA;export JWT_TOKEN=$(curl -sSL &amp;#34;https://gcr.io/v2/token?service=gcr.io&amp;amp;scope=repository:&amp;lt;your-project&amp;gt;/alpine:pull&amp;#34; -u _token:$ACCESS_TOKEN | jq --raw-output &amp;#39;.token&amp;#39;)&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# fetch list of tags&lt;/span&gt;&#xA;curl -sSL -H &amp;#34;Authorization: Bearer $JWT_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/&amp;lt;your-project&amp;gt;/alpine/tags/list&amp;#34; | jq &amp;#39;.tags&amp;#39;&#xA;&#xA;[&#xA;  &amp;#34;3.11&amp;#34;&#xA;]&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Next we&amp;#39;re going to look into making this process shorter. GCR which let us skip the request for the OAuth2 Access Token and the request for the JWT token by using different users.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Skipping the Docker Registry JWT&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The GCR Docker registry has a user called &lt;code&gt;oauth2accesstoken&lt;/code&gt; that lets us send the OAuth2 access token to the Docker Registry without having to obtain the JWT token. The process can be reduced to 2 steps and looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                    +-------------+ +-----+&#xA;| Client  |                                    | GoogleAuth  | | GCR |&#xA;+---------+                                    +-------------+ +-----+&#xA;     |                                                |           |&#xA;     | 1) Authenticate (needs Service Account)        |           |&#xA;     |-----------------------------------------------&amp;gt;|           |&#xA;     |                                                |           |&#xA;     |                            OAuth2 Access Token |           |&#xA;     |&amp;lt;-----------------------------------------------|           |&#xA;     |                                                |           |&#xA;     | 2) Get Image Catalog (needs Access Token)      |           |&#xA;     |-----------------------------------------------------------&amp;lt;|&#xA;     |                                                |           |&#xA;     |                                              Image Catalog |&#xA;     |&amp;lt;-----------------------------------------------------------|&#xA;     |                                                |           |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The command to get the list of image tags with the access token is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -u &amp;#34;oauth2accesstoken:$ACCESS_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/&amp;lt;your-project&amp;gt;/alpine/tags/list | jq &amp;#39;.tags&amp;#39;&amp;#34;&#xA;&#xA;[&#xA;  &amp;#34;3.11&amp;#34;&#xA;]&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;JSON User&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can also use the &lt;code&gt;_json_key&lt;/code&gt; user to authenticate with the GCR Docker registry. It lets us save another request by not having to obtain the OAuth2 access token. The process of getting data from the GCR Docker registry can then be reduced to a single request and looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                      +-----+&#xA;| Client  |                                      | GCR |&#xA;+---------+                                      +-----+&#xA;     |                                              |&#xA;     | Get Image Catalog (needs Service Account)    |&#xA;     |---------------------------------------------&amp;gt;|&#xA;     |                                              |&#xA;     |                                Image Catalog |&#xA;     |&amp;lt;---------------------------------------------|&#xA;     |                                              |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When using the &lt;code&gt;_json_key&lt;/code&gt; user we have to pass in the content of the service account json keyfile as the password:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -u &amp;#34;_json_key:$(cat gcr-svc-acc-keyfile.json)&amp;#34; &amp;#34;https://gcr.io/v2/&amp;lt;your-project&amp;gt;/alpine/tags/list&amp;#34; | jq &amp;#39;.tags&amp;#39;&#xA;&#xA;[&#xA;  &amp;#34;3.11&amp;#34;&#xA;]&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;In this tutorial we&amp;#39;ve walked through the process of obtaining an OAuth2 access token from the Google Auth Server and a JWT token from the GCR Docker registry. We&amp;#39;ve then used the Docker registry API to fetch a list of images and tags for a specific image.&#xA;&#xA;&lt;/p&gt;&lt;p&gt;We&amp;#39;ve seen three different authentication methods by using either the &lt;code&gt;_token&lt;/code&gt;, &lt;code&gt;oauth2accesstoken&lt;/code&gt; or &lt;code&gt;_json_key&lt;/code&gt; users.&lt;/p&gt;&#xA;&lt;p&gt;Using the &lt;code&gt;_token&lt;/code&gt; user involves more requests but is the same process that will work for all Docker registries. Depending on if we need to fetch the list of images we can also restrict the service account permissions by not having to grant it the &lt;code&gt;Project Browser&lt;/code&gt; role, not having to enable the Cloud Resource Manager API and using the &lt;code&gt;devstorage.read_only&lt;/code&gt; scope instead of the &lt;code&gt;cloud-platform.read-only&lt;/code&gt; scope for the OAuth2 Access Key.&#xA;&#xA;    &lt;/p&gt;&lt;p&gt;Using the &lt;code&gt;oauth2accesstoken&lt;/code&gt; or &lt;code&gt;_json_key&lt;/code&gt; users is more convenient but only works with the GCR Docker registry and requires project-level permissions for the service account.&lt;/p&gt;&#xA;&#xA;    &lt;p&gt;The example code available in &lt;a href=&#34;https://gist.github.com/arthurk/ab9ced56ce78bb8309599ccc62fa2576&#34;&gt;this gist&lt;/a&gt;.&#xA;&#xA;    &lt;/p&gt;</content>
    <link href="https://www.arthurkoziel.com/listing-images-and-tags-in-gcr/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>DockerHub Docker Registry API Examples</title>
    <updated>2020-05-10T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-05-10:/dockerhub-registry-api/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        This post contains examples of REST API calls to &lt;a href=&#34;https://hub.docker.com&#34;&gt;DockerHub&lt;/a&gt;&#xA;        and the DockerHub &lt;a href=&#34;https://docs.docker.com/registry/spec/api/&#34;&gt;Docker Registry&lt;/a&gt;. We&amp;#39;re going to list all images&#xA;        for a user, list all tags for an image and get the manifest&#xA;        for an image.&#xA;        &lt;/p&gt;&#xA;&#xA;&#xA;        &lt;h2&gt;List public images&lt;/h2&gt;&#xA;        &lt;p&gt;We&amp;#39;re going to use the DockerHub API to get the list of images for a user.&#xA;        This is because the DockerHub Docker Registry does not implement the&#xA;        &lt;a href=&#34;https://docs.docker.com/registry/spec/api/#listing-repositories&#34;&gt;/v2/_catalog&lt;/a&gt; endpoint to list all repositories in the registry.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -s &amp;#34;https://hub.docker.com/v2/repositories/ansible/?page_size=100&amp;#34; | jq -r &amp;#39;.results|.[]|.name&amp;#39;&#xA;&#xA;awx_task&#xA;awx_web&#xA;awx_rabbitmq&#xA;ansible&#xA;centos7-ansible&#xA;ubuntu14.04-ansible&#xA;...&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;The DockerHub API is undocumented but there are projects out there like &lt;a href=&#34;https://github.com/RyanTheAllmighty/Docker-Hub-API&#34;&gt;this one&lt;/a&gt;&#xA;        who did a great job listing available endpoints.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;List private images&lt;/h2&gt;&#xA;        &lt;p&gt;To include private images we need to get an authentication token (JWT) which&#xA;        we can then include in subsequent requests:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export DOCKER_USERNAME=&amp;#34;myusername&amp;#34;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export DOCKER_PASSWORD=&amp;#34;mypassword&amp;#34;&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export TOKEN=$(curl -s -H &amp;#34;Content-Type: application/json&amp;#34; -X POST -d &amp;#39;{&amp;#34;username&amp;#34;: &amp;#34;&amp;#39;${DOCKER_USERNAME}&amp;#39;&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;&amp;#39;${DOCKER_PASSWORD}&amp;#39;&amp;#34;}&amp;#39; https://hub.docker.com/v2/users/login/ | jq -r .token)&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -s -H &amp;#34;Authorization: JWT ${TOKEN}&amp;#34; &amp;#34;https://hub.docker.com/v2/repositories/arthurk/?page_size=100&amp;#34; | jq -r &amp;#39;.results|.[]|.name&amp;#39;&#xA;&#xA;my-private-repo&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;List tags&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;We need to get an authentication token for the Docker Registry. Note that&#xA;        the JWT from the previous step does not work here. DockerHub and the DockerHub Docker Registry&#xA;        are different services and require different authentication credentials.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export AUTH_SERVICE=&amp;#39;registry.docker.io&amp;#39;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export AUTH_SCOPE=&amp;#34;repository:ansible/ansible:pull&amp;#34;&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export TOKEN=$(curl -fsSL &amp;#34;https://auth.docker.io/token?service=$AUTH_SERVICE&amp;amp;scope=$AUTH_SCOPE&amp;#34; | jq --raw-output &amp;#39;.token&amp;#39;)&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -fsSL \&#xA;    -H &amp;#34;Authorization: Bearer $TOKEN&amp;#34; \&#xA;    &amp;#34;$REGISTRY_URL/v2/ansible/ansible/tags/list&amp;#34; | jq&#xA;{&#xA;  &amp;#34;name&amp;#34;: &amp;#34;ansible/ansible&amp;#34;,&#xA;  &amp;#34;tags&amp;#34;: [&#xA;    &amp;#34;centos6&amp;#34;,&#xA;    &amp;#34;centos7&amp;#34;,&#xA;    &amp;#34;cloudstack-simulator&amp;#34;,&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Get image manifest&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;We re-use the token from the previous step to make a&#xA;        request that gets the manifest for the &lt;code&gt;ansible:centos7&lt;/code&gt; image:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -fsSL \&#xA;    -H &amp;#34;Authorization: Bearer $TOKEN&amp;#34; \&#xA;    &amp;#34;$REGISTRY_URL/v2/ansible/ansible/manifests/centos7&amp;#34; | jq&#xA;&#xA;{&#xA;  &amp;#34;schemaVersion&amp;#34;: 1,&#xA;  &amp;#34;name&amp;#34;: &amp;#34;ansible/ansible&amp;#34;,&#xA;  &amp;#34;tag&amp;#34;: &amp;#34;centos7&amp;#34;,&#xA;  &amp;#34;architecture&amp;#34;: &amp;#34;amd64&amp;#34;,&#xA;  &amp;#34;fsLayers&amp;#34;: [&#xA;    {&#xA;      &amp;#34;blobSum&amp;#34;: &amp;#34;sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4&amp;#34;&#xA;    },&#xA;    ...&#xA;  ],&#xA;  &amp;#34;history&amp;#34;: [...],&#xA;  &amp;#34;signatures&amp;#34;: [...]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Skopeo&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&lt;a href=&#34;https://github.com/containers/skopeo&#34;&gt;Skopeo&lt;/a&gt; is a CLI tool that makes it easy to quickly check information&#xA;        about docker images such as the available tags:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; skopeo inspect docker://ansible/galaxy&#xA;{&#xA;    &amp;#34;Name&amp;#34;: &amp;#34;docker.io/ansible/galaxy&amp;#34;,&#xA;    &amp;#34;Digest&amp;#34;: &amp;#34;sha256:24d0b67d936ca6e7bca253169fc268748d7585c0cee723c14e8e51f37cfd3591&amp;#34;,&#xA;    &amp;#34;RepoTags&amp;#34;: [&#xA;        &amp;#34;2.3.0&amp;#34;,&#xA;        &amp;#34;2.3.1&amp;#34;,&#xA;        &amp;#34;2.4.0&amp;#34;,&#xA;        &amp;#34;3.0.0&amp;#34;,&#xA;        &amp;#34;3.0.1&amp;#34;,&#xA;        ...&#xA;        &amp;#34;3.1.6&amp;#34;,&#xA;        &amp;#34;3.1.7&amp;#34;,&#xA;        &amp;#34;3.1.8&amp;#34;,&#xA;        &amp;#34;develop&amp;#34;,&#xA;        &amp;#34;latest&amp;#34;&#xA;    ],&#xA;    &amp;#34;Created&amp;#34;: &amp;#34;2019-03-15T09:24:18.817740071Z&amp;#34;,&#xA;    &amp;#34;DockerVersion&amp;#34;: &amp;#34;17.09.0-ce&amp;#34;,&#xA;    &amp;#34;Labels&amp;#34;: {&#xA;        &amp;#34;org.label-schema.build-date&amp;#34;: &amp;#34;20190305&amp;#34;,&#xA;        &amp;#34;org.label-schema.license&amp;#34;: &amp;#34;GPLv2&amp;#34;,&#xA;        &amp;#34;org.label-schema.name&amp;#34;: &amp;#34;CentOS Base Image&amp;#34;,&#xA;        &amp;#34;org.label-schema.schema-version&amp;#34;: &amp;#34;1.0&amp;#34;,&#xA;        &amp;#34;org.label-schema.vendor&amp;#34;: &amp;#34;CentOS&amp;#34;&#xA;    },&#xA;    &amp;#34;Architecture&amp;#34;: &amp;#34;amd64&amp;#34;,&#xA;    &amp;#34;Os&amp;#34;: &amp;#34;linux&amp;#34;,&#xA;    &amp;#34;Layers&amp;#34;: [&#xA;        &amp;#34;sha256:8ba884070f611d31cb2c42eddb691319dc9facf5e0ec67672fcfa135181ab3df&amp;#34;,&#xA;        ...&#xA;    ],&#xA;    &amp;#34;Env&amp;#34;: [&#xA;        &amp;#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&amp;#34;,&#xA;        &amp;#34;PIP_NO_CACHE_DIR=off&amp;#34;,&#xA;        &amp;#34;VENV_BIN=/var/lib/galaxy/venv/bin&amp;#34;,&#xA;        &amp;#34;TINI_VERSION=v0.16.1&amp;#34;,&#xA;        &amp;#34;HOME=/var/lib/galaxy&amp;#34;,&#xA;        &amp;#34;DJANGO_SETTINGS_MODULE=galaxy.settings.production&amp;#34;,&#xA;        &amp;#34;GIT_COMMITTER_NAME=Ansible Galaxy&amp;#34;,&#xA;        &amp;#34;GIT_COMMITTER_EMAIL=galaxy@ansible.com&amp;#34;&#xA;    ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/dockerhub-registry-api/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Creating CI Pipelines with Tekton (Part 2/2)</title>
    <updated>2020-05-03T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-05-03:/creating-ci-pipelines-with-tekton-part-2/</id>
    <content type="html">&#xA;        &#xA;        ,&#xA;        Updated &#xA;&#xA;        &lt;p&gt;&#xA;            In this blog post we&amp;#39;re going to continue&#xA;            creating a CI pipeline with &lt;a href=&#34;https://tekton.dev&#34;&gt;Tekton&lt;/a&gt;. In &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/&#34;&gt;Part 1&lt;/a&gt; we&#xA;            installed Tekton on a local &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; cluster and defined&#xA;            our first Task which clones a GitHub repository and runs application tests for a Go application (&lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;repo&lt;/a&gt;).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            In this part we&amp;#39;re going to create a Task&#xA;            that will build a Docker image for our Go application&#xA;            and push it to &lt;a href=&#34;https://hub.docker.com&#34;&gt;DockerHub&lt;/a&gt;. Afterward we will combine&#xA;            our tasks into a Pipeline.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Adding DockerHub Credentials&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        To build and push our Docker image we use &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;Kaniko&lt;/a&gt;, which&#xA;        can build Docker images inside a Kubernetes cluster without depending on a Docker daemon.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        Kaniko will build and push the image in the same command. This means before running our task we need to set up&#xA;        credentials for DockerHub so that the docker image can be pushed to the registry.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The credentials are saved in a Kubernetes Secret. Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/04-secret.yaml&#34;&gt;secret.yaml&lt;/a&gt;&lt;/code&gt; with&#xA;        the following content and replace &lt;code&gt;myusername&lt;/code&gt; and &lt;code&gt;mypassword&lt;/code&gt; with your DockerHub credentials:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: v1&#xA;kind: Secret&#xA;metadata:&#xA;  name: basic-user-pass&#xA;  annotations:&#xA;    tekton.dev/docker-0: https://index.docker.io/v1/&#xA;type: kubernetes.io/basic-auth&#xA;stringData:&#xA;    username: myusername&#xA;    password: mypassword&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note the &lt;code&gt;tekton.dev/docker-0&lt;/code&gt; annotation in the metadata&#xA;            which tells Tekton the Docker registry these credentials belong to.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Next we create a &lt;code&gt;ServiceAccount&lt;/code&gt; that uses the &lt;code&gt;basic-user-pass&lt;/code&gt; Secret. Create&#xA;        a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/05-serviceaccount.yaml&#34;&gt;serviceaccount.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: v1&#xA;kind: ServiceAccount&#xA;metadata:&#xA;  name: build-bot&#xA;secrets:&#xA;  - name: basic-user-pass&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Apply both files with kubectl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f secret.yaml&#xA;secret/basic-user-pass created&#xA;&#xA;$ kubectl apply -f serviceaccount.yaml&#xA;serviceaccount/build-bot created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now use this ServiceAccount (named &lt;code&gt;build-bot&lt;/code&gt;) when&#xA;            running Tekton tasks or pipelines by specifying a &lt;code&gt;serviceAccountName&lt;/code&gt;.&#xA;            We will see examples of this below.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating a Task to build and push a Docker image&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Now that the credentials are set up we can continue by creating&#xA;            the Task that will build and push the Docker image.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/06-task-build-push.yaml&#34;&gt;task-build-push.yaml&lt;/a&gt;&lt;/code&gt; with&#xA;            the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Task&#xA;metadata:&#xA;  name: build-and-push&#xA;spec:&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        type: git&#xA;  steps:&#xA;    - name: build-and-push&#xA;      image: gcr.io/kaniko-project/executor:v1.3.0&#xA;      env:&#xA;        - name: DOCKER_CONFIG&#xA;          value: /tekton/home/.docker&#xA;      command:&#xA;        - /kaniko/executor&#xA;        - --dockerfile=Dockerfile&#xA;        - --context=/workspace/repo/src&#xA;        - --destination=arthurk/tekton-test:latest&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Similarly to the first task this task takes a git repo as an input (the input name is &lt;code&gt;repo&lt;/code&gt;)&#xA;            and consists of only a single step since Kaniko builds and pushes the image in the same command.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Make sure to create a DockerHub repository and replace &lt;code&gt;arthurk/tekton-test&lt;/code&gt;&#xA;            with your repository name. In this example it will always tag and push the image&#xA;            with the &lt;code&gt;latest&lt;/code&gt; tag.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Tekton has support for&#xA;            &lt;a href=&#34;https://github.com/tektoncd/pipeline/blob/master/docs/pipelines.md#specifying-parameters&#34;&gt;parameters&lt;/a&gt;&#xA;            to avoid hardcoding values like this.&#xA;            However to keep this tutorial simple I&amp;#39;ve left them out.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            The &lt;code&gt;DOCKER_CONFIG&lt;/code&gt; env var is required for Kaniko to be able to &lt;a href=&#34;https://github.com/tektoncd/pipeline/pull/706&#34;&gt;find the Docker credentials&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Apply the file with kubectl:&lt;/p&gt;&#xA;&lt;pre&gt;$ kubectl apply -f task-build-push.yaml&#xA;task.tekton.dev/build-and-push created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            There are two ways we can test this Task, either by manually creating a TaskRun&#xA;            definition and then applying it with &lt;code&gt;kubectl&lt;/code&gt; or by using the Tekton CLI (&lt;code&gt;tkn&lt;/code&gt;).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In the following two sections I will show both methods.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Run the Task with kubectl&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            To run the Task with &lt;code&gt;kubectl&lt;/code&gt; we create a TaskRun that looks identical to the&#xA;            &lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/03-taskrun.yaml&#34;&gt;previous&lt;/a&gt; with the&#xA;            exception that we now specify a ServiceAccount (&lt;code&gt;serviceAccountName&lt;/code&gt;) to use when executing the Task.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/07-taskrun-build-push.yaml&#34;&gt;taskrun-build-push.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: TaskRun&#xA;metadata:&#xA;  name: build-and-push&#xA;spec:&#xA;  serviceAccountName: build-bot&#xA;  taskRef:&#xA;    name: build-and-push&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        resourceRef:&#xA;          name: arthurk-tekton-example&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Apply the task and check the log of the Pod by listing all Pods that start with the Task name &lt;code&gt;build-and-push&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f taskrun-build-push.yaml&#xA;taskrun.tekton.dev/build-and-push created&#xA;&#xA;$ kubectl get pods | grep build-and-push&#xA;build-and-push-pod-c698q   2/2     Running     0          4s&#xA;&#xA;$ kubectl logs --all-containers build-and-push-pod-c698q --follow&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588478267.3476844,&amp;#34;caller&amp;#34;:&amp;#34;creds-init/main.go:44&amp;#34;, &amp;#34;msg&amp;#34;:&amp;#34;Credentials initialized.&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588478279.2681644,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588478279.3249557,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;INFO[0004] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;INFO[0004] Retrieving image manifest golang:1.14-alpine&#xA;INFO[0012] Built cross stage deps: map[]&#xA;...&#xA;INFO[0048] Taking snapshot of full filesystem...&#xA;INFO[0048] Resolving paths&#xA;INFO[0050] CMD [&amp;#34;app&amp;#34;]&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;The task executed without problems and we can now pull/run our Docker image:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ docker run arthurk/tekton-test:latest&#xA;hello world&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Run the Task with the Tekton CLI&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Running the Task with the Tekton CLI is more convenient. With a single command&#xA;            it generates a TaskRun manifest from the Task definition, applies it, and follows the logs.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ tkn task start build-and-push --inputresource repo=arthurk-tekton-example --serviceaccount build-bot --showlog&#xA;Taskrun started: build-and-push-run-ctjvv&#xA;Waiting for logs to be available...&#xA;[git-source-arthurk-tekton-example-p9zxz] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588479279.271127,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[git-source-arthurk-tekton-example-p9zxz] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588479279.329212,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[build-and-push] INFO[0004] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;[build-and-push] INFO[0008] Retrieving image manifest golang:1.14-alpine&#xA;[build-and-push] INFO[0012] Built cross stage deps: map[]&#xA;...&#xA;[build-and-push] INFO[0049] Taking snapshot of full filesystem...&#xA;[build-and-push] INFO[0049] Resolving paths&#xA;[build-and-push] INFO[0051] CMD [&amp;#34;app&amp;#34;]&#xA;&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            What happens in the background is similar to what we did with kubectl in the&#xA;            previous section but this time we only have to run a single command.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating a Pipeline&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Now that we have both of our Tasks ready (test, build-and-push) we can create&#xA;        a Pipeline that will run them sequentially: First it will run the application&#xA;        tests and if they pass it will build the Docker image and push it to DockerHub.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/08-pipeline.yaml&#34;&gt;pipeline.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Pipeline&#xA;metadata:&#xA;  name: test-build-push&#xA;spec:&#xA;  resources:&#xA;    - name: repo&#xA;      type: git&#xA;  tasks:&#xA;    # Run application tests&#xA;    - name: test&#xA;      taskRef:&#xA;        name: test&#xA;      resources:&#xA;        inputs:&#xA;          - name: repo      # name of the Task input (see Task definition)&#xA;            resource: repo  # name of the Pipeline resource&#xA;&#xA;    # Build docker image and push to registry&#xA;    - name: build-and-push&#xA;      taskRef:&#xA;        name: build-and-push&#xA;      runAfter:&#xA;        - test&#xA;      resources:&#xA;        inputs:&#xA;          - name: repo      # name of the Task input (see Task definition)&#xA;            resource: repo  # name of the Pipeline resource&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The first thing we need to define is what resources our Pipeline requires. A resource&#xA;        can either be an input or an output. In our case we only have an input: the git repo with&#xA;        our application source code. We name the resource &lt;code&gt;repo&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Next we define our tasks. Each task has a &lt;code&gt;taskRef&lt;/code&gt; (a reference to a Task)&#xA;        and passes the tasks required inputs.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Apply the file with kubectl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f pipeline.yaml&#xA;pipeline.tekton.dev/test-build-push created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Similar to how we can run as Task by creating a TaskRun, we can run&#xA;            a Pipeline by creating a PipelineRun.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            This can either be done with kubectl or the Tekton CLI. In the following&#xA;            two sections I will show both ways.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Run the Pipeline with kubectl&#xA;        &lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To run the file with kubectl we have to create a PipelineRun.&#xA;        Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/09-pipelinerun.yaml&#34;&gt;pipelinerun.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: PipelineRun&#xA;metadata:&#xA;  name: test-build-push-pr&#xA;spec:&#xA;  serviceAccountName: build-bot&#xA;  pipelineRef:&#xA;    name: test-build-push&#xA;  resources:&#xA;  - name: repo&#xA;    resourceRef:&#xA;      name: arthurk-tekton-example&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Apply the file, get the Pods that are prefixed with the PiplelineRun name, and&#xA;            view the logs to get the container output:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f pipelinerun.yaml&#xA;pipelinerun.tekton.dev/test-build-push-pr created&#xA;&#xA;$ kubectl get pods | grep test-build-push-pr&#xA;test-build-push-pr-build-and-push-gh4f4-pod-nn7k7   0/2     Completed   0          2m39s&#xA;test-build-push-pr-test-d2tck-pod-zh5hn             0/2     Completed   0          2m51s&#xA;&#xA;$ kubectl logs test-build-push-pr-build-and-push-gh4f4-pod-nn7k7 --all-containers --follow&#xA;INFO[0005] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;INFO[0005] Retrieving image manifest golang:1.14-alpine&#xA;...&#xA;INFO[0048] Taking snapshot of full filesystem...&#xA;INFO[0048] Resolving paths&#xA;INFO[0050] CMD [&amp;#34;app&amp;#34;]&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Next we will run the same Pipeline but we&amp;#39;re going to use the Tekton CLI instead.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Run the Pipeline with Tekton CLI&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        When using the CLI we don&amp;#39;t have to write a PipelineRun, it will be generated&#xA;        from the Pipeline manifest. By using the &lt;code&gt;--showlog&lt;/code&gt; argument it will&#xA;        also display the Task (container) logs:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ tkn pipeline start test-build-push --resource repo=arthurk-tekton-example --serviceaccount build-bot --showlog&#xA;&#xA;Pipelinerun started: test-build-push-run-9lmfj&#xA;Waiting for logs to be available...&#xA;[test : git-source-arthurk-tekton-example-k98k8] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483940.4913514,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[test : git-source-arthurk-tekton-example-k98k8] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483940.5485842,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[test : run-test] PASS&#xA;[test : run-test] ok  &#x9;_/workspace/repo/src&#x9;0.006s&#xA;&#xA;[build-and-push : git-source-arthurk-tekton-example-2vqls] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483950.2051432,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[build-and-push : git-source-arthurk-tekton-example-2vqls] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483950.2610846,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[build-and-push : build-and-push] INFO[0003] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;[build-and-push : build-and-push] INFO[0003] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;[build-and-push : build-and-push] INFO[0003] Retrieving image manifest golang:1.14-alpine&#xA;...&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Summary&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            In &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/&#34;&gt;Part 1&lt;/a&gt; we installed Tekton on a local Kubernetes cluster, defined a Task, and tested it by creating a TaskRun via YAML manifest as well as the Tekton CLI tkn.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In this part we created our first Tektok Pipeline that consists of two tasks.&#xA;            The first one clones a repo from GitHub and runs application tests.&#xA;            The second one builds a Docker image and pushes it to DockerHub.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            All code examples are available &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-2/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Creating CI Pipelines with Tekton (Part 1/2)</title>
    <updated>2020-04-26T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-26:/creating-ci-pipelines-with-tekton-part-1/</id>
    <content type="html">&#xA;        &#xA;        ,&#xA;        Updated &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post we&amp;#39;re going to build a continuous integration (CI) pipeline with &lt;a href=&#34;https://tekton.dev&#34;&gt;Tekton&lt;/a&gt;,&#xA;        an open-source framework for creating CI/CD pipelines in Kubernetes.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We&amp;#39;re going to provision a local Kubernetes cluster via &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; and install Tekton on it.&#xA;        After that we&amp;#39;ll create a pipeline consisting of two steps which will run application unit tests, build a Docker image,&#xA;        and push it to DockerHub.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;This is part 1 of 2 in which we will install Tekton and create a task that runs our application test.&#xA;        The second part is available &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-2/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating the k8s cluster&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We use &lt;a href=&#34;http://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; to create a Kubernetes cluster for our Tekton installation:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind create cluster --name tekton&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Installing Tekton&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can install Tekton by applying the &lt;code&gt;release.yaml&lt;/code&gt; file from the latest release of the &lt;a href=&#34;https://github.com/tektoncd/pipeline&#34;&gt;tektoncd/pipeline&lt;/a&gt; GitHub repo:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.20.1/release.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        This will install Tekton into the &lt;code&gt;tekton-pipelines&lt;/code&gt; namespace. We can check&#xA;        that the installation succeeded by listing the Pods in that namespace and&#xA;        making sure they&amp;#39;re in &lt;code&gt;Running&lt;/code&gt; state.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods --namespace tekton-pipelines&#xA;NAME                                           READY   STATUS    RESTARTS   AGE&#xA;tekton-pipelines-controller-74848c44df-m42gf   1/1     Running   0          20s&#xA;tekton-pipelines-webhook-6f764dc8bf-zq44s      1/1     Running   0          19s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Setting up the Tekton CLI&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Installing the CLI is optional but I found it to be more convenient&#xA;        than &lt;code&gt;kubectl&lt;/code&gt; when managing Tekton resources. The examples&#xA;        later on will show both ways.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can install it via Homebrew:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; brew tap tektoncd/tools&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; brew install tektoncd/tools/tektoncd-cli&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; tkn version&#xA;Client version: 0.16.0&#xA;Pipeline version: v0.20.1&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Concepts&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Tekton provides custom resource definitions (&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;CRDs&lt;/a&gt;) for Kubernetes that can be used to define our Pipelines.&#xA;        In this tutorial we will use the following custom resources:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Task: A series of steps that execute commands (In CircleCI this is called a &lt;em&gt;Job&lt;/em&gt;)&lt;/li&gt;&#xA;            &lt;li&gt;Pipeline: A set of Tasks (In CircleCI this is called a &lt;em&gt;Workflow&lt;/em&gt;)&lt;/li&gt;&#xA;            &lt;li&gt;PipelineResource: Input or Output of a Pipeline (for example a git repo or a tar file)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;We will use the following two resources to define the execution of our Tasks and Pipeline:&lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;TaskRun: Defines the execution of a Task&lt;/li&gt;&#xA;            &lt;li&gt;PipelineRun: Defines the execution of a Pipeline&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;For example,&#xA;        if we write a Task and want to test it we can execute it with a TaskRun. The same applies for a Pipeline:&#xA;        To execute a Pipeline we need to create a PipelineRun.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Application Code&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        In our example Pipeline we&amp;#39;re going to use a Go application&#xA;        that simply prints the sum of two integers. You can find the application code, test, and Dockerfile in the &lt;code&gt;src/&lt;/code&gt; directory in &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;this repo&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating our first task&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Our first Task will run the application tests inside the cloned git repo.&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/01-task-test.yaml&#34;&gt;01-task-test.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Task&#xA;metadata:&#xA;  name: test&#xA;spec:&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        type: git&#xA;  steps:&#xA;    - name: run-test&#xA;      image: golang:1.14-alpine&#xA;      workingDir: /workspace/repo/src&#xA;      command: [&amp;#34;go&amp;#34;]&#xA;      args: [&amp;#34;test&amp;#34;]&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;code&gt;resources:&lt;/code&gt; block defines the inputs that our task needs to execute its steps.&#xA;        Our step (named &lt;code&gt;run-test&lt;/code&gt;) needs the cloned &lt;a href=&#34;https://github.com/arthurk/tekton-example/&#34;&gt;tekton-example&lt;/a&gt; git repository as an input and we can create this input with a PipelineResource.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file called &lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/02-pipelineresource.yaml&#34;&gt;02-pipelineresource.yaml&lt;/a&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1alpha1&#xA;kind: PipelineResource&#xA;metadata:&#xA;  name: arthurk-tekton-example&#xA;spec:&#xA;  type: git&#xA;  params:&#xA;    - name: url&#xA;      value: https://github.com/arthurk/tekton-example&#xA;    - name: revision&#xA;      value: master&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;code&gt;git&lt;/code&gt; resource type will use git to clone the repo into the &lt;code&gt;/workspace/$input_name&lt;/code&gt; directory everytime the Task is run.&#xA;        Since our input is named &lt;code&gt;repo&lt;/code&gt; the code will be cloned to &lt;code&gt;/workspace/repo&lt;/code&gt;. If our input would be named &lt;code&gt;foobar&lt;/code&gt; it would be cloned into &lt;code&gt;/workspace/foobar&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The next block in our Task (&lt;code&gt;steps:&lt;/code&gt;) specifies the command to execute and the Docker image in which to run that command. We&amp;#39;re going to use the &lt;a href=&#34;https://hub.docker.com/_/golang&#34;&gt;golang&lt;/a&gt; Docker image as it already has Go installed.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        For the &lt;code&gt;go test&lt;/code&gt; command to run we need to change the directory. By default the command will run&#xA;        in the &lt;code&gt;/workspace/repo&lt;/code&gt; directory but in our &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;tekton-example&lt;/a&gt; repo the Go application is in the &lt;code&gt;src&lt;/code&gt; directory. We do this by setting &lt;code&gt;workingDir: /workspace/repo/src&lt;/code&gt;.&#xA;&#xA;        &lt;/p&gt;&lt;p&gt;&#xA;        Next we specify the command to run (&lt;code&gt;go test&lt;/code&gt;) but note that the command (&lt;code&gt;go&lt;/code&gt;) and args (&lt;code&gt;test&lt;/code&gt;) need to be defined separately&#xA;        in the YAML file.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Apply the Task and the PipelineResource with kubectl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f 01-task-test.yaml&#xA;task.tekton.dev/test created&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f 02-pipelineresource.yaml&#xA;pipelineresource.tekton.dev/arthurk-tekton-example created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Running our task&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To run our &lt;code&gt;Task&lt;/code&gt; we have to create a &lt;code&gt;TaskRun&lt;/code&gt; that references the previously created &lt;code&gt;Task&lt;/code&gt; and passes in all required inputs (&lt;code&gt;PipelineResource&lt;/code&gt;).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/03-taskrun.yaml&#34;&gt;03-taskrun.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: TaskRun&#xA;metadata:&#xA;  name: testrun&#xA;spec:&#xA;  taskRef:&#xA;    name: test&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        resourceRef:&#xA;          name: arthurk-tekton-example&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        This will take our Task (&lt;code&gt;taskRef&lt;/code&gt; is a reference to our previously created task named &lt;code&gt;test&lt;/code&gt;)&#xA;        with our &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;tekton-example&lt;/a&gt; git repo as an input (&lt;code&gt;resourceRef&lt;/code&gt; is a reference to our PipelineResource named &lt;code&gt;arthurk-tekton-example&lt;/code&gt;)&#xA;        and execute it.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Apply the file with kubectl and then check the Pods and TaskRun resources. The Pod will go through the &lt;code&gt;Init:0/2&lt;/code&gt; and &lt;code&gt;PodInitializing&lt;/code&gt; status and then succeed:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f 03-taskrun.yaml&#xA;pipelineresource.tekton.dev/arthurk-tekton-example created&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods&#xA;NAME                READY   STATUS      RESTARTS   AGE&#xA;testrun-pod-pds5z   0/2     Completed   0          4m27s&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get taskrun&#xA;NAME      SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME&#xA;testrun   True        Succeeded   70s         57s&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To see the output of the containers we can run the following command. Make sure to replace &lt;code&gt;testrun-pod-pds5z&lt;/code&gt; with the the Pod name from the output above (it will be different for each run).&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl logs testrun-pod-pds5z --all-containers&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477119.3692405,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477119.4230678,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;PASS&#xA;ok  &#x9;_/workspace/repo/src&#x9;0.003s&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Our tests passed and our task succeeded. Next we will&#xA;        use the Tekton CLI to see how we can make this whole process easier.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Using the Tekton CLI to run a Task&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The Tekton CLI provides a faster and more convenient way to run Tasks.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Instead of manually&#xA;        writing a &lt;code&gt;TaskRun&lt;/code&gt; manifest we can run the following command&#xA;        which takes our Task (named &lt;code&gt;test&lt;/code&gt;), generates a &lt;code&gt;TaskRun&lt;/code&gt; (with a random name) and shows&#xA;        its logs:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; tkn task start test --inputresource repo=arthurk-tekton-example --showlog&#xA;Taskrun started: test-run-8t46m&#xA;Waiting for logs to be available...&#xA;[git-source-arthurk-tekton-example-dqjfb] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477372.740875,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[git-source-arthurk-tekton-example-dqjfb] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477372.7954974,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[run-test] PASS&#xA;[run-test] ok  &#x9;_/workspace/repo/src&#x9;0.006s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            We have successfully installed Tekton on a local Kubernetes&#xA;            cluster, defined a Task, and tested it by creating a TaskRun via&#xA;            YAML manifest as well as the Tekton CLI &lt;code&gt;tkn&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            All example code is available &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        In the next part we&amp;#39;re going to create a task that will&#xA;        use &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;Kaniko&lt;/a&gt; to build a Docker image&#xA;        for our application and then push it to DockerHub.&#xA;        We will then create a Pipeline that runs both of our tasks sequentially (run application tests, build and push).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Part 2 is available &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-2/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Running Knative with Istio in a Kind Cluster</title>
    <updated>2020-04-19T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-19:/running-knative-with-istio-in-kind/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post I&amp;#39;m going to show how to run &lt;a href=&#34;https://knative.dev&#34;&gt;Knative&lt;/a&gt; with &lt;a href=&#34;https://istio.io&#34;&gt;Istio&lt;/a&gt;&#xA;        as a networking layer on a local &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; cluster.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        I&amp;#39;m assuming that kind and kubectl are installed. Installation instructions for kind are &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/#installation&#34;&gt;here&lt;/a&gt;&#xA;        and kubectl &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind --version&#xA;kind kind version 0.7.0&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl version --client&#xA;Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;15&amp;#34;, GitVersion:&amp;#34;v1.15.5&amp;#34;, GitCommit:&amp;#34;20c265fef0741dd71a66480e35bd69f18351daea&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2019-10-15T19:16:51Z&amp;#34;, GoVersion:&amp;#34;go1.12.10&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Setup a kind cluster&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To get traffic into our cluster we need to create our kind cluster with a&#xA;        custom configuration that sets up a port forward from host to ingress&#xA;        controller.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        In this setup we&amp;#39;re going to use port &lt;code&gt;32000&lt;/code&gt;. Later we will configure&#xA;        the Istio ingress gateway to accept connections on this port.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;kind-config-istio.yml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;kind&lt;/span&gt;: Cluster&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;apiVersion&lt;/span&gt;: kind.sigs.k8s.io/v1alpha3&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;nodes&lt;/span&gt;:&#xA;- &lt;span style=&#34;color:#66d9ef&#34;&gt;role&lt;/span&gt;: control-plane&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;extraPortMappings&lt;/span&gt;:&#xA;  - &lt;span style=&#34;color:#66d9ef&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32000&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;hostPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To create the cluster with our custom configuration we use the &lt;code&gt;--config&lt;/code&gt; argument:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind create cluster --config kind-config-istio.yml&#xA;&#xA;Creating cluster &amp;#34;kind&amp;#34; ...&#xA; ✓ Ensuring node image (kindest/node:v1.17.0) 🖼&#xA; ✓ Preparing nodes 📦&#xA; ✓ Writing configuration 📜&#xA; ✓ Starting control-plane 🕹️&#xA; ✓ Installing CNI 🔌&#xA; ✓ Installing StorageClass 💾&#xA;Set kubectl context to &amp;#34;kind-kind&amp;#34;&#xA;You can now use your cluster with:&#xA;&#xA;kubectl cluster-info --context kind-kind&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Install Istio&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We&amp;#39;re going to install Istio via the &lt;a href=&#34;https://istio.io/docs/reference/commands/istioctl/&#34;&gt;istioctl&lt;/a&gt;&#xA;        command-line tool.&#xA;        The following command will download version istioctl v1.5.1 for macOS and extract it into the current directory:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -L https://github.com/istio/istio/releases/download/1.5.1/istioctl-1.5.1-osx.tar.gz | tar xvz -&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl version --remote=false&#xA;1.5.1&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Istio can be installed with different configuration profiles. In this example&#xA;        we are going to use the &lt;code&gt;default&lt;/code&gt; profile which will&#xA;        install the pilot, ingressgateway and prometheus.&#xA;        A list of all built-in&#xA;        configuration profiles and their differences can be found &lt;a href=&#34;https://istio.io/docs/setup/additional-setup/config-profiles/&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The following command will perform the installation:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl manifest apply --set profile=default&#xA;&#xA;Detected that your cluster does not support third party JWT authentication. Falling back to less secure first party JWT. See https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens for details.&#xA;- Applying manifest for component Base...&#xA;✔ Finished applying manifest for component Base.&#xA;- Applying manifest for component Pilot...&#xA;✔ Finished applying manifest for component Pilot.&#xA;- Applying manifest for component IngressGateways...&#xA;- Applying manifest for component AddonComponents...&#xA;✔ Finished applying manifest for component AddonComponents.&#xA;✔ Finished applying manifest for component IngressGateways.&#xA;&#xA;✔ Installation complete&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can check that the pods are running via kubectl:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods -n istio-system&#xA;&#xA;NAME                                    READY   STATUS    RESTARTS   AGE&#xA;istio-ingressgateway-5f54974979-crw9d   1/1     Running   0          21s&#xA;istiod-6548b95486-djvd6                 1/1     Running   0          6m57s&#xA;prometheus-6c88c4cb8-wtdtn              2/2     Running   0          21s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To verify the installation we can run the &lt;code&gt;verify-install&lt;/code&gt; command&#xA;        and pass in the manifest of the default configuration profile:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl manifest generate --set profile=default | ./istioctl verify-install -f -&#xA;...&#xA;Checked 25 crds&#xA;Checked 1 Istio Deployments&#xA;Istio is installed successfully&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The configuration profile&#xA;        will set the ingress type to &lt;code&gt;LoadBalancer&lt;/code&gt;, which&#xA;        is not working on a local cluster.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        For the ingress gateway to accept incoming connections we have to change&#xA;        the type from &lt;code&gt;LoadBalancer&lt;/code&gt; to &lt;code&gt;NodePort&lt;/code&gt; and change the assigned port&#xA;        to &lt;code&gt;32000&lt;/code&gt; (the port we forwarded during the cluster creation).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;patch-ingressgateway-nodeport.yaml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt;: NodePort&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;ports&lt;/span&gt;:&#xA;  - &lt;span style=&#34;color:#66d9ef&#34;&gt;name&lt;/span&gt;: http2&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32000&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;protocol&lt;/span&gt;: TCP&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We apply the file with &lt;code&gt;kubectl patch&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl patch service istio-ingressgateway -n istio-system --patch &amp;#34;$(cat patch-ingressgateway-nodeport.yaml)&amp;#34;&#xA;&#xA;service/istio-ingressgateway patched&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Istio is now set up and ready to accept connections.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Install Knative&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative consists of two components: &lt;a href=&#34;https://knative.dev/docs/serving/&#34;&gt;Serving&lt;/a&gt; and &lt;a href=&#34;https://knative.dev/docs/eventing/&#34;&gt;Eventing&lt;/a&gt;. In this example we&amp;#39;re going to install the Serving component.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        We start by applying the Kubernetes manifests for the CRDs, Core and Istio ingress controller:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/serving/releases/download/v0.14.0/serving-crds.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/serving/releases/download/v0.14.0/serving-core.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/net-istio/releases/download/v0.14.0/net-istio.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We check the pods via kubectl and wait until they have the status &lt;code&gt;Running&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods --namespace knative-serving&#xA;&#xA;NAME                                READY   STATUS    RESTARTS   AGE&#xA;activator-65fc4d666-2bj8r           2/2     Running   0          9m&#xA;autoscaler-74b4bb97bd-9rql4         2/2     Running   0          9m&#xA;controller-6b6978c965-rks25         2/2     Running   0          9m&#xA;istio-webhook-856d84fbf9-8nswp      2/2     Running   0          8m58s&#xA;networking-istio-6845f7cf59-6h25b   1/1     Running   0          8m58s&#xA;webhook-577576647-rw264             2/2     Running   0          9m&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative will create a custom URL for each service and for this to work&#xA;        it needs to have DNS configured. Since our cluster is running locally we need to&#xA;        use a wildcard DNS service (for example &lt;a href=&#34;https://nip.io&#34;&gt;nip.io&lt;/a&gt;).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        We patch the Knative config via kubectl and set the domain to &lt;code&gt;127.0.0.1.nip.io&lt;/code&gt;&#xA;        which will forward all requests to &lt;code&gt;127.0.0.1&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl patch configmap/config-domain \&#xA;  --namespace knative-serving \&#xA;  --type merge \&#xA;  --patch &amp;#39;{&amp;#34;data&amp;#34;:{&amp;#34;127.0.0.1.nip.io&amp;#34;:&amp;#34;&amp;#34;}}&amp;#39;&#xA;&#xA;configmap/config-domain patched&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative is now installed and ready to use.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating a test service&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To check that Knative is working correctly we deploy a test service&#xA;        that consists of an &lt;a href=&#34;https://github.com/jmalloc/echo-server&#34;&gt;echo-server&lt;/a&gt; which will return the request&#xA;        headers and body.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We start by creating a file named &lt;code&gt;knative-echoserver.yaml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;apiVersion&lt;/span&gt;: serving.knative.dev/v1&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;kind&lt;/span&gt;: Service&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;metadata&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;name&lt;/span&gt;: helloworld&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt;: default&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt;:&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;      &lt;span style=&#34;color:#66d9ef&#34;&gt;containers&lt;/span&gt;:&#xA;        - &lt;span style=&#34;color:#66d9ef&#34;&gt;image&lt;/span&gt;: jmalloc/echo-server&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We enable Istio sidecar injection for the default namespace&#xA;        and deploy the Knative service in it:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl label namespace default istio-injection=enabled&#xA;&#xA;namespace/default labeled&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f knative-echoserver.yaml&#xA;&#xA;service.serving.knative.dev/helloworld created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can check the deployment of the pods via kubectl:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods&#xA;&#xA;NAME                                           READY   STATUS    RESTARTS   AGE&#xA;helloworld-96c68-deployment-6744444b5f-6htld   3/3     Running   0          108s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        When all pods are running we can get the URL of the service and&#xA;        make an HTTP request to it:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get ksvc&#xA;&#xA;NAME         URL                                          LATESTCREATED      LATESTREADY        READY   REASON&#xA;helloworld   http://helloworld.default.127.0.0.1.nip.io   helloworld-96c68   helloworld-96c68   True&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl http://helloworld.default.127.0.0.1.nip.io&#xA;&#xA;Request served by helloworld-96c68-deployment-6744444b5f-6htld&#xA;&#xA;HTTP/1.1 GET /&#xA;&#xA;Host: helloworld.default.127.0.0.1.nip.io&#xA;X-Request-Id: 9e5bf3c9-0bc8-4551-9302-ea2eca5f6446&#xA;User-Agent: curl/7.64.1&#xA;Accept-Encoding: gzip&#xA;Forwarded: for=10.244.0.1;proto=http, for=127.0.0.1&#xA;X-B3-Traceid: d22e218318367687170ce339b13b0c91&#xA;X-Forwarded-For: 10.244.0.1, 127.0.0.1, 127.0.0.1&#xA;X-B3-Spanid: 0e3174748253699d&#xA;X-Forwarded-Proto: http&#xA;Accept: */*&#xA;K-Proxy-Request: activator&#xA;X-B3-Parentspanid: c39ad4d28b42b25f&#xA;X-B3-Sampled: 0&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            The response shows the pod which served the request (&lt;code&gt;helloworld-96c68-deployment-6744444b5f-6htld&lt;/code&gt;)&#xA;            and the tracing headers that Istio will add to every request.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        If we wait a few minutes we can see that Knative will scale down our&#xA;        service to zero replicas (no incoming requests). In this case&#xA;        we can make another request to the service and see it scale up again.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/running-knative-with-istio-in-kind/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Writing tar.gz files in Go</title>
    <updated>2020-04-12T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-12:/writing-tar-gz-files-in-go/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            In this blog post I&amp;#39;m going to explain how to use the Go &lt;code&gt;archive/tar&lt;/code&gt; and &lt;code&gt;compress/gzip&lt;/code&gt;&#xA;            packages to create a tar archive and compress it with gzip.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;Below is the full example code and after that there&amp;#39;s an explanation of the parts.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Full Code&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#f92672&#34;&gt;package&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;io&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;archive/tar&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;compress/gzip&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fmt&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;os&amp;#34;&lt;/span&gt;&#xA;)&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Files which to include in the tar.gz archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test/test.txt&amp;#34;&lt;/span&gt;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create output file&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Create&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output.tar.gz&amp;#34;&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error writing archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create the archive and write the output to the &amp;#34;out&amp;#34; Writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error creating archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Println&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Archive created successfully&amp;#34;&lt;/span&gt;)&#xA;}&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create new Writers for gzip and tar&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// These writers are chained. Writing to the tar writer will&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// write to the gzip writer which in turn will write to&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// the &amp;#34;buf&amp;#34; writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gzip&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Iterate over files and add them to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;range&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;&#x9;}&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Open the file which will be written into the archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Open&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Get FileInfo about our file providing file size, mode, etc.&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Stat&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create a tar Header from the FileInfo data&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;FileInfoHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt;())&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Use full path as name (FileInfoHeader only takes the basename)&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// If we don&amp;#39;t do this the directory strucuture would&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// not be preserved&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// https://golang.org/src/archive/tar/common.go?#L626&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Write file header to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;WriteHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Copy file content to tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Copy&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Explanation&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            In the main function we first declare &lt;code&gt;files&lt;/code&gt; as a string slice.&#xA;            It contains the paths of the files that will be included in the archive.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            For this example I&amp;#39;ve created two text files. I placed one of them in the same&#xA;            directory as the &lt;code&gt;main.go&lt;/code&gt; file and the other one in a subdirectory. The purpose&#xA;            of this is to test that the directory structure will be correctly restored after extraction.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We then create the output file with &lt;code&gt;&lt;a href=&#34;https://golang.org/pkg/os/#Create&#34;&gt;os.Create()&lt;/a&gt;&lt;/code&gt;&#xA;            and pass it to the &lt;code&gt;createArchive&lt;/code&gt; function along with our file paths.&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Files which to include in the tar.gz archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test/test.txt&amp;#34;&lt;/span&gt;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create output file&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Create&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output.tar.gz&amp;#34;&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error writing archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create the archive and write the output to the &amp;#34;out&amp;#34; Writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error creating archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Println&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Archive created successfully&amp;#34;&lt;/span&gt;)&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;&#xA;        The &lt;code&gt;createArchive&lt;/code&gt; function creates two Writer: The &lt;a href=&#34;https://golang.org/pkg/archive/tar/#NewWriter&#34;&gt;tar Writer&lt;/a&gt;&#xA;        and the &lt;a href=&#34;https://golang.org/pkg/compress/gzip/#NewWriter&#34;&gt;gzip Writer&lt;/a&gt;. Both implement the &lt;a href=&#34;https://golang.org/pkg/io/#Writer&#34;&gt;io.Writer&lt;/a&gt; interface.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        The Writers are chained which means that bytes written to the tar Writer &lt;code&gt;tw&lt;/code&gt;&#xA;        will simultaneously be written to the gzip Writer &lt;code&gt;gw&lt;/code&gt;.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        We will then iterate over the files in the &lt;code&gt;files&lt;/code&gt; slice and&#xA;        call the &lt;code&gt;addToArchive&lt;/code&gt; function for each of them with the filename and the tar Writer&#xA;        as arguments.&#xA;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create new Writers for gzip and tar&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// These writers are chained. Writing to the tar Writer will&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// write to the gzip writer which in turn will write to&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// the &amp;#34;buf&amp;#34; writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gzip&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Iterate over files and and add them to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;range&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;&#x9;}&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Inside the &lt;code&gt;addToArchive&lt;/code&gt; function we open the file and get&#xA;            a &lt;code&gt;&lt;a href=&#34;https://golang.org/pkg/os/#FileInfo&#34;&gt;FileInfo&lt;/a&gt;&lt;/code&gt;.&#xA;            The FileInfo contains information such as the file name, size or mode which is necessary for the next step.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Open the file which will be written into the archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Open&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Get FileInfo about our file providing file size, mode, etc.&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Stat&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;&#xA;        Each file in a tar archive has a &lt;a href=&#34;https://golang.org/pkg/archive/tar/#Header&#34;&gt;header&lt;/a&gt; containing metadata about the file&#xA;        followed by the file content. In this step we create the header by&#xA;        calling &lt;a href=&#34;https://golang.org/pkg/archive/tar/#FileInfoHeader&#34;&gt;FileInfoHeader&lt;/a&gt; which will take our FileInfo &lt;code&gt;info&lt;/code&gt;&#xA;        and generate a valid tar Header from it.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        The os.FileInfo &lt;code&gt;info&lt;/code&gt; only stores the base name of the file. For example if we pass in &lt;code&gt;test/test.txt&lt;/code&gt; it&#xA;        will only store the filename &lt;code&gt;test.txt&lt;/code&gt;. This is a problem when creating the tar archive as it would omit&#xA;        the directory structure of our files.&#xA;        To fix this we have to set &lt;code&gt;header.Name&lt;/code&gt; to the full file path.&#xA;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create a tar Header from the FileInfo data&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;FileInfoHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt;())&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Use full path as name (FileInfoHeader only takes the basename)&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// If we don&amp;#39;t do this the directory strucuture would&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// not be preserved&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// https://golang.org/src/archive/tar/common.go?#L626&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Now we can write the header and the file content to the Writer.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Write file header to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;WriteHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Copy file content to tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Copy&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;h2&gt;Run the program&lt;/h2&gt;&#xA;    &lt;p&gt;We can now run our program and check that the files can be extracted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; go run main.go&#xA;Archive created successfully&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; tar xzfv output.tar.gz&#xA;x example.txt&#xA;x test/test.txt&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; exa --tree&#xA;.&#xA;├── example.txt&#xA;├── output.tar.gz&#xA;└── test&#xA;   └── test.txt&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Both files have been extracted successfully.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/writing-tar-gz-files-in-go/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Notes about Google CloudSQL for Postgres</title>
    <updated>2020-04-05T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-05:/notes-about-google-cloudsql-for-postgres/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            Here are a few things that I&amp;#39;ve learned about Google CloudSQL for Postgres&#xA;            during the last 2 years in which I&amp;#39;ve been using it.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Price&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;It&amp;#39;s usually the most expensive part of the project accounting for around 80% of the total cost. The parts that need to be paid for are CPU Cores, RAM, Disk Storage and Network Internet Egress. Automated backups and HA are optional and cost extra.&lt;/li&gt;&#xA;            &lt;li&gt;There&amp;#39;s a &lt;a href=&#34;https://cloud.google.com/products/calculator&#34;&gt;pricing calculator&lt;/a&gt; available but I haven&amp;#39;t been able to replicate the price on the invoice (at least in my case the total on the invoice was &lt;em&gt;lower&lt;/em&gt; than what the calculator showed). I suggest to just try it for one or two months and see for yourself. Make sure to set a budget in GCP so the cost doesn&amp;#39;t go too high.&lt;/li&gt;&#xA;            &lt;li&gt;Read-only replicas need to have at least the same hardware as the master instance. This means each read-replica will double the cost.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Replication&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;As mentioned above a read-replica needs to have at least the same hardware (cores, memory, storage) as the master instance. It can have better hardware.&lt;/li&gt;&#xA;            &lt;li&gt;External replication is not supported. You can create read-only replicas in CloudSQL but it&amp;#39;s not possible to use streaming replication to an external Postgres instance.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;HA (High Availability)&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Failover will take place after the master instance is unresponsive for 1 minute. In total it takes 2-3 minutes for connections to be re-established.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Backups&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Deleting the instance will delete all of its backups too. Always make sure to have an additional backup job running that will export the data to another location.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Performance&lt;/h2&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Network throughput (MB/s) depends on the number of CPU cores. More CPU Cores = More throughput. 1 CPU core has 250 MB/s throughput and the maximum is 2000 MB/s which is reached at 8 cores.&lt;/li&gt;&#xA;            &lt;li&gt;Disk throughput and IOPS depend on the disk size. The minimum size is 10 GB which has 4.8 MB/s of read/write throughput and 300 IOPS (read/write). The maximum is 800 MB/s read and 400 MB/s write throughput with 15,000 IOPS (read/write) which is reached at 500 GB disk size.&lt;/li&gt;&#xA;            &lt;li&gt;The network latency from a GKE instance to CloudSQL is around 3ms. There is no difference in latency between using a private and public ip.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Maintenance&lt;/h2&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;The maintenance downtime is 1-2 minutes and occurs during a selected time window.&lt;/li&gt;&#xA;            &lt;li&gt;Maintenance notifications were recently added. Only e-mail notifications are supported.&lt;/li&gt;&#xA;            &lt;li&gt;Upgrading Postgres to a new major version is only possible by dumping the data and then importing it after the upgrade. For our 128 GB database it took around 40 minutes to export and 5 hours to import (pg_restore). This is not including the time it took to download the export from Cloud Storage.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/notes-about-google-cloudsql-for-postgres/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Managing Helm Charts with Helmfile</title>
    <updated>2020-03-29T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-29:/managing-helm-charts-with-helmfile/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            In this blog post I&amp;#39;m going to show how &lt;a href=&#34;https://github.com/roboll/helmfile&#34;&gt;Helmfile&lt;/a&gt; makes it easier&#xA;            to manage Helm charts and environments.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this I&amp;#39;m going to walk through an example where at the beginning&#xA;            we install helm charts over the CLI using the &lt;code&gt;helm&lt;/code&gt; command,&#xA;            and then refactor the code in steps to use the &lt;code&gt;helmfile&lt;/code&gt; command instead.&#xA;        &lt;/p&gt;&#xA;        &lt;h2&gt;Setup&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Our setup consists of 2 applications (backend and frontend) and Prometheus&#xA;            for metrics. We have helm charts for:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Backend (custom chart)&lt;/li&gt;&#xA;            &lt;li&gt;Frontend (custom chart)&lt;/li&gt;&#xA;            &lt;li&gt;Prometheus (chart from the &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus&#34;&gt;helm stable repo&lt;/a&gt;)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            which are deployed into these environments:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Development&lt;/li&gt;&#xA;            &lt;li&gt;Staging&lt;/li&gt;&#xA;            &lt;li&gt;Production&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            The files are organized in this directory structure:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;.&#xA;└── charts&#xA;   ├── backend&#xA;   │  ├── Chart.yaml&#xA;   │  ├── templates&#xA;   │  └── values-development.yaml&#xA;   │  └── values-staging.yaml&#xA;   │  └── values-production.yaml&#xA;   │  └── secrets-development.yaml&#xA;   │  └── secrets-staging.yaml&#xA;   │  └── secrets-production.yaml&#xA;   └── frontend&#xA;   │  ├── Chart.yaml&#xA;   │  ├── templates&#xA;   │  └── values-development.yaml&#xA;   │  └── values-staging.yaml&#xA;   │  └── values-production.yaml&#xA;   │  └── secrets-development.yaml&#xA;   │  └── secrets-staging.yaml&#xA;   │  └── secrets-production.yaml&#xA;   └── prometheus&#xA;      └── values-development.yaml&#xA;      └── values-staging.yaml&#xA;      └── values-production.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            Each values-development.yaml, values-staging.yaml, values-production.yaml file&#xA;            contains values that are specific to that environment.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            For example the development environment only needs to deploy 1 replica of&#xA;            the backend while the staging and production environments need 3 replicas.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We use &lt;a href=&#34;https://github.com/fstech/helm-secrets&#34;&gt;helm-secrets&lt;/a&gt; to manage secrets.&#xA;            Each secrets file is encrypted and has to be manually decrypted before deploying the chart.&#xA;            After the deployment is done the decrypted file has to be deleted.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Installation and Upgrades&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            With the above setup we use the following commands to deploy&#xA;            (install/upgrade) the backend chart in the staging environment:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helm secrets dec ./charts/backend/secrets-backend.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helm upgrade --install --atomic --cleanup-on-fail -f ./charts/backend/values-staging.yaml -f ./charts/backend/secrets-staging.yaml backend ./charts/backend&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; rm ./charts/backend/secrets-backend.yaml.dec&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            We use the &lt;code&gt;helm upgrade&lt;/code&gt; command with the &lt;code&gt;--install&lt;/code&gt; flag to&#xA;            be able to install and upgrade charts with the same command.&#xA;&#xA;            We also use the &lt;code&gt;--atomic&lt;/code&gt; and &lt;code&gt;--cleanup-on-fail&lt;/code&gt;&#xA;            flags to rollback changes in case a chart upgrade fails.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To deploy the other charts we have to repeat the same commands (for the prometheus&#xA;            chart we can leave out the part that handles secrets).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Now the problem is that it&amp;#39;s hard to remember the exact commands to&#xA;            run when deploying a chart (especially when the upgrades are not very frequent).&#xA;&#xA;            When multiple people are responsible for deployments it&amp;#39;s also difficult to&#xA;            make sure the same commands are used. If, for example, the secrets were not&#xA;            decrypted beforehand it will lead to encrypted values being deployed and probably&#xA;            crash the application.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Writing Bash Scripts&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To fix the issues mentioned above we can write bash scripts that execute&#xA;            the exact commands needed for a deployment.&#xA;&#xA;            We create one script per environment in each chart directory which leads to the&#xA;            following directory tree for the backend chart:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;.&#xA;└── charts&#xA;   ├── backend&#xA;      ├── Chart.yaml&#xA;      ├── templates/&#xA;      └── values-development.yaml&#xA;      └── values-staging.yaml&#xA;      └── values-production.yaml&#xA;      └── secrets-development.yaml&#xA;      └── secrets-staging.yaml&#xA;      └── secrets-production.yaml&#xA;      └── deploy-development.sh&#xA;      └── deploy-staging.sh&#xA;      └── deploy-production.sh&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            When we want to deploy the backend chart in the staging environment&#xA;            we can run:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./charts/backend/deploy-staging.sh&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            This works fine for small environments like in the example above, but&#xA;            for larger environments with 15 or 20 charts it will lead to a lot of&#xA;            similar-looking bash scripts with large amounts of code duplication.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Provisioning a new environment would mean that a new deploy script&#xA;            has to be created in each chart directory. If we have 15 charts&#xA;            that means we have to copy one of the existing deploy scripts 15 times&#xA;            and search/replace the contents to match the new environment name.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To avoid duplicating the same code over and over again we could consolidate&#xA;            all of our small deploy scripts into one large deploy script. But this comes&#xA;            with a cost: We have to spend time maintaining it, fixing bugs and possibly&#xA;            extend it to handle new environments.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            At this point Helmfile comes in handy. Instead of writing our custom deploy&#xA;            script we can declare our environments in a YAML file and let it handle the&#xA;            deployment logic for us.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Using a Helmfile&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Using the backend chart as an example we can write the following content&#xA;            into a &lt;code&gt;helmfile.yaml&lt;/code&gt; file to manage the staging deployment:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-staging.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-staging.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can deploy the chart by running:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            In the background Helmfile will run the same &lt;code&gt;helm upgrade --install ...&lt;/code&gt; command as before.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note that there&amp;#39;s no need to manually decrypt secrets anymore as Helmfile has built-in&#xA;            support for helm-secrets.&#xA;            This means that any file that is listed under &lt;code&gt;secrets:&lt;/code&gt; will automatically be decrypted&#xA;            and after the deployment is finished the decrypted file will automatically be removed.&#xA;        &lt;/p&gt;&#xA;&#xA;&#xA;        &lt;h2&gt;Environments&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            The above example uses the &lt;code&gt;values-staging.yaml&lt;/code&gt; file as chart&#xA;            values. To be able to use multiple environments we can list them under&#xA;            the &lt;code&gt;environments:&lt;/code&gt; key at the beginning of the helmfile&#xA;            and then use the environment name as a variable in the release definition.&#xA;            The file would now look like this:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-{{ .Environment.Name }}.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            When deploying the chart we now have to use the &lt;code&gt;--environment/-e&lt;/code&gt; option when&#xA;            executing the helmfile command:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging sync&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now easily create new environments by listing them under&#xA;            &lt;code&gt;environments&lt;/code&gt; instead of duplicating our bash scripts.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Templates&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            After adding all of our helm charts into the helmfile the file content&#xA;            would look like this:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;- name: frontend&#xA;  chart: charts/frontend&#xA;  values:&#xA;  - charts/frontend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/frontend/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;- name: prometheus&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;  values:&#xA;  - charts/prometheus/values-{{ .Environment.Name }}.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The same pattern (for values and secrets) is repeated for each release. While in the&#xA;            example above we only have 3 releases the pattern will continue for future additions and&#xA;            eventually lead to much duplicated code.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can avoid copy/pasting the release definitions by using Helmfile templates.&#xA;            A template is defined at the top of the file and then referenced in the release by using&#xA;            &lt;a href=&#34;https://confluence.atlassian.com/bitbucket/yaml-anchors-960154027.html&#34;&gt;YAML anchors&lt;/a&gt;.&#xA;            This is our helmfile after using templates:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;templates:&#xA;  default: &amp;amp;default&#xA;    chart: charts/{{`{{ .Release.Name }}`}}&#xA;    missingFileHandler: Warn&#xA;    values:&#xA;    - charts/{{`{{ .Release.Name }}`}}/values-{{ .Environment.Name }}.yaml&#xA;    secrets:&#xA;    - charts/{{`{{ .Release.Name }}`}}/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;releases:&#xA;- name: backend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: frontend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: prometheus&#xA;  &amp;lt;&amp;lt;: *default&#xA;  # override the defaults since it&amp;#39;s a remote chart&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            We have removed much of the duplicated code from our helmfile and can&#xA;            now easily add new environments and releases.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Helm Defaults&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We&amp;#39;ve previously used the the &lt;code&gt;--atomic&lt;/code&gt; and &lt;code&gt;--cleanup-on-fail&lt;/code&gt;&#xA;            options when deploying charts. To do the same when using Helmfile we just&#xA;            have to specify them under &lt;code&gt;helmDefaults&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;helmDefaults:&#xA;  atomic: true&#xA;  cleanupOnFail: true&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Running Helmfile Commands&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Here are a few examples of helmfile commands for common operations.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To install or upgrade all charts in an environment (using staging as an example)&#xA;            we run:&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            If we just want to sync (meaning to install/upgrade) a single chart we can use selectors. This command&#xA;            will sync the backend chart in the staging environment with our local values:&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging -l name=backend sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            To show the changes an operation would perform on a cluster without&#xA;            actually applying them we can run the following command (requires the&#xA;            &lt;a href=&#34;https://github.com/databus23/helm-diff&#34;&gt;helm-diff&lt;/a&gt; plugin):&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging -l name=prometheus diff&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Full Example Code&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            This is the final content of our helmfile.yaml file:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;helmDefaults:&#xA;  atomic: true&#xA;  cleanupOnFail: true&#xA;&#xA;templates:&#xA;  default: &amp;amp;default&#xA;    chart: charts/{{`{{ .Release.Name }}`}}&#xA;    missingFileHandler: Warn&#xA;    values:&#xA;    - charts/{{`{{ .Release.Name }}`}}/values-{{ .Environment.Name }}.yaml&#xA;    secrets:&#xA;    - charts/{{`{{ .Release.Name }}`}}/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;releases:&#xA;- name: backend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: frontend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: prometheus&#xA;  &amp;lt;&amp;lt;: *default&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;        The directory structure did not change and is the same as described at the top of the post.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/managing-helm-charts-with-helmfile/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Setting up Vim for YAML editing</title>
    <updated>2020-03-21T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-21:/setting-up-vim-for-yaml/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post I&amp;#39;m going to show how to set up Vim for easier YAML editing.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;https://www.arthurkoziel.com/setting-up-vim-for-yaml/full-example.png&#34; alt=&#34;Screenshot of Vim&#34;/&gt;&#xA;&#xA;        &lt;p&gt;You can scroll down to the end for a summary of all installed plugins and config file changes.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Syntax Highlighting&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s not much to do here. VIM has YAML syntax highlighting built-in and it&amp;#39;s great.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s one thing I want to mention though. A few years back YAML highlighting in Vim&#xA;            was very slow, and there was often a noticeable lag when opening large files.&#xA;            The workaround was to use the &lt;a href=&#34;https://github.com/stephpy/vim-yaml&#34;&gt;vim-yaml&lt;/a&gt; plugin for&#xA;            fast syntax highlighting.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Not sure if it&amp;#39;s still worth installing I decided to make a performance benchmark.&#xA;        I loaded up a &lt;a href=&#34;https://github.com/istio/istio/blob/master/manifests/base/files/gen-istio-cluster.yaml&#34;&gt;large YAML file&lt;/a&gt; (6100 lines) and compared the time:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# default syntax highlighting&lt;/span&gt;&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; vim gen-istio-cluster.yaml --startuptime default.log&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; tail -1 default.log&#xA;&lt;span class=&#34;out&#34;&gt;055.563&lt;/span&gt;&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# vim-yaml plugin&lt;/span&gt;&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; vim gen-istio-cluster.yaml --startuptime vimyaml.log&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; tail -1 vimyaml.log&#xA;&lt;span class=&#34;out&#34;&gt;060.320&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            As we can see the default syntax highlighting is just as fast as the plugin and there&amp;#39;s&#xA;            no need to install a separate plugin to fix the slow syntax highlighting anymore.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Indentation&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Indentation probably the most annoying part about editing YAML files.&#xA;            Large documents with deeply nested blocks are often hard to track and&#xA;            errors are easily made.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            YAML documents are required to have a 2 space indentation. However, Vim does not set this&#xA;            by default but it&amp;#39;s an easy fix by putting the following line in the vim config:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;autocmd FileType yaml setlocal ts=2 sts=2 sw=2 expandtab&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can also setup Indentation guides.&#xA;            Indentation guides are thin vertical lines at each indentation level and useful&#xA;            to help line up nested YAML blocks.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can display those lines by using the &lt;a href=&#34;https://github.com/Yggdroot/indentLine&#34;&gt;indentLine plugin&lt;/a&gt;.&#xA;            I&amp;#39;ve modified the indentation character to display a thinner line (default is &amp;#34;¦&amp;#34;):&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;let g:indentLine_char = &amp;#39;⦙&amp;#39;&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The result should look like this:&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;https://www.arthurkoziel.com/setting-up-vim-for-yaml/indentlines.png&#34; alt=&#34;Screenshot of Vim showing the indentLine plugin&#34;/&gt;&#xA;&#xA;        &lt;h2&gt;Folding&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            With folding we can hide parts of the file that are not relevant to our current task.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Vim has built-in support for folding based on the indentation level but the default&#xA;            folding rules make it hard to tell what is folded. This is because the folding starts&#xA;            on the line &lt;em&gt;following&lt;/em&gt; the start of a block. To change this we can install&#xA;            the &lt;a href=&#34;https://github.com/pedrohdz/vim-yaml-folds&#34;&gt;vim-yaml-folds&lt;/a&gt; plugin.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Here&amp;#39;s a side-by-side comparison of the default folding (left) compared to vim-yaml-folds (right):&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;https://www.arthurkoziel.com/setting-up-vim-for-yaml/folding-compare.png&#34; alt=&#34;comparison of default folding with vim-yaml-folds&#34;/&gt;&#xA;        &lt;p&gt;&#xA;            To work with folding we need to remember a few keyboard commands. Vimcasts has&#xA;            a great episode on this &lt;a href=&#34;http://vimcasts.org/episodes/how-to-fold/&#34;&gt;here&lt;/a&gt;.&#xA;            Most of the time I use the following commands:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;za: Toggle current fold&lt;/li&gt;&#xA;            &lt;li&gt;zR: Expand all folds&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            After the plugin is installed and folding is enabled the default settings will fold all&#xA;            blocks by default. To start with unfolded content we can set:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;set foldlevelstart=20&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s also a plugin called &lt;a href=&#34;https://www.vim.org/scripts/script.php?script_id=4021&#34;&gt;restore_view&lt;/a&gt;&#xA;            which will save the folds for each file. But be aware that this plugin will create an&#xA;            extra file with folding information for each opened document.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Linting&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Linting will analyze the code and show any potential errors while we&amp;#39;re writing it which helps&#xA;            us catch formatting or syntax errors early on.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this in Vim we can use &lt;a href=&#34;https://github.com/dense-analysis/ale&#34;&gt;ALE&lt;/a&gt;,&#xA;            an asynchronous linting framework that has support for many languages and tools including YAML.&#xA;            To enable YAML linting in ALE we have to install &lt;a href=&#34;https://github.com/adrienverge/yamllint&#34;&gt;yamllint&lt;/a&gt;,&#xA;            a Python-based YAML linter.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Installation instructions are &lt;a href=&#34;https://yamllint.readthedocs.io/en/stable/quickstart.html#installing-yamllint&#34;&gt;here&lt;/a&gt;.&#xA;            On macOS we can install it with Homebrew:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; brew install yamllint&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The default configuration is fairly strict and shows errors in document style such as&#xA;            line length, trailing spaces or comment indentation.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can modify the configuration to be less strict.&#xA;            Yamllint already comes with a &lt;a href=&#34;https://github.com/adrienverge/yamllint/blob/master/yamllint/conf/relaxed.yaml&#34;&gt;relaxed&lt;/a&gt;&#xA;            version of the default config that is a good starting point.&#xA;            The only additional thing I&amp;#39;ve decided to disable is line length checking.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this we open up &lt;code&gt;~/.config/yamllint/config&lt;/code&gt; and&#xA;            paste the following:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;extends: relaxed&#xA;&#xA;rules:&#xA;  line-length: disable&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;I&amp;#39;ve modified the ALE configuration to change the message format, error symbols and only lint&#xA;    when the file is saved:&lt;/p&gt;&#xA;&lt;pre&gt;let g:ale_echo_msg_format = &amp;#39;[%linter%] %s [%severity%]&amp;#39;&#xA;let g:ale_sign_error = &amp;#39;✘&amp;#39;&#xA;let g:ale_sign_warning = &amp;#39;⚠&amp;#39;&#xA;let g:ale_lint_on_text_changed = &amp;#39;never&amp;#39;&#xA;&lt;/pre&gt;&#xA;    &lt;p&gt;&#xA;        We can see the errors and warnings on the left side:&#xA;    &lt;/p&gt;&#xA;        &lt;img src=&#34;https://www.arthurkoziel.com/setting-up-vim-for-yaml/full-example.png&#34; alt=&#34;Screenshot of Vim&#34;/&gt;&#xA;&#xA;    &lt;h2&gt;Summary&lt;/h2&gt;&#xA;    &lt;p&gt;&#xA;        Here&amp;#39;s a summary of the plugins, applications and config modifications:&#xA;    &lt;/p&gt;&#xA;    &lt;h3&gt;Vim Plugins&lt;/h3&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/Yggdroot/indentLine&#34;&gt;indentLine&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/pedrohdz/vim-yaml-folds&#34;&gt;vim-yaml-folds&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/dense-analysis/ale&#34;&gt;ALE&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;h3&gt;Applicatins&lt;/h3&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/adrienverge/yamllint&#34;&gt;yamllint&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;h3&gt;Config&lt;/h3&gt;&#xA;    &lt;p&gt;In &lt;code&gt;~/.vimrc&lt;/code&gt; or &lt;code&gt;~/.config/nvim/init.vim&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;autocmd FileType yaml setlocal ts=2 sts=2 sw=2 expandtab&#xA;&#xA;set foldlevelstart=20&#xA;&#xA;let g:ale_echo_msg_format = &amp;#39;[%linter%] %s [%severity%]&amp;#39;&#xA;let g:ale_sign_error = &amp;#39;✘&amp;#39;&#xA;let g:ale_sign_warning = &amp;#39;⚠&amp;#39;&#xA;let g:ale_lint_on_text_changed = &amp;#39;never&amp;#39;&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;In &lt;code&gt;~/.config/yamllint/config&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;extends: relaxed&#xA;&#xA;rules:&#xA;  line-length: disable&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/setting-up-vim-for-yaml/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Private Helm Repo with GCS and GitHub Actions</title>
    <updated>2020-03-08T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-08:/private-helm-repo-with-gcs-and-github-actions/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;In this blog post I&amp;#39;m going to show how to setup a private Helm chart repository on&#xA;Google Cloud Storage (GCS) and use GitHub Actions to automatically push charts on new commits.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Setting up the GCS Bucket&lt;/h2&gt;&#xA;        &lt;p&gt;The first step is to create a GCS bucket that will hold our charts. We can do this over the CLI with the gcloud-sdk or over the Web UI. I&amp;#39;m going to use the CLI for the following examples.&lt;/p&gt;&lt;p&gt;To make it easier to handle access permissions we use the &lt;code&gt;-b on&lt;/code&gt; flag to enable uniform bucket-level access. It let&amp;#39;s us manage permissions on a bucket-level rather than on an object-level:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil mb -b on gs://my-chart-repo-arthurk&#xA;&lt;span class=&#34;out&#34;&gt;Creating gs://my-chart-repo-arthurk/...&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For Helm to be able to push charts to this bucket we need a Cloud IAM service account (with key) with &lt;em&gt;Storage Object Admin&lt;/em&gt; permissions:&lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud iam service-accounts create my-chart-repo-svc-acc&#xA;&lt;span class=&#34;out&#34;&gt;Created service account [my-chart-repo-svc-acc].&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud iam service-accounts keys create service-account.json --iam-account=my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com&#xA;&lt;span class=&#34;out&#34;&gt;created key [123123123] of type [json] as [service-account.json] for [my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com]&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil iam ch serviceAccount:my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com:roles/storage.objectAdmin gs://my-chart-repo-arthurk&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;When referring to the service account we have to use the email (not the name) which has the format &lt;code&gt;SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Setting up GitHub Actions&lt;/h2&gt;&#xA;        &lt;p&gt;In this step we&amp;#39;re going to setup GitHub Actions to detect charts that have changed and add them to our&#xA;            helm repo.&lt;/p&gt;&#xA;        &lt;p&gt;We start by creating the &lt;code&gt;.github/workflows/helm-ci.yml&lt;/code&gt; file and add:&lt;/p&gt;&#xA;        &lt;pre&gt;name: Helm Charts&#xA;on: [push]&#xA;&#xA;jobs:&#xA;  release:&#xA;    name: Release&#xA;    runs-on: ubuntu-latest&#xA;    steps:&#xA;      - name: Checkout&#xA;        uses: actions/checkout@v2&#xA;        with:&#xA;          fetch-depth: 2&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;By default the checkout action will clone the repo with a detached HEAD. To later compare files that have changed between the current HEAD and the previous commit we have to pass &lt;code&gt;fetch-depth: 2&lt;/code&gt; to the action.&lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            After pushing the code we can open GitHub Actions in the browser and check the workflow. It should look like this:&#xA;        &lt;/p&gt;&#xA;&#xA;            &lt;img src=&#34;https://www.arthurkoziel.com/private-helm-repo-with-gcs-and-github-actions/gh-actions-1.png&#34; alt=&#34;GitHub Actions checkout step finished successfully&#34;/&gt;&#xA;&#xA;        &lt;h2&gt;Installing Helm and helm-gcs&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            The next step in the CI pipeline is to install Helm and the &lt;a href=&#34;https://github.com/hayorov/helm-gcs&#34;&gt;helm-gcs plugin&lt;/a&gt;. We add the following step to our workflow:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;- name: Install helm and plugins&#xA;  run: ./scripts/install.sh&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;and then create the &lt;code&gt;scripts/install.sh&lt;/code&gt; file with the following content:&lt;/p&gt;&#xA;        &lt;pre&gt;#!/usr/bin/env bash&#xA;&#xA;set -o errexit&#xA;&#xA;HELM_VERSION=3.1.1&#xA;HELM_GCS_VERSION=0.3.1&#xA;&#xA;echo &amp;#34;Installing Helm...&amp;#34;&#xA;wget -q https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz&#xA;tar -zxf helm-v${HELM_VERSION}-linux-amd64.tar.gz&#xA;sudo mv linux-amd64/helm /usr/local/bin/helm&#xA;helm version&#xA;&#xA;echo &amp;#34;Installing helm-gcs plugin...&amp;#34;&#xA;helm plugin install https://github.com/hayorov/helm-gcs --version ${HELM_GCS_VERSION}&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Set &lt;code&gt;chmod u+x scripts/install.sh&lt;/code&gt; and push the file. We can check GitHub Actions to make sure everything installed correctly:&lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;https://www.arthurkoziel.com/private-helm-repo-with-gcs-and-github-actions/gh-actions-2.png&#34; alt=&#34;GitHub Actions showing that helm and helm-gcs have been installed successfully&#34;/&gt;&#xA;&#xA;        &lt;p&gt;This shows us that Helm 3.1.1 and helm-gcs 0.3.0 have been successfully installed&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Initializing the helm repository&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;We can now initialize the helm repository. For this to work we need to add our previously created&#xA;            service account key to GitHub. To do this we navigate to the repository and click on&#xA;            &lt;strong&gt;&amp;#34;Settings&amp;#34; → &amp;#34;Secrets&amp;#34; → &amp;#34;Add a new secret&amp;#34;&lt;/strong&gt;. There we set the name to be&#xA;            &lt;code&gt;GCLOUD_SERVICE_ACCOUNT_KEY&lt;/code&gt; and as a value add the content of the service-account.json file.&#xA;            After saving the secret it should look like this:&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;https://www.arthurkoziel.com/private-helm-repo-with-gcs-and-github-actions/gh-actions-3.png&#34; alt=&#34;GitHub showing that a secret has been added to the project&#34;/&gt;&#xA;        &lt;p&gt;We can now modify the workflow to pass the secret as an environment variable to our next shell script:&lt;/p&gt;&#xA;&lt;pre&gt;- name: Release charts&#xA;  run: ./scripts/release.sh&#xA;  env:&#xA;    GCLOUD_SERVICE_ACCOUNT_KEY: ${{ secrets.GCLOUD_SERVICE_ACCOUNT_KEY }}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;In the release.sh script we save the service account to a file and point the&#xA;            &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt; environment variable to it. This is needed for helm-gcs&#xA;            plugin to authenticate. Afterwards we initialize the GCS repo which will create an&#xA;            empty &lt;code&gt;index.yaml&lt;/code&gt; file in the GCS bucket. Finally we can add the repo to helm&#xA;            so it can access its packages.&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;#!/usr/bin/env bash&#xA;&#xA;set -o errexit&#xA;set -o nounset&#xA;set -o pipefail&#xA;&#xA;GCS_BUCKET_NAME=&amp;#34;gs://my-chart-repo-arthurk&amp;#34;&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# setup service account for helm-gcs plugin&lt;/span&gt;&#xA;echo &amp;#34;${GCLOUD_SERVICE_ACCOUNT_KEY}&amp;#34; &amp;gt; svc-acc.json&#xA;export GOOGLE_APPLICATION_CREDENTIALS=svc-acc.json&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# initializing helm repo&lt;/span&gt;&#xA;&lt;span class=&#34;out&#34;&gt;# (only needed on first run but will do nothing if already exists)&lt;/span&gt;&#xA;echo &amp;#34;Initializing helm repo&amp;#34;&#xA;helm gcs init ${GCS_BUCKET_NAME}&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# add gcs bucket as helm repo&lt;/span&gt;&#xA;echo &amp;#34;Adding gcs bucket repo ${GCS_BUCKET_NAME}&amp;#34;&#xA;helm repo add private ${GCS_BUCKET_NAME}&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;Before committing the file make sure to mark it as executable with &lt;code&gt;chmod u+x scripts/release.sh&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Packaging and Pushing changed Charts&lt;/h2&gt;&#xA;        &lt;p&gt;In the final step of our CI script we need to identify which charts have changed and then&#xA;            package and push them to the helm repo. We do this by running &lt;code&gt;git diff&lt;/code&gt; on the previous&#xA;            revision with the following arguments:&lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&lt;code&gt;--find-renames&lt;/code&gt; detect if a file has been renamed&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;--diff-filter=d&lt;/code&gt; will ignore deleted files (we can&amp;#39;t package/push a deleted chart)&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;--name-only&lt;/code&gt; only print the name of the changed file&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;cut -d &amp;#39;/&amp;#39; -f 2 | uniq&lt;/code&gt; we only need unique directory names of files that have changed&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;We add the following content to the release.sh file:&lt;/p&gt;&#xA;&lt;pre&gt;prev_rev=$(git rev-parse HEAD^)&#xA;echo &amp;#34;Identifying changed charts since git rev ${prev_rev}&amp;#34;&#xA;&#xA;changed_charts=()&#xA;readarray -t changed_charts &amp;lt;&amp;lt;&amp;lt; &amp;#34;$(git diff --find-renames --diff-filter=d --name-only &amp;#34;$prev_rev&amp;#34; -- charts | cut -d &amp;#39;/&amp;#39; -f 2 | uniq)&amp;#34;&#xA;&#xA;if [[ -n &amp;#34;${changed_charts[*]}&amp;#34; ]]; then&#xA;    for chart in &amp;#34;${changed_charts[@]}&amp;#34;; do&#xA;        echo &amp;#34;Packaging chart &amp;#39;$chart&amp;#39;...&amp;#34;&#xA;        chart_file=$(helm package &amp;#34;charts/$chart&amp;#34; | awk &amp;#39;{print $NF}&amp;#39;)&#xA;&#xA;        echo &amp;#34;Pushing $chart_file...&amp;#34;&#xA;        helm gcs push &amp;#34;$chart_file&amp;#34; private&#xA;    done&#xA;else&#xA;    echo &amp;#34;No chart changes detected&amp;#34;&#xA;fi&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Commit and push the changes. After the CI run has finished, the GCS&#xA;            bucket will be initialized and have an &lt;code&gt;index.yaml&lt;/code&gt; file in it.&#xA;        This file is an index of all the helm charts in the repo. As we currently have no charts indexed it has&#xA;        the following content:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil cat gs://my-chart-repo-arthurk/index.yaml&#xA;&lt;span class=&#34;out&#34;&gt;apiVersion: v1&#xA;entries: {}&#xA;generated: &amp;#34;2020-03-08T06:51:49.496564824Z&amp;#34;&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Releasing our first chart&lt;/h2&gt;&#xA;        &lt;p&gt;We can now create and add the first chart to our helm repository. We do this by creating a &lt;em&gt;chart/&lt;/em&gt;&#xA;            directory and running &lt;code&gt;helm create&lt;/code&gt; to create an example chart:&#xA;&#xA;&lt;/p&gt;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; mkdir charts&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm create charts/foo&#xA;&lt;span class=&#34;out&#34;&gt;Creating charts/foo&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Add, commit and push all new files, then check GitHub Actions. It will show us that the chart&#xA;            was successfully packaged and pushed to the repo:&lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;https://www.arthurkoziel.com/private-helm-repo-with-gcs-and-github-actions/gh-actions-4.png&#34; alt=&#34;GitHub Actions showing that the chart has been successfully added to the repo&#34;/&gt;&#xA;&#xA;        &lt;p&gt;Note that it&amp;#39;s not possible to push the same chart version to the same repo. The push will fail.&#xA;        We need to always make sure to increase the &lt;code&gt;version&lt;/code&gt; value in the &lt;code&gt;Chart.yaml&lt;/code&gt; file when releasing a new chart.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Trying it out&lt;/h2&gt;&#xA;        &lt;p&gt;To try out our private helm repo we can add it to helm on our client machine and list the repo contents:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm plugin install https://github.com/hayorov/helm-gcs&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud auth application-default login&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm repo add private-repo gs://my-chart-repo-arthurk&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm repo update&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm search repo private-repo -l&#xA;&lt;span class=&#34;out&#34;&gt;NAME            &#x9;CHART VERSION&#x9;APP VERSION&#x9;DESCRIPTION&#xA;private-repo/foo&#x9;0.1.0        &#x9;1.16.0     &#x9;A Helm chart for Kubernetes&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;As you can see the chart was successfully added to the registry. It can now be used as any other chart, for example by installing it with &lt;code&gt;helm install private-repo/foo --version 0.1.0&lt;/code&gt;.&lt;/p&gt;&#xA;        &lt;p&gt;The source code for all examples is available &lt;a href=&#34;https://github.com/arthurk/private-gcs-helm&#34;&gt;in this GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/private-helm-repo-with-gcs-and-github-actions/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Writing Reusable Helm Charts</title>
    <updated>2020-03-01T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-01:/writing-reusable-helm-charts/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm charts&lt;/a&gt; make it possible to easily package &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; manifests, version them and share them with&#xA;            other developers. To use a Helm chart across projects with different requirements it needs to be &lt;em&gt;reusable&lt;/em&gt;, meaning&#xA;            that common parts of the Kubernetes manifests can be changed in a values file without having to re-write the templates.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Let&amp;#39;s say we are looking into deploying Prometheus via Helm into our Kubernetes cluster. We search&#xA;            around and find a chart that is stable, well documented and actively maintained. It looks like a good choice.&#xA;            But there are a few options that you need to change in order to fit our requirements. Normally this could be done&#xA;            by creating a &lt;code&gt;values.yaml&lt;/code&gt; file and overriding the default settings. However, the chart that is available&#xA;            is not reusable enough and the options that we need to change are not available.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In such a case the only option for us is to copy the whole chart and modify it to fit the requirements even if the modification&#xA;            is only 1 or 2 lines of code. After copying the chart we also have to maintain it and keep it up to date with the upstream&#xA;            branch. It would&amp;#39;ve saved us a lot of time and work if the chart added a few options to make it reusable&#xA;            for projects that have different requirements.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In the next sections I&amp;#39;m going to go over the templates from the default Helm chart template (that is created when running&#xA;            &lt;code&gt;helm create&lt;/code&gt;) and explain what makes them reusable (and what can be improved).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Ingress&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;An Ingress allows external users to access Kubernetes Services. It provides a reverse-proxy,&#xA;        configurable traffic routing and TLS termination. There are several Ingress controllers available&#xA;        such as &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;nginx&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes/ingress-gce&#34;&gt;GCE&lt;/a&gt;, &lt;a href=&#34;https://github.com/containous/traefik&#34;&gt;Traefik&lt;/a&gt;, &lt;a href=&#34;https://github.com/haproxytech/kubernetes-ingress/&#34;&gt;HAProxy&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34;&gt;and more&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;For a reusable Ingress template we need to consider the following requirements:&lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Using the Ingress should be optional. Not every developer wants to expose their service to external users&lt;/li&gt;&#xA;            &lt;li&gt;It should be possible to choose an ingress controller such as nginx or GCE&lt;/li&gt;&#xA;            &lt;li&gt;Traffic routing should be configurable&lt;/li&gt;&#xA;            &lt;li&gt;TLS should be optional&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;a href=&#34;https://gist.github.com/arthurk/d872e92fabfca4f2e6af84662da10106&#34;&gt;default Ingress template&lt;/a&gt; meets all of&#xA;        our requirements and is a great example of a reusable template. The custom annotations are a very important part.&#xA;        A typical usage example of would look like this:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;ingress:&#xA;  enabled: true&#xA;  annotations:&#xA;    kubernetes.io/ingress.class: nginx&#xA;    nginx.ingress.kubernetes.io/server-snippet: |&#xA;      add_header X-Frame-Options &amp;#34;DENY&amp;#34;;&#xA;      proxy_set_header X-Frame-Options &amp;#34;DENY&amp;#34;;&#xA;    certmanager.k8s.io/cluster-issuer: letsencrypt&#xA;    certmanager.k8s.io/acme-challenge-type: dns01&#xA;    certmanager.k8s.io/acme-dns01-provider: cloudflare&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;We have the ingress enabled and use nginx as a controller. We specify a custom&#xA;            &lt;code&gt;server-snippet&lt;/code&gt; (used by the nginx-ingress to inject custom code into the server config) that adds a custom header&#xA;        (&lt;code&gt;X-Frame-Options&lt;/code&gt;). We use annotations to signal &lt;a href=&#34;cert-manager.readthedocs.io/&#34;&gt;cert-manager&lt;/a&gt; to&#xA;        provision a SSL certificate for this host.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Other ingress controllers such as &lt;a href=&#34;https://github.com/kubernetes/ingress-gce&#34;&gt;GCE&lt;/a&gt; also make use of annotations&#xA;        to integrate with Google Cloud services. In this example we assign an external static IP and provision an SSL certificate&#xA;        (with &lt;a href=&#34;http://github.com/GoogleCloudPlatform/gke-managed-certs&#34;&gt;gke-managed-certs&lt;/a&gt;):&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;ingress:&#xA;  enabled: true&#xA;  annotations:&#xA;    kubernetes.io/ingress.class: gce&#xA;    kubernetes.io/ingress.global-static-ip-name: my-external-ip&#xA;    kubernetes.io/ingress.allow-http: &amp;#39;false&amp;#39;&#xA;    networking.gke.io/managed-certificates: example-certificate&#xA;  hosts:&#xA;    - host: example.org&#xA;      paths:&#xA;        - &amp;#34;/*&amp;#34;&#xA;  tls:&#xA;    - hosts:&#xA;        - example.org&#xA;      secretName: &amp;#34;example-org-tls&amp;#34;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Service&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        A Service is an abstraction for a grouping of pods. It selects pods based on labels and&#xA;        allows network access to them. There are several Service types that Kubernetes supports such as&#xA;        ClusterIP, LoadBalancer or NodePort.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The requirements for a Service template in a reusable Helm chart are:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;          &lt;li&gt;It should be possible to pick a Service type. Not everyone wants to run an application behind a Load Balancer&lt;/li&gt;&#xA;          &lt;li&gt;It should be possible to add annotations&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            These are fairly simple requirements and the &lt;a href=&#34;https://gist.github.com/arthurk/e7bec72e9e7f4ea8785656f582846421&#34;&gt;default Service template&lt;/a&gt; meets all of them. A usage example looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;service:&#xA;  type: ClusterIP&#xA;  port: 80&#xA;  annotations:&#xA;    prometheus.io/scrape: &amp;#34;true&amp;#34;&#xA;    prometheus.io/port: &amp;#34;4000&amp;#34;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The service has a &lt;code&gt;ClusterIP&lt;/code&gt; Service type. In environments where higher availability is required&#xA;        this could be changed to a &lt;code&gt;LoadBalancer&lt;/code&gt;. The annotations are used by the Prometheus Helm chart:&#xA;        The prometheus server looks for all services in a cluster that have the &lt;code&gt;prometheus.io/scrape: &amp;#34;true&amp;#34;&lt;/code&gt;&#xA;        annotation and automatically scrapes them every minute.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Deployment&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        A Deployment is an abstraction for pods. It runs multiple replicas of an application and keeps them&#xA;        in the desired state. If an application fails or becomes unresponsive it will be replaced automatically.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;In a reusable Deployment template we should be able to:&lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Set the number of replicas&lt;/strong&gt;: Depending on the environment we should be able to adjust&#xA;                this value. A test environment doesn&amp;#39;t need to run as many replicas as a production environment.&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add Pod annotations&lt;/strong&gt;: Applications such as &lt;a href=&#34;https://linkerd.io/&#34;&gt;Linkerd&lt;/a&gt; use&#xA;                annotations to identify Pods into which to inject a sidecar container (&lt;code&gt;linkerd.io/inject: enabled&lt;/code&gt;)&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Pull the Docker image from a custom (private) registry&lt;/strong&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Modify arguments and environment variables&lt;/strong&gt;: As an example&#xA;                we should be able to pass &lt;code&gt;--log.level=debug&lt;/code&gt; to a container to see debug logs in case we have&#xA;                to identify problems with our application. Environment variables such as &lt;code&gt;MIX_ENV=prod&lt;/code&gt; often&#xA;                tell the application in which environment it is running and which configuration it should load&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add custom ConfigMaps and Secrets&lt;/strong&gt;: It should be possible to load application-specific&#xA;                configuration files or secrets that were added externally (for example SSL certificates for a database connection or API keys)&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add Liveness and Readiness probes&lt;/strong&gt; to check if the container is started and alive&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Configure container resource limits and requests&lt;/strong&gt;: In test or staging environments we should be able to&#xA;                disable it or set it to a low value&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Run Sidecar Containers&lt;/strong&gt;: If the application requires a database connection but the database is on CloudSQL it&#xA;                is often recommended to run &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloudsql-proxy/&#34;&gt;cloudsql-proxy&lt;/a&gt; as a&#xA;                sidecar container to establish a secure connection to the database&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Allow to set Affinity and Tolerations&lt;/strong&gt;: To optimize the performance of the application we should be able&#xA;                to run it on the same machine as certain other applications or have it scheduled on a specific node pool&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;Unlike the Ingress and Service templates, the &lt;a href=&#34;https://gist.github.com/arthurk/5f833ec5b264b84b6e1bedbd8eac69ea&#34;&gt;default template&lt;/a&gt; doesn&amp;#39;t meet the requirements from above. Specifically we need to allow to:&#xA;&#xA;&lt;/p&gt;&lt;ul&gt;&#xA;    &lt;li&gt;Add Pod annotations so that other applications such as Linkerd know where to inject sidecar containers&lt;/li&gt;&#xA;    &lt;li&gt;Replace &lt;code&gt;appVersion&lt;/code&gt; with &lt;code&gt;image.tag&lt;/code&gt;. This allows to change the docker image tag&#xA;        without having to re-package the chart with a different &lt;code&gt;appVersion&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;extraArgs&lt;/code&gt; to allow additional arguments to be passed into the container&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;env&lt;/code&gt; to allow additional environment variables to be passed into the container&lt;/li&gt;&#xA;    &lt;li&gt;Replace the default livenessProbe/readinessProbe with a block that allows us to set all values (the default&#xA;        template only allows &lt;code&gt;httpGet&lt;/code&gt; probes on &lt;code&gt;/&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;extraVolumes&lt;/code&gt; and &lt;code&gt;extraVolumeMounts&lt;/code&gt; to allow mounting of custom ConfigMaps and Secrets&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;sidecarContainers&lt;/code&gt; to allow to inject additional containers into the pod&#xA;        (such as &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloudsql-proxy/&#34;&gt;cloudsql-proxy&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The modified template code looks as follows (&lt;span class=&#34;diff_add&#34;&gt;green&lt;/span&gt; text marks added and changed code):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: {{ include &amp;#34;mychart.fullname&amp;#34; . }}&#xA;  labels:&#xA;    {{- include &amp;#34;mychart.labels&amp;#34; . | nindent 4 }}&#xA;spec:&#xA;  replicas: {{ .Values.replicaCount }}&#xA;  selector:&#xA;    matchLabels:&#xA;      {{- include &amp;#34;mychart.selectorLabels&amp;#34; . | nindent 6 }}&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        {{- include &amp;#34;mychart.selectorLabels&amp;#34; . | nindent 8 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;      annotations:&#xA;      {{- if .Values.podAnnotations }}&#xA;        {{- toYaml .Values.podAnnotations | nindent 8 }}&#xA;      {{- end }}&lt;/span&gt;&#xA;    spec:&#xA;    {{- with .Values.imagePullSecrets }}&#xA;      imagePullSecrets:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;      serviceAccountName: {{ include &amp;#34;mychart.serviceAccountName&amp;#34; . }}&#xA;      securityContext:&#xA;        {{- toYaml .Values.podSecurityContext | nindent 8 }}&#xA;      containers:&#xA;        - name: {{ .Chart.Name }}&#xA;          securityContext:&#xA;            {{- toYaml .Values.securityContext | nindent 12 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          image: &amp;#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}&amp;#34;&lt;/span&gt;&#xA;          imagePullPolicy: {{ .Values.image.pullPolicy }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          args:&#xA;          {{- range $key, $value := .Values.extraArgs }}&#xA;            - --{{ $key }}={{ $value }}&#xA;          {{- end }}&#xA;          {{- if .Values.env }}&#xA;          env:&#xA;            {{ toYaml .Values.env | nindent 12}}&#xA;          {{- end }}&lt;/span&gt;&#xA;          ports:&#xA;            - name: http&#xA;              containerPort: 80&#xA;              protocol: TCP&lt;span class=&#34;diff_add&#34;&gt;&#xA;          {{- with .Values.livenessProbe }}&#xA;          livenessProbe:&#xA;            {{- toYaml . | nindent 12 }}&#xA;          {{- end }}&#xA;          {{- with .Values.readinessProbe }}&#xA;          readinessProbe:&#xA;            {{- toYaml . | nindent 12 }}&#xA;          {{- end }}&lt;/span&gt;&#xA;          resources:&#xA;            {{- toYaml .Values.resources | nindent 12 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          volumeMounts:&#xA;          {{- if .Values.extraVolumeMounts }}&#xA;          {{ toYaml .Values.extraVolumeMounts | nindent 12 }}&#xA;          {{- end }}&#xA;       {{- if .Values.sidecarContainers }}&#xA;       {{- toYaml .Values.sidecarContainers | nindent 8 }}&#xA;       {{- end }}&#xA;      volumes:&#xA;      {{- if .Values.extraVolumes }}&#xA;      {{ toYaml .Values.extraVolumes | nindent 8}}&#xA;      {{- end }}&lt;/span&gt;&#xA;      {{- with .Values.nodeSelector }}&#xA;      nodeSelector:&#xA;        {{- toYaml . | nindent 8 }}&#xA;      {{- end }}&#xA;    {{- with .Values.affinity }}&#xA;      affinity:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;    {{- with .Values.tolerations }}&#xA;      tolerations:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;    &lt;p&gt;&#xA;    The default helm chart template is a great starting point for building reusable helm charts. The Ingress and Service&#xA;    templates are perfect examples. The Deployment template is lacking a few options to be reusable enough but can&#xA;    easily be modified and improved.&#xA;    &lt;/p&gt;&#xA;&#xA;    &lt;p&gt;&#xA;    For good examples of reusable Helm charts I recommend checking the&#xA;    &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable&#34;&gt;helm/charts stable repo&lt;/a&gt;.&#xA;    Charts such as &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus&#34;&gt;Prometheus&lt;/a&gt;, &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/grafana&#34;&gt;Grafana&lt;/a&gt; or &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/nginx-ingress&#34;&gt;nginx-ingress&lt;/a&gt; are actively maintained and constantly improved. They are good references to look at when writing a new Helm chart.&#xA;    &lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/writing-reusable-helm-charts/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Pre-Installed Daemons on Google Compute Engine</title>
    <updated>2020-02-23T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-23:/pre-installed-daemons-on-google-compute-engine/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;I found out that Google Compute Engine instances will come with the &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Google Guest Environment&lt;/a&gt; pre-installed which runs daemons in the background. This is unlike AWS EC2 instances which don&amp;#39;t install any daemons (but come with &lt;a href=&#34;https://github.com/aws/aws-cli&#34;&gt;aws-cli&lt;/a&gt; pre-installed). We can see the following output when listing the running processes on a new Debian GCE intance:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ ps ax | grep google&#xA;&#xA;418 ?        Ssl    2:52 /usr/bin/google_osconfig_agent&#xA;526 ?        Ss     2:17 /usr/bin/python3 /usr/bin/google_network_daemon&#xA;528 ?        Ss     3:32 /usr/bin/python3 /usr/bin/google_accounts_daemon&#xA;529 ?        Ss     1:14 /usr/bin/python3 /usr/bin/google_clock_skew_daemon&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can check all installed google packages:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ apt list --installed | grep google&#xA;&#xA;gce-disk-expand&#xA;google-cloud-packages-archive-keyring&#xA;google-cloud-sdk&#xA;google-compute-engine-oslogin&#xA;google-compute-engine&#xA;google-osconfig-agent&#xA;python-google-compute-engine&#xA;python3-google-compute-engine&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and systemd services:&lt;/p&gt;&#xA;&lt;pre&gt;$ systemctl list-unit-files | grep google&#xA;&#xA;google-accounts-daemon.service         enabled&#xA;google-clock-skew-daemon.service       enabled&#xA;google-instance-setup.service          enabled&#xA;google-network-daemon.service          enabled&#xA;google-osconfig-agent.service          enabled&#xA;google-shutdown-scripts.service        enabled&#xA;google-startup-scripts.service         enabled&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;These packages and services are part of the Google &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Linux Guest Environment&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/managing-instance-access&#34;&gt;OS Login Guest Environment&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The GCP docs have some information on the &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Guest Environment&lt;/a&gt; but it lacks details on the specifics of each daemon/script. A better source is the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/compute-image-packages/tree/master/packages/python-google-compute-engine&#34;&gt;GitHub repo&lt;/a&gt; where we can find a good explanation for each daemon and script:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-network-daemon:&lt;/strong&gt; handles network setup for multiple network interfaces on boot and integrates network load balancing with forwarding rule changes into the guest&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-accounts-daemon:&lt;/strong&gt; daemon to setup and manage user accounts, and to enable SSH key based authentication&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-clock-skew-daemon:&lt;/strong&gt; daemon to keep the system clock in sync after VM start and stop events&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-instance-setup:&lt;/strong&gt; scripts to execute VM configuration scripts during boot&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-startup-scripts/google-shutdown-scripts:&lt;/strong&gt; run user-provided scripts at VM startup and shutdown&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The remaining daemon is the agent for the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/guest-oslogin&#34;&gt;OS Login Guest Environment&lt;/a&gt;. It manages access control when using the &lt;a href=&#34;https://cloud.google.com/compute/docs/oslogin/&#34;&gt;OS Login&lt;/a&gt; feature by linking linux user accounts to Google accounts (which can then be managed with Cloud IAM). This feature is disabled by default and I&amp;#39;m not sure why the package is installed and the daemon is running.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Uninstall&lt;/h2&gt;&#xA;&lt;p&gt;If all that&amp;#39;s needed is a simple VM instance without Google Cloud integration, all daemons and scripts can be uninstalled by removing the packages:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ apt-get remove python-google-compute-engine python3-google-compute-engine \&#xA;                 google-osconfig-agent google-compute-engine-oslogin&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;I think it&amp;#39;s good to at least remove the &lt;code&gt;google-osconfig-agent&lt;/code&gt; package and get rid of the &lt;code&gt;google_osconfig_agent&lt;/code&gt; daemon running in the background. The package can be re-installed before enabling OS Login.&lt;/p&gt;&#xA;&lt;p&gt;Each daemon can also be disabled separately:&lt;/p&gt;&#xA;&lt;pre&gt;$ systemctl disable google-accounts-daemon.service&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/pre-installed-daemons-on-google-compute-engine/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Generating Ethereum Addresses in Python</title>
    <updated>2020-02-16T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-16:/generating-ethereum-addresses-in-python/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            I&amp;#39;ve been wondering how long it would take to generate all Ethereum private keys with addresses on my laptop.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            I know there is &lt;a href=&#34;https://bitcointalk.org/index.php?topic=7769.msg1010711#msg1010711&#34;&gt;not enough energy in our star system&lt;/a&gt;&#xA;            to do this in a reasonable timeframe, even on an imaginative computer that would use the absolute minimum of energy possible.&#xA;            This was more of a learning experience for me to get to know more about SHA-3 and KECCAK hashes,&#xA;            ECDSA curves, Public Keys and Ethereum addresses.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Due to its slow interpreter, Python is usually not a good choice when it comes to writing performant applications.&#xA;            The exception being Python modules which use an interface that calls C/C++ code. These modules are usually very fast,&#xA;            popular examples are &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&#34;https://numpy.org/&#34;&gt;Numpy&lt;/a&gt;.&#xA;            To generate Ethereum addresses we can use the following two Python modules which are both C based and have a good performance:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;https://github.com/ofek/coincurve/&#34;&gt;coincurve&lt;/a&gt;: Cross-platform Python CFFI bindings for libsecp256k1&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;https://github.com/tiran/pysha3&#34;&gt;pysha3&lt;/a&gt;: SHA-3 wrapper for Python (with support for keccak)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            Generating Ethereum addresses is a 3-step process:&#xA;        &lt;/p&gt;&#xA;        &lt;ol&gt;&#xA;            &lt;li&gt;Generate a private key&lt;/li&gt;&#xA;            &lt;li&gt;Derive the public key from the private key&lt;/li&gt;&#xA;            &lt;li&gt;Derive the Ethereum address from the public key&lt;/li&gt;&#xA;        &lt;/ol&gt;&#xA;        &lt;p&gt;&#xA;            Note that public keys and Ethereum addresses are not the same. Addresses are hashes of public keys.&#xA;            It&amp;#39;s not possible to send funds to a public key.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 1: Generate a private key&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Ethereum private keys are based on &lt;a href=&#34;https://keccak.team/keccak.html&#34;&gt;KECCAK-256 hashes&lt;/a&gt;. To generate such a hash we&#xA;            use the &lt;code&gt;keccak_256&lt;/code&gt; function from the pysha3 module on a random 32 byte seed:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;import secrets&#xA;from sha3 import keccak_256&#xA;&#xA;private_key = keccak_256(secrets.token_bytes(32)).digest()&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note that a KECCAK hash is not the same as a SHA-3 hash.&#xA;            KECCAK won a competition to become the SHA-3 standard but was slightly modified&#xA;            before it became standardized. Some SHA3 libraries such as pysha3 include&#xA;            the legacy KECCAK algorithm while others, such as the &lt;a href=&#34;https://docs.python.org/3.7/library/hashlib.html&#34;&gt;Python hashlib module&lt;/a&gt;,&#xA;            only implement the official SHA-3 standard.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 2: Derive the public key from the private key&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To get our public key we need to sign our private key with an Elliptic Curve Digital Signature Algorithm (ECDSA).&#xA;            Ethereum uses the &lt;a href=&#34;https://en.bitcoin.it/wiki/Secp256k1&#34;&gt;secp256k1 curve ECDSA&lt;/a&gt;. Coincurve uses this as a default&#xA;            so we don&amp;#39;t need to explicitly specify it when calling the function:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;from coincurve import PublicKey&#xA;&#xA;public_key = PublicKey.from_valid_secret(private_key).format(compressed=False)[1:]&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The &lt;a href=&#34;https://ethereum.github.io/yellowpaper/paper.pdf&#34;&gt;Ethereum Yellow Paper&lt;/a&gt; states that the public key has to be a byte array of size 64.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            By default coincurve uses the compressed format for public keys (libsecp256k1 was developed for Bitcoin where compressed keys are commonly used) which is 33 bytes in size.&#xA;            Uncompressed keys are 65 bytes in size. Additionally all public keys are&#xA;            prepended with a single byte to indicate if they are compressed or uncompressed.&#xA;            This means we first need to get the uncompressed 65 byte key (&lt;code&gt;compressed=False&lt;/code&gt;)&#xA;            and then strip the first byte (&lt;code&gt;[1:]&lt;/code&gt;) to get our 64 byte Ethereum public key.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 3: Derive the Ethereum address from the public key&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now generate our Ethereum address:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;addr = keccak_256(public_key).digest()[-20:]&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;As specified in the &lt;a href=&#34;https://ethereum.github.io/yellowpaper/paper.pdf&#34;&gt;Yellow Paper&lt;/a&gt; we take the right most 20 bytes of the 32 byte KECCAK hash of the corresponding ECDSA public key.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Full Example&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            This is the full example code from the above steps. It generates a random private key, derives the address&#xA;            and prints them in hex format:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;from secrets import token_bytes&#xA;from coincurve import PublicKey&#xA;from sha3 import keccak_256&#xA;&#xA;private_key = keccak_256(token_bytes(32)).digest()&#xA;public_key = PublicKey.from_valid_secret(private_key).format(compressed=False)[1:]&#xA;addr = keccak_256(public_key).digest()[-20:]&#xA;&#xA;print(&amp;#39;private_key:&amp;#39;, private_key.hex())&#xA;print(&amp;#39;eth addr: 0x&amp;#39; + addr.hex())&#xA;&#xA;### Output ###&#xA;# private_key: 7bf19806aa6d5b31d7b7ea9e833c202e51ff8ee6311df6a036f0261f216f09ef&#xA;# eth addr: 0x3db763bbbb1ac900eb2eb8b106218f85f9f64a13&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            I used the Python &lt;code&gt;timeit&lt;/code&gt; module to do &lt;a href=&#34;https://gist.github.com/arthurk/fbc876951379e2b0c889ea71b5167b4e&#34;&gt;a quick benchmark&lt;/a&gt; with the above code.&#xA;            The result is that my laptop can generate 18k addresses per second on a single cpu core.&#xA;            Using all 4 cpu cores that&amp;#39;s 72k addresses per second, ~6.2 billion (6.220.800.000) addresses per day or around&#xA;            two trillion (2.270.592.000.000) addresses per year.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Ethereum&amp;#39;s address space is 2^160. This means that by using this method it would take my laptop&#xA;            643665439999999976814879449351716864 (six hundred and forty-three decillion ...) years&#xA;            to generate all Ethereum private keys with addresses.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/generating-ethereum-addresses-in-python/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Validating Helm Chart Values with JSON Schemas</title>
    <updated>2020-02-08T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-08:/validate-helm-chart-values-with-json-schemas/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;Helm v3 &lt;a href=&#34;https://github.com/helm/helm/pull/5350&#34;&gt;added support&lt;/a&gt; to validate values in a chart&amp;#39;s &lt;code&gt;values.yaml&lt;/code&gt; file with &lt;a href=&#34;https://json-schema.org/&#34;&gt;JSON schemas&lt;/a&gt;. It allows us to do:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;Requirement checks. Example: An &lt;code&gt;API_KEY&lt;/code&gt; environment variable is set&lt;/li&gt;&#xA;    &lt;li&gt;Type validation. Example: The image tag is a string such as &lt;code&gt;&amp;#34;1.5&amp;#34;&lt;/code&gt; and not the number &lt;code&gt;1.5&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Range validation. Example: The value for a CPU utilization percentage key is between 1 and 100&lt;/li&gt;&#xA;    &lt;li&gt;Constraint Validation. Example: The &lt;code&gt;pullPolicy&lt;/code&gt; is &lt;code&gt;IfNotPresent&lt;/code&gt;, &lt;code&gt;Always&lt;/code&gt;, or &lt;code&gt;Never&lt;/code&gt;; A URL has the format &lt;code&gt;http(s)://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In this post I&amp;#39;m going to show how to create a JSON schema and use it to validate a chart&amp;#39;s &lt;code&gt;values.yaml&lt;/code&gt; file. After that I&amp;#39;m going to show how to automatically generate a schema from an existing values file.&lt;/p&gt;&lt;p&gt;&#xA;&#xA;&lt;/p&gt;&lt;h2&gt;Example&lt;/h2&gt;&#xA;&lt;p&gt;For this example I&amp;#39;m using the chart that is created when running &lt;code&gt;helm create mychart&lt;/code&gt;. We&amp;#39;ll create a JSON schema that will validate that the following conditions are met:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;code&gt;image.repository&lt;/code&gt; is a valid docker image name&lt;/li&gt;&#xA;    &lt;li&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt; is &lt;code&gt;IfNotPresent&lt;/code&gt;, &lt;code&gt;Always&lt;/code&gt; or &lt;code&gt;Never&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;The relevant part in the &lt;code&gt;values.yaml&lt;/code&gt; file is:&#xA;&#xA;&lt;pre&gt;image:&#xA;  repository: my-docker-image&#xA;  pullPolicy: IfNotPresent&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The JSON schema needs to be in a file named &lt;code&gt;values.schema.json&lt;/code&gt;. It has to be located in the same directory as the &lt;code&gt;values.yaml&lt;/code&gt; file. To match the requirements from above the file needs to have the following content:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;{&#xA;  &amp;#34;$schema&amp;#34;: &amp;#34;http://json-schema.org/schema#&amp;#34;,&#xA;  &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;,&#xA;  &amp;#34;required&amp;#34;: [&#xA;    &amp;#34;image&amp;#34;&#xA;  ],&#xA;  &amp;#34;properties&amp;#34;: {&#xA;    &amp;#34;image&amp;#34;: {&#xA;      &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;,&#xA;      &amp;#34;required&amp;#34;: [&#xA;        &amp;#34;repository&amp;#34;,&#xA;        &amp;#34;pullPolicy&amp;#34;&#xA;      ],&#xA;      &amp;#34;properties&amp;#34;: {&#xA;        &amp;#34;repository&amp;#34;: {&#xA;          &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,&#xA;          &amp;#34;pattern&amp;#34;: &amp;#34;^[a-z0-9-_]+$&amp;#34;&#xA;        },&#xA;        &amp;#34;pullPolicy&amp;#34;: {&#xA;          &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,&#xA;          &amp;#34;pattern&amp;#34;: &amp;#34;^(Always|Never|IfNotPresent)$&amp;#34;&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;Note that putting a key in the &lt;code&gt;required&lt;/code&gt; array does not mean that it has a value. In YAML if a key doesn&amp;#39;t have a value it will be set to an empty string. To make sure the value was set, a regex for the &lt;code&gt;pattern&lt;/code&gt; key has to be added that matches a non-empty string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To demonstrate that the validation is working I&amp;#39;m leaving the &lt;code&gt;repo&lt;/code&gt; empty and set &lt;code&gt;pullPolicy&lt;/code&gt; to an invalid value. Running lint shows the following output:&lt;/p&gt;&#xA;&lt;pre&gt;$ helm lint .&#xA;&#xA;==&amp;gt; Linting .&#xA;[ERROR] values.yaml: - image.repository: Invalid type. Expected: string, given: null&#xA;- image.pullPolicy: Does not match pattern &amp;#39;^(Always|Never|IfNotPresent)$&amp;#39;&#xA;&#xA;[ERROR] templates/: values don&amp;#39;t meet the specifications of the schema(s) in the following chart(s):&#xA;mychart:&#xA;- image.repository: Invalid type. Expected: string, given: null&#xA;- image.pullPolicy: Does not match pattern &amp;#39;^(Always|Never|IfNotPresent)$&amp;#39;&#xA;&#xA;Error: 1 chart(s) linted, 1 chart(s) failed&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The schema is automatically validated when running the following commands:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;helm install&lt;/li&gt;&#xA;    &lt;li&gt;helm upgrade&lt;/li&gt;&#xA;    &lt;li&gt;helm lint&lt;/li&gt;&#xA;    &lt;li&gt;helm template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The YAML values and the JSON schema need to be kept in sync manually. Helm will not check if keys from the YAML values file are missing from the schema. It will only validate fields that are specified in the schema.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating a JSON Schema for existing YAML values&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can infer a schema from existing YAML values and use it as a starting point when writing a new schema. The steps are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;    &lt;li&gt;Convert your values YAML file to JSON on &lt;a href=&#34;https://www.json2yaml.com/&#34;&gt;https://www.json2yaml.com/&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Paste the JSON on &lt;a href=&#34;https://www.jsonschema.net&#34;&gt;https://www.jsonschema.net/&lt;/a&gt; and click on &amp;#34;Infer Schema&amp;#34;&lt;/li&gt;&#xA;    &lt;li&gt;Paste the schema into the &lt;code&gt;values.schema.json&lt;/code&gt; file&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;We can run &lt;code&gt;helm lint&lt;/code&gt; to make sure the schema has been generated correctly:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ helm lint mychart/&#xA;&#xA;==&amp;gt; Linting .&#xA;&#xA;1 chart(s) linted, 0 chart(s) failed&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The inferred schema will mark all keys as required and set their type. A regex can be added to keys to make sure they have a value set. The &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;default&lt;/code&gt; and &lt;code&gt;examples&lt;/code&gt; fields are not necessary for validating helm charts and can be removed.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/validate-helm-chart-values-with-json-schemas/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Extracting Data from Invoices with Google AutoML Natural Language</title>
    <updated>2020-02-02T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-02:/automl-invoice-data-extraction/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;    &lt;p&gt;In this tutorial I will show how to use &lt;a href=&#34;https://cloud.google.com/natural-language/automl/docs/&#34;&gt;Google AutoML Natural Language&lt;/a&gt; to setup a machine learning model that will automatically extract the total from invoices.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Why?&lt;/h2&gt;&#xA;    &lt;p&gt;Manually extracting data from invoices and entering them into an accounting system is time-consuming and tedious work.&lt;/p&gt;&#xA;    &lt;p&gt;To automate this there are template-based systems like &lt;a href=&#34;https://github.com/invoice-x/invoice2data&#34;&gt;invoice2data&lt;/a&gt; available. They extract the data using predefined extraction rules (regular expressions):&lt;/p&gt;&#xA;    &lt;pre&gt;Invoice Total: \$(\d+.\d{2})&lt;/pre&gt;&#xA;    &lt;p&gt;With such a system there&amp;#39;s still manual work required. YAML templates with extraction rules need to be written for each supplier and then maintained as invoice structures change over time. In the example above the supplier could decide to change &lt;code&gt;Invoice Total&lt;/code&gt; to &lt;code&gt;Total&lt;/code&gt; on the next invoice. The extraction would fail and the rule would have to be adjusted.&lt;/p&gt;&#xA;    &lt;p&gt;A better solution is to use a machine learning model that can extract the information without writing extraction rules. In this post I&amp;#39;m going to show how to setup such a model.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Steps To Do&lt;/h2&gt;&#xA;    &lt;p&gt;To build our invoice data extraction ML model we have to do the following steps:&lt;/p&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;Collect the training documents&lt;/li&gt;&#xA;        &lt;li&gt;Upload the documents to Google Cloud Storage&lt;/li&gt;&#xA;        &lt;li&gt;Create a CSV and JSONL file for AutoML to import the uploaded documents&lt;/li&gt;&#xA;        &lt;li&gt;Import the documents&lt;/li&gt;&#xA;        &lt;li&gt;Annotate the documents&lt;/li&gt;&#xA;        &lt;li&gt;Build/Train the model&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;p&gt;After that we can manually test the model by uploading invoices and checking how well it&amp;#39;s able to extract the data.&lt;/p&gt;&#xA;    &lt;p&gt;I&amp;#39;m using Google AutoML Natural Language which is part of Google Cloud Platform and a GCP account is required. Regarding cost there are two things to note: Google will give $300 free credit for new GCP accounts and $25 promotional credit for developers using AutoML for the first time.&lt;/p&gt;&#xA;    &lt;p&gt;Note that for the &lt;code&gt;gsutil&lt;/code&gt; commands below I installed the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud SDK&lt;/a&gt; but it&amp;#39;s not necessary to do so, the GCS operations from below can also be done over the Web UI.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Training Documents&lt;/h2&gt;&#xA;    &lt;p&gt;The first step is to collect training documents that are structured in the same way as the documents we want the model to handle later on. I was able to collect 150 invoices from different consultants. All invoices are text based PDF files with either 1 or 2 pages and the total written somewhere in the bottom right (single page invoices) or top-right (two-page invoices).&lt;/p&gt;&#xA;    &lt;p&gt;This is almost the minimum amount of training documents that is required by AutoML to work. Feel free to add more. The more training documents the better the model&amp;#39;s performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It&amp;#39;s important to use documents that have a similar structure. AutoML will use those documents to build the model by trying out different algorithms to find patterns. If those documents are structured differently it won&amp;#39;t be able to find any patterns and the model will have a poor performance.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Uploading Documents to GCS&lt;/h2&gt;&#xA;&lt;p&gt;All documents need to be stored in a Google Cloud Storage (GCS) bucket. AutoML doesn&amp;#39;t support other document sources. There are three important restrictions when creating the bucket:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Location Type&lt;/em&gt; has to be &lt;code&gt;Region&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Location&lt;/em&gt; has to be &lt;code&gt;us-central1 (Iowa)&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Storage Class&lt;/em&gt; has to be &lt;code&gt;Standard&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The bucket can be created by running:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil mb -l us-central1 gs://automl-nlp-example&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have all my training documents in a folder called &lt;code&gt;invoices/&lt;/code&gt; and named with the same pattern: &lt;code&gt;invoice-X&lt;/code&gt; where X is the number of the invoice (1 to 150). I recommend using the same pattern for all the invoice files as we will need to iterate over them later on. We can upload the folder with the invoices by running:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;gsutil -m cp -r invoices/ gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Importing the documents&lt;/h2&gt;&#xA;&lt;p&gt;For AutoML to be able to import the training documents we need to create a CSV file. The CSV file contains a link to a &lt;a href=&#34;http://jsonlines.org/&#34;&gt;JSONL&lt;/a&gt; document and the JSONL file then contains links to the actual invoice PDF files. The files are imported in the following way:&lt;/p&gt;&#xA;&lt;p&gt;AutoML -&amp;gt; CSV file → JSONL file → PDF file(s)&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating the CSV file&lt;/h2&gt;&#xA;&lt;p&gt;Creating the CSV file is simple and requires only one line:&lt;/p&gt;&lt;p&gt;&#xA;&lt;/p&gt;&lt;pre&gt;,gs://automl-nlp-example/data.jsonl&lt;/pre&gt;&#xA;&lt;p&gt;It&amp;#39;s important to have the comma at the beginning of the line which will make AutoML randomly assign the documents to different sets (TRAIN, VALIDATION, TEST). It will use:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;80% of the PDF documents for training the model&lt;/li&gt;&#xA;    &lt;li&gt;10% for validating the results during training&lt;/li&gt;&#xA;    &lt;li&gt;10% for verifying the model&amp;#39;s results after it has been trained&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We can upload the CSV file to GCS:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil cp data.csv gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Creating the JSONL File&lt;/h2&gt;&#xA;&lt;p&gt;The JSONL file contains links to the invoice PDF documents. Each line links to one PDF document and needs to have the following structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;{&amp;#34;document&amp;#34;: {&amp;#34;input_config&amp;#34;: {&amp;#34;gcs_source&amp;#34;: {&amp;#34;input_uris&amp;#34;: [ &amp;#34;gs://automl-nlp-example/invoice-1.pdf&amp;#34; ]}}}}&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We need to repeat this line for all documents and change the value of &lt;code&gt;input_uris&lt;/code&gt; to use the actual filename of the PDF file. I used a small Python script to generate the file (this is easy since my documents follow the same filename pattern) and upload it to GCS:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;python3 -c &amp;#39;for x in range(1, 151): print(&amp;#34;&amp;#34;&amp;#34;{&amp;#34;document&amp;#34;: {&amp;#34;input_config&amp;#34;: {&amp;#34;gcs_source&amp;#34;: {&amp;#34;input_uris&amp;#34;: [ &amp;#34;gs://automl-nlp-example/invoice-%s.pdf&amp;#34; ]}}}}&amp;#34;&amp;#34;&amp;#34; % x)&amp;#39; &amp;gt; data.jsonl&#xA;&#xA;gsutil cp data.jsonl gs://automl-nlp-example/&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now we should have the following files in the GCS bucket:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;The &lt;code&gt;invoice/&lt;/code&gt; directory contains all invoices as PDF files&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;code&gt;data.csv&lt;/code&gt; file contains a link to the JSONL file&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;code&gt;data.jsonl&lt;/code&gt; file contains links to the PDF files&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Creating the AutoML Dataset&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can start creating the dataset in the GCP console. Go to &lt;em&gt;Natual Language&lt;/em&gt; and then &lt;em&gt;AutoML Entity Extraction&lt;/em&gt; to create the dataset. The location has to be &lt;em&gt;Global&lt;/em&gt; and the model objective has to be &lt;em&gt;Entity Extraction&lt;/em&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/automl-invoice-data-extraction/create-new-dataset.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The CSV file can be imported from the GCS bucket at the bottom of the screen:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/automl-invoice-data-extraction/import-csv.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case the import process took &lt;em&gt;13 minutes&lt;/em&gt; to finish.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/automl-invoice-data-extraction/import-done.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I created a &lt;code&gt;totalPrice&lt;/code&gt; label. We can do this in the bottom by clicking on &amp;#34;&lt;em&gt;Add New Label&lt;/em&gt;&amp;#34;. We will use that label in the next step to annotate the entity we want to extract.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Annotating the Documents&lt;/h2&gt;&#xA;&lt;p&gt;Annotating the documents is the most time-consuming part. We need to go through the following number of invoice documents in each set and mark the total in them:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;100 documents in the Training set&lt;/li&gt;&#xA;    &lt;li&gt;10 documents in the Validation set&lt;/li&gt;&#xA;    &lt;li&gt;10 documents in the Test set&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;This is the minimum number of annotations to make the model train. Feel free to annotate more documents if there&amp;#39;s time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next we click on a document and navigate to the &lt;em&gt;Structured Text&lt;/em&gt; view which. This will show the content of the PDF file and make it easy to annotate by simply selecting the text with the mouse and picking a label in the overlay popup. In the example below I would annotate the total: &lt;code&gt;5,032.50&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/automl-invoice-data-extraction/annotating.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using this view AutoML will use the PDFs annotation&amp;#39;s position during training and learn to distinguish between entities based on the position of the annotation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After all the documents have been annotated we can switch to the &amp;#34;TRAIN&amp;#34; tab and start the training:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/automl-invoice-data-extraction/pre-start-training.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case the training process took &lt;em&gt;2 hours and 18 minutes&lt;/em&gt; to finish.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Manual Testing&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;After we trained the model we can try it out by uploading an invoice to GCS:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil cp testing-invoice.pdf gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then selecting it in the &amp;#34;TEST &amp;amp; USE&amp;#34; tab and clicking on the &amp;#34;PREDICT&amp;#34; button:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/automl-invoice-data-extraction/test-and-use.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The prediction will only take a second and shows the result in a PDF view:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.arthurkoziel.com/automl-invoice-data-extraction/prediction-results.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the example above we can see that the &lt;code&gt;totalPrice&lt;/code&gt; was successfully extracted. Feel free to try it out with other invoices. Multiple documents can be submitted by using the &lt;a href=&#34;https://cloud.google.com/natural-language/automl/docs/predict#batch_prediction&#34;&gt;Batch prediction REST API&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I found Google AutoML Natural Language easy to use. Most of the tasks (except creating the CSV and JSONL files) can be done in the Web UI and the whole process doesn&amp;#39;t require any coding experience. I only had a very small training dataset available but the results are good enough. In my manual tests I was able to extract the total in around 80% of the cases. I&amp;#39;m sure this could be improved by making a higher quality training dataset. The downside is that the price for AutoML is high. It cost me around $25 to train, test and deploy this model.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/automl-invoice-data-extraction/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
</feed>
