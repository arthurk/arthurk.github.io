<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>Arthur Koziel</title>
  <id>https://www.arthurkoziel.com/</id>
  <updated>2020-09-11T12:44:53+08:00</updated>
  <subtitle>Recent Blog Posts</subtitle>
  <link href="https://www.arthurkoziel.com/"></link>
  <entry>
    <title>Using the VBoxManage CLI to create a Ubuntu 20.04 VM</title>
    <updated>2020-09-08T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-09-08:/vboxmanage-cli-ubuntu-20-04/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;&#x9;&lt;p&gt;This tutorial describes the installation of Ubuntu 20.04 in VirtualBox by using the &lt;a href=&#34;https://www.virtualbox.org/manual/ch08.html&#34;&gt;VBoxManage CLI&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#x9;&lt;img src=&#34;ubuntu-desktop.png&#34; alt=&#34;Ubuntu 20.04 Desktop&#34;/&gt;&#xA;&#xA;&#x9;&lt;p&gt;I&amp;#39;m assuming that &lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;VirtualBox 6.1&lt;/a&gt; is installed, the VBoxManage CLI is ready to use and the &lt;a href=&#34;https://ubuntu.com/download/desktop&#34;&gt;Ubuntu 20.04 ISO&lt;/a&gt; has been downloaded.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I&amp;#39;ve tested this on an apple laptop with retina (HiDPI) display and included instructions to scale the display output. The scaling commands can be ignored for non-HiDPI displays.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Getting started&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We first create the VM and increase the hardware resources. The default settings use 1 CPU core, 128MB RAM and 8MB VRAM which is not enough. We&amp;#39;ll increase it to use 2 CPU cores, 4GB RAM and 128MB VRAM:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage createvm --name &amp;#34;Ubuntu 20.04&amp;#34; --ostype Ubuntu_64 --register&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage modifyvm &amp;#34;Ubuntu 20.04&amp;#34; --cpus 2 --memory 4096 --vram 128 --graphicscontroller vmsvga --usbohci on --mouse usbtablet&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In the above command I&amp;#39;ve also enabled USB and set the Graphics Controller to VMSVGA which is the default controller when using the GUI but set to the legacy VBoxVGA when using the CLI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To see all available options run &lt;code&gt;VBoxManage modifyvm&lt;/code&gt; which will output a list of all flags. The current values can be seen with &lt;code&gt;VBoxManage showvminfo &amp;#34;Ubuntu 20.04&amp;#34;&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Storage Devices&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Next we need to configure the storage devices. We create an empty 20GB hard drive that we can install Ubuntu on. A minimal Ubuntu installation takes about 5GB of space:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage createhd --filename ~/VirtualBox\ VMs/Ubuntu\ 20.04/Ubuntu\ 20.04.vdi --size 20480 --variant Standard&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storagectl &amp;#34;Ubuntu 20.04&amp;#34; --name &amp;#34;SATA Controller&amp;#34; --add sata --bootable on&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storageattach &amp;#34;Ubuntu 20.04&amp;#34; --storagectl &amp;#34;SATA Controller&amp;#34; --port 0 --device 0 --type hdd --medium ~/VirtualBox\ VMs/Ubuntu\ 20.04/Ubuntu\ 20.04.vdi &#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We also create a dvd drive with the Ubuntu ISO mounted in it. The VM will boot from this drive first when starting:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storagectl &amp;#34;Ubuntu 20.04&amp;#34; --name &amp;#34;IDE Controller&amp;#34; --add ide&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storageattach &amp;#34;Ubuntu 20.04&amp;#34; --storagectl &amp;#34;IDE Controller&amp;#34; --port 0 --device 0 --type dvddrive --medium ~/Downloads/ubuntu-20.04.1-desktop-amd64.iso&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Display output scaling&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The Ubuntu installer runs with a resolution of 800x600 and would look tiny on HiDPI screens if we wouldn&amp;#39;t scale up the display output. The following command will scale up the display output by a factor of 2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage setextradata &amp;#34;Ubuntu 20.04&amp;#34; GUI/ScaleFactor 2&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Start the VM&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Now we can start the VM which will load the Ubuntu GUI installer from the mounted ISO:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage startvm &amp;#34;Ubuntu 20.04&amp;#34;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After the installation is done the installer will prompt to remove the installation media. We can ignore this for now and press enter to reboot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When Ubuntu is booted up we install the VirtualBox Guest Additions to get the display drivers. This can be done by clicking on the menu bar and selecting &amp;#34;Devices&amp;#34;, then  &amp;#34;Insert Guest Additions CD Image&amp;#34;. The installer will start automatically.&lt;/p&gt; &#xA;&#xA;&lt;h2&gt;Finishing the installation&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;After the Guest Additions are installed we can let GNOME handle the display output scaling which has a better quality.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the Ubuntu terminal (right click on Desktop and select &amp;#34;Open in Terminal&amp;#34;) enter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; gsettings set org.gnome.desktop.interface scaling-factor 2&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can now shut down the VM to remove the installation media and disable the VirtualBox display scaling. On the Desktop click on the arrow in the top right corner and select &amp;#34;Power Off&amp;#34;. Then run the following command on the host:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage storageattach &amp;#34;Ubuntu 20.04&amp;#34; --storagectl &amp;#34;IDE Controller&amp;#34; --port 0 --device 0 --type dvddrive --medium emptydrive&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; VBoxManage setextradata &amp;#34;Ubuntu 20.04&amp;#34; GUI/ScaleFactor 1&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With that done the Ubuntu VM is ready to use.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;&#xA;We created a Ubuntu 20.04 VM by using the CLI instead of the GUI wizard. Personally I find this useful since I sometimes forget to set values in the GUI (such as increasing the VRAM). The Ubuntu Desktop installer has to run in a GUI, to automate this we&amp;#39;d need to use the Ubuntu Server ISO and install the GNOME desktop manually.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/vboxmanage-cli-ubuntu-20-04/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Listing Images and Tags in GCR</title>
    <updated>2020-06-02T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-06-02:/listing-images-and-tags-in-gcr/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&lt;p&gt;&#xA;In this blog post I&amp;#39;m going to describe how to use the &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;GCR&lt;/a&gt; Docker Registry API to list all Docker images and tags in the registry. An installation of the Docker daemon is not needed (we&amp;#39;re going to use a service account).&#xA;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Authentication Flow&lt;/h2&gt;&#xA;&lt;p&gt;In the next sections we&amp;#39;ll walk through the authentication and authorization with the Google Auth Server and the GCR Docker registry.&lt;/p&gt;&#xA;&lt;p&gt;The whole process looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                 +-------------+ +-----+&#xA;| Client  |                                 | GoogleAuth  | | GCR |&#xA;+---------+                                 +-------------+ +-----+&#xA;     |                                             |           |&#xA;     | 1) Authenticate (needs Service Account)     |           |&#xA;     |--------------------------------------------&amp;gt;|           |&#xA;     |                                             |           |&#xA;     |                         OAuth2 Access Token |           |&#xA;     |&amp;lt;--------------------------------------------|           |&#xA;     |                                             |           |&#xA;     | 2) Get JWT token (needs Access Token)       |           |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                             |           |&#xA;     |                                             | JWT Token |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;     |                                             |           |&#xA;     | 3) Get Catalog/Tags (needs JWT token)       |           |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                             |           |&#xA;     |                                           Image Catalog |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;     |                                             |           |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We&amp;#39;ll later make this process shorter by using the service account key for authentication which allows us to skip steps 1 and 2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 3-step process shown above is longer but makes it possible to use more restricted permissions on the service account. This is not possible when using the service account key for authentication.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating the Service Account&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Before we go into step 1 we need to create a service account that is able to authenticate with the Google Authentication Server and get authorization to fetch the necessary information from the Docker registry.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To create the service account and a JSON keyfile we run the following command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# create service account&lt;/span&gt;&#xA;$ gcloud iam service-accounts create gcr-svc-acc&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# create JSON keyfile&lt;/span&gt;&#xA;$ gcloud iam service-accounts keys create gcr-svc-acc-keyfile.json --iam-account gcr-svc-acc@&amp;lt;your-project&amp;gt;.iam.gserviceaccount.com&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We should now have a file called &lt;code&gt;gcr-svc-acc-keyfile.json&lt;/code&gt; that we can later use to get the OAuth2 Access Token, but first we need to grant the service account permissions to access resources in the GCP project.&lt;/p&gt;&#xA;&lt;p&gt;In our use case we need to grant it the following roles:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;Project Browser&lt;/li&gt;&#xA;    &lt;li&gt;Storage Object Viewer on the GCS bucket for the container registry (the bucket is automatically created for each project and named &lt;code&gt;artifacts.&amp;lt;your-project&amp;gt;.appspot.com&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Note: If you don&amp;#39;t want to list all images in the registry (only the tags for an image) you don&amp;#39;t have to grant the Project Browser role.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# grant &amp;#34;Project Browser&amp;#34; role&lt;/span&gt;&#xA;$ gcloud projects add-iam-policy-binding &amp;lt;your-project&amp;gt; --member=&amp;#34;serviceAccount:gcr-svc-acc@&amp;lt;your-project&amp;gt;.iam.gserviceaccount.com&amp;#34; --role=&amp;#34;roles/browser&amp;#34;&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# grant &amp;#34;Storage Object&amp;#34; Viewer role for GCR bucket&lt;/span&gt;&#xA;$ gsutil iam ch serviceAccount:gcr-svc-acc@&amp;lt;your-project&amp;gt;.iam.gserviceaccount.com:objectViewer gs://artifacts.&amp;lt;your-project&amp;gt;.appspot.com/&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Next we need to enable the Cloud Resource Manager API. You can skip this if you only need the image tags.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ gcloud services enable cloudresourcemanager.googleapis.com&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We are now ready to use our service account to get an OAuth2 Access Token from the Google Authorization Server.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Step 1: Getting the OAuth2 Access Token&lt;/h2&gt;&#xA;&lt;pre&gt;+---------+                                 +-------------+&#xA;| Client  |                                 | GoogleAuth  |&#xA;+---------+                                 +-------------+&#xA;     |                                             |&#xA;     | 1) Authenticate (needs Service Account)     |&#xA;     |--------------------------------------------&amp;gt;|&#xA;     |                                             |&#xA;     |                         OAuth2 Access Token |&#xA;     |&amp;lt;--------------------------------------------|&#xA;     |                                             |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The OAuth2 Access Token is required to access data over Google APIs. In our case we want to access the images and tags from the GCR Docker Registry API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obtaining this token by directly communicating via HTTP/REST is &lt;a href=&#34;https://developers.google.com/identity/protocols/oauth2/service-account#authorizingrequests&#34;&gt;complicated&lt;/a&gt; but fortunately there are client libraries available which let us abstract the cryptography parts away.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I&amp;#39;m going to use &lt;a href=&#34;https://github.com/google/oauth2l&#34;&gt;oauth2l&lt;/a&gt; CLI tool.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When requesting the access token we need to pass in a scope that specifies the level of access we need. In our case that&amp;#39;s the &lt;code&gt;cloud-platform.read-only&lt;/code&gt; scope, or the &lt;code&gt;devstorage.read_only&lt;/code&gt; scope if you only want the image tags.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;TODO: restricted&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We run the following command to get the access token:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ export ACCESS_TOKEN=$(oauth2l fetch --credentials gcr-svc-acc-keyfile.json --scope cloud-platform.read-only --cache=&amp;#34;&amp;#34;)&#xA;&#xA;$ echo $ACCESS_TOKEN&#xA;ya29.c.Ko9BzQetXXzZ6mOZTg71LdmqabQx...&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With the OAuth2 Access Token saved we can continue with the Docker registry authentication.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Step 2: Docker Registry JWT token&lt;/h2&gt;&#xA;&lt;pre&gt;+---------+                                                 +-----+&#xA;| Client  |                                                 | GCR |&#xA;+---------+                                                 +-----+&#xA;     |                                                         |&#xA;     | 2) Get JWT token (needs OAuth2 Access Token)            |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                                         |&#xA;     |                                               JWT Token |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For communication with the Docker registry we need to obtain a JWT token that authorizes us to access the data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depending on which data we want to request from the Docker registry we have to use a different scope for the JWT token:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;code&gt;registry:catalog:*&lt;/code&gt; scope to get the list of images&lt;/li&gt;&#xA;    &lt;li&gt;&lt;code&gt;repository:&amp;lt;image_name&amp;gt;:pull&lt;/code&gt; scope to get the tags for an image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To get this token we have to make a request to the gcr.io token server with the username &lt;code&gt;_token&lt;/code&gt; and the previously obtained OAuth2 access token as the password:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# Get JWT token with &amp;#34;registry:catalog:*&amp;#34; scope&lt;/span&gt;&#xA;$ export JWT_TOKEN=$(curl -sSL &amp;#34;https://gcr.io/v2/token?service=gcr.io&amp;amp;scope=registry:catalog:*&amp;#34; -u _token:$ACCESS_TOKEN | jq --raw-output &amp;#39;.token&amp;#39;)&#xA;&#xA;$ echo $JWT_TOKEN&#xA;AJAD5v14fnm+N...&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With the JWT token we can use the &lt;a href=&#34;https://docs.docker.com/registry/spec/api/&#34;&gt;Docker Registry HTTP API&lt;/a&gt; to list all images in the registry.&#xA;&#xA;&lt;/p&gt;&lt;h2&gt;Step 3: Get the image catalog and tags&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                                 +-----+&#xA;| Client  |                                                 | GCR |&#xA;+---------+                                                 +-----+&#xA;     |                                                         |&#xA;     | 3) Get Catalog/Tags (needs JWT token)                   |&#xA;     |--------------------------------------------------------&amp;gt;|&#xA;     |                                                         |&#xA;     |                                      Image Catalog/Tags |&#xA;     |&amp;lt;--------------------------------------------------------|&#xA;     |                                                         |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To get a list of all docker images in the registry we use the &lt;code&gt;&lt;a href=&#34;https://docs.docker.com/registry/spec/api/#listing-repositories&#34;&gt;_catalog&lt;/a&gt;&lt;/code&gt; endpoint. In the HTTP request we have to pass the JWT token in the &lt;code&gt;Authorization&lt;/code&gt; header:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -H &amp;#34;Authorization: Bearer $JWT_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/_catalog&amp;#34; | jq&#xA;&#xA;{&#xA;  &amp;#34;repositories&amp;#34;: [&#xA;    &amp;#34;project/alpine&amp;#34;,&#xA;    &amp;#34;project/busybox&amp;#34;&#xA;  ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The registry has the &amp;#34;project/alpine&amp;#34; and &amp;#34;project/busybox&amp;#34; images in it (which I pushed there before writing this tutorial). The project name is always part of the image name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next we can pick an image and list the tags. For this we can use the &lt;code&gt;&lt;a href=&#34;https://docs.docker.com/registry/spec/api/#listing-image-tags&#34;&gt;&amp;lt;name&amp;gt;/tags/list&lt;/a&gt;&lt;/code&gt; endpoint. But before we can do this, we need a new JWT token.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Our previously created token (with the &lt;code&gt;registry:catalog:*&lt;/code&gt; scope) doesn&amp;#39;t have the permission to get this information. If we try to send a request it will respond with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -H &amp;#34;Authorization: Bearer $JWT_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/project/alpine/tags/list&amp;#34; | jq&#xA;&#xA;{&#xA;  &amp;#34;errors&amp;#34;: [&#xA;    {&#xA;      &amp;#34;code&amp;#34;: &amp;#34;UNAUTHORIZED&amp;#34;,&#xA;      &amp;#34;message&amp;#34;: &amp;#34;Requested repository does not match bearer token resource: project/alpine&amp;#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The scope we need is called &lt;code&gt;repository:&amp;lt;image_name&amp;gt;:pull&lt;/code&gt;. The &lt;code&gt;image_name&lt;/code&gt; includes the GCP project name. In the following example we&amp;#39;re going to generate a new token and list the tags for the &lt;code&gt;project/alpine&lt;/code&gt; image:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# request a new JWT token&lt;/span&gt;&#xA;export JWT_TOKEN=$(curl -sSL &amp;#34;https://gcr.io/v2/token?service=gcr.io&amp;amp;scope=repository:&amp;lt;your-project&amp;gt;/alpine:pull&amp;#34; -u _token:$ACCESS_TOKEN | jq --raw-output &amp;#39;.token&amp;#39;)&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# fetch list of tags&lt;/span&gt;&#xA;curl -sSL -H &amp;#34;Authorization: Bearer $JWT_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/&amp;lt;your-project&amp;gt;/alpine/tags/list&amp;#34; | jq &amp;#39;.tags&amp;#39;&#xA;&#xA;[&#xA;  &amp;#34;3.11&amp;#34;&#xA;]&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Next we&amp;#39;re going to look into making this process shorter. GCR which let us skip the request for the OAuth2 Access Token and the request for the JWT token by using different users.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Skipping the Docker Registry JWT&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The GCR Docker registry has a user called &lt;code&gt;oauth2accesstoken&lt;/code&gt; that lets us send the OAuth2 access token to the Docker Registry without having to obtain the JWT token. The process can be reduced to 2 steps and looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                    +-------------+ +-----+&#xA;| Client  |                                    | GoogleAuth  | | GCR |&#xA;+---------+                                    +-------------+ +-----+&#xA;     |                                                |           |&#xA;     | 1) Authenticate (needs Service Account)        |           |&#xA;     |-----------------------------------------------&amp;gt;|           |&#xA;     |                                                |           |&#xA;     |                            OAuth2 Access Token |           |&#xA;     |&amp;lt;-----------------------------------------------|           |&#xA;     |                                                |           |&#xA;     | 2) Get Image Catalog (needs Access Token)      |           |&#xA;     |-----------------------------------------------------------&amp;lt;|&#xA;     |                                                |           |&#xA;     |                                              Image Catalog |&#xA;     |&amp;lt;-----------------------------------------------------------|&#xA;     |                                                |           |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The command to get the list of image tags with the access token is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -u &amp;#34;oauth2accesstoken:$ACCESS_TOKEN&amp;#34; &amp;#34;https://gcr.io/v2/&amp;lt;your-project&amp;gt;/alpine/tags/list | jq &amp;#39;.tags&amp;#39;&amp;#34;&#xA;&#xA;[&#xA;  &amp;#34;3.11&amp;#34;&#xA;]&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;JSON User&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can also use the &lt;code&gt;_json_key&lt;/code&gt; user to authenticate with the GCR Docker registry. It lets us save another request by not having to obtain the OAuth2 access token. The process of getting data from the GCR Docker registry can then be reduced to a single request and looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;+---------+                                      +-----+&#xA;| Client  |                                      | GCR |&#xA;+---------+                                      +-----+&#xA;     |                                              |&#xA;     | Get Image Catalog (needs Service Account)    |&#xA;     |---------------------------------------------&amp;gt;|&#xA;     |                                              |&#xA;     |                                Image Catalog |&#xA;     |&amp;lt;---------------------------------------------|&#xA;     |                                              |&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When using the &lt;code&gt;_json_key&lt;/code&gt; user we have to pass in the content of the service account json keyfile as the password:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ curl -sSL -u &amp;#34;_json_key:$(cat gcr-svc-acc-keyfile.json)&amp;#34; &amp;#34;https://gcr.io/v2/&amp;lt;your-project&amp;gt;/alpine/tags/list&amp;#34; | jq &amp;#39;.tags&amp;#39;&#xA;&#xA;[&#xA;  &amp;#34;3.11&amp;#34;&#xA;]&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&lt;p&gt;In this tutorial we&amp;#39;ve walked through the process of obtaining an OAuth2 access token from the Google Auth Server and a JWT token from the GCR Docker registry. We&amp;#39;ve then used the Docker registry API to fetch a list of images and tags for a specific image.&#xA;&#xA;&lt;/p&gt;&lt;p&gt;We&amp;#39;ve seen three different authentication methods by using either the &lt;code&gt;_token&lt;/code&gt;, &lt;code&gt;oauth2accesstoken&lt;/code&gt; or &lt;code&gt;_json_key&lt;/code&gt; users.&lt;/p&gt;&#xA;&lt;p&gt;Using the &lt;code&gt;_token&lt;/code&gt; user involves more requests but is the same process that will work for all Docker registries. Depending on if we need to fetch the list of images we can also restrict the service account permissions by not having to grant it the &lt;code&gt;Project Browser&lt;/code&gt; role, not having to enable the Cloud Resource Manager API and using the &lt;code&gt;devstorage.read_only&lt;/code&gt; scope instead of the &lt;code&gt;cloud-platform.read-only&lt;/code&gt; scope for the OAuth2 Access Key.&#xA;&#xA;    &lt;/p&gt;&lt;p&gt;Using the &lt;code&gt;oauth2accesstoken&lt;/code&gt; or &lt;code&gt;_json_key&lt;/code&gt; users is more convenient but only works with the GCR Docker registry and requires project-level permissions for the service account.&lt;/p&gt;&#xA;&#xA;    &lt;p&gt;The example code available in &lt;a href=&#34;https://gist.github.com/arthurk/ab9ced56ce78bb8309599ccc62fa2576&#34;&gt;this gist&lt;/a&gt;.&#xA;&#xA;    &lt;/p&gt;</content>
    <link href="https://www.arthurkoziel.com/listing-images-and-tags-in-gcr/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>DockerHub Docker Registry API Examples</title>
    <updated>2020-05-10T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-05-10:/dockerhub-registry-api/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        This post contains examples of REST API calls to &lt;a href=&#34;https://hub.docker.com&#34;&gt;DockerHub&lt;/a&gt;&#xA;        and the DockerHub &lt;a href=&#34;https://docs.docker.com/registry/spec/api/&#34;&gt;Docker Registry&lt;/a&gt;. We&amp;#39;re going to list all images&#xA;        for a user, list all tags for an image and get the manifest&#xA;        for an image.&#xA;        &lt;/p&gt;&#xA;&#xA;&#xA;        &lt;h2&gt;List public images&lt;/h2&gt;&#xA;        &lt;p&gt;We&amp;#39;re going to use the DockerHub API to get the list of images for a user.&#xA;        This is because the DockerHub Docker Registry does not implement the&#xA;        &lt;a href=&#34;https://docs.docker.com/registry/spec/api/#listing-repositories&#34;&gt;/v2/_catalog&lt;/a&gt; endpoint to list all repositories in the registry.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -s &amp;#34;https://hub.docker.com/v2/repositories/ansible/?page_size=100&amp;#34; | jq -r &amp;#39;.results|.[]|.name&amp;#39;&#xA;&#xA;awx_task&#xA;awx_web&#xA;awx_rabbitmq&#xA;ansible&#xA;centos7-ansible&#xA;ubuntu14.04-ansible&#xA;...&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;The DockerHub API is undocumented but there are projects out there like &lt;a href=&#34;https://github.com/RyanTheAllmighty/Docker-Hub-API&#34;&gt;this one&lt;/a&gt;&#xA;        who did a great job listing available endpoints.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;List private images&lt;/h2&gt;&#xA;        &lt;p&gt;To include private images we need to get an authentication token (JWT) which&#xA;        we can then include in subsequent requests:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export DOCKER_USERNAME=&amp;#34;myusername&amp;#34;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export DOCKER_PASSWORD=&amp;#34;mypassword&amp;#34;&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export TOKEN=$(curl -s -H &amp;#34;Content-Type: application/json&amp;#34; -X POST -d &amp;#39;{&amp;#34;username&amp;#34;: &amp;#34;&amp;#39;${DOCKER_USERNAME}&amp;#39;&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;&amp;#39;${DOCKER_PASSWORD}&amp;#39;&amp;#34;}&amp;#39; https://hub.docker.com/v2/users/login/ | jq -r .token)&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -s -H &amp;#34;Authorization: JWT ${TOKEN}&amp;#34; &amp;#34;https://hub.docker.com/v2/repositories/arthurk/?page_size=100&amp;#34; | jq -r &amp;#39;.results|.[]|.name&amp;#39;&#xA;&#xA;my-private-repo&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;List tags&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;We need to get an authentication token for the Docker Registry. Note that&#xA;        the JWT from the previous step does not work here. DockerHub and the DockerHub Docker Registry&#xA;        are different services and require different authentication credentials.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export AUTH_SERVICE=&amp;#39;registry.docker.io&amp;#39;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export AUTH_SCOPE=&amp;#34;repository:ansible/ansible:pull&amp;#34;&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; export TOKEN=$(curl -fsSL &amp;#34;https://auth.docker.io/token?service=$AUTH_SERVICE&amp;amp;scope=$AUTH_SCOPE&amp;#34; | jq --raw-output &amp;#39;.token&amp;#39;)&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -fsSL \&#xA;    -H &amp;#34;Authorization: Bearer $TOKEN&amp;#34; \&#xA;    &amp;#34;$REGISTRY_URL/v2/ansible/ansible/tags/list&amp;#34; | jq&#xA;{&#xA;  &amp;#34;name&amp;#34;: &amp;#34;ansible/ansible&amp;#34;,&#xA;  &amp;#34;tags&amp;#34;: [&#xA;    &amp;#34;centos6&amp;#34;,&#xA;    &amp;#34;centos7&amp;#34;,&#xA;    &amp;#34;cloudstack-simulator&amp;#34;,&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Get image manifest&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;We re-use the token from the previous step to make a&#xA;        request that gets the manifest for the &lt;code&gt;ansible:centos7&lt;/code&gt; image:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -fsSL \&#xA;    -H &amp;#34;Authorization: Bearer $TOKEN&amp;#34; \&#xA;    &amp;#34;$REGISTRY_URL/v2/ansible/ansible/manifests/centos7&amp;#34; | jq&#xA;&#xA;{&#xA;  &amp;#34;schemaVersion&amp;#34;: 1,&#xA;  &amp;#34;name&amp;#34;: &amp;#34;ansible/ansible&amp;#34;,&#xA;  &amp;#34;tag&amp;#34;: &amp;#34;centos7&amp;#34;,&#xA;  &amp;#34;architecture&amp;#34;: &amp;#34;amd64&amp;#34;,&#xA;  &amp;#34;fsLayers&amp;#34;: [&#xA;    {&#xA;      &amp;#34;blobSum&amp;#34;: &amp;#34;sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4&amp;#34;&#xA;    },&#xA;    ...&#xA;  ],&#xA;  &amp;#34;history&amp;#34;: [...],&#xA;  &amp;#34;signatures&amp;#34;: [...]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Skopeo&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&lt;a href=&#34;https://github.com/containers/skopeo&#34;&gt;Skopeo&lt;/a&gt; is a CLI tool that makes it easy to quickly check information&#xA;        about docker images such as the available tags:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; skopeo inspect docker://ansible/galaxy&#xA;{&#xA;    &amp;#34;Name&amp;#34;: &amp;#34;docker.io/ansible/galaxy&amp;#34;,&#xA;    &amp;#34;Digest&amp;#34;: &amp;#34;sha256:24d0b67d936ca6e7bca253169fc268748d7585c0cee723c14e8e51f37cfd3591&amp;#34;,&#xA;    &amp;#34;RepoTags&amp;#34;: [&#xA;        &amp;#34;2.3.0&amp;#34;,&#xA;        &amp;#34;2.3.1&amp;#34;,&#xA;        &amp;#34;2.4.0&amp;#34;,&#xA;        &amp;#34;3.0.0&amp;#34;,&#xA;        &amp;#34;3.0.1&amp;#34;,&#xA;        ...&#xA;        &amp;#34;3.1.6&amp;#34;,&#xA;        &amp;#34;3.1.7&amp;#34;,&#xA;        &amp;#34;3.1.8&amp;#34;,&#xA;        &amp;#34;develop&amp;#34;,&#xA;        &amp;#34;latest&amp;#34;&#xA;    ],&#xA;    &amp;#34;Created&amp;#34;: &amp;#34;2019-03-15T09:24:18.817740071Z&amp;#34;,&#xA;    &amp;#34;DockerVersion&amp;#34;: &amp;#34;17.09.0-ce&amp;#34;,&#xA;    &amp;#34;Labels&amp;#34;: {&#xA;        &amp;#34;org.label-schema.build-date&amp;#34;: &amp;#34;20190305&amp;#34;,&#xA;        &amp;#34;org.label-schema.license&amp;#34;: &amp;#34;GPLv2&amp;#34;,&#xA;        &amp;#34;org.label-schema.name&amp;#34;: &amp;#34;CentOS Base Image&amp;#34;,&#xA;        &amp;#34;org.label-schema.schema-version&amp;#34;: &amp;#34;1.0&amp;#34;,&#xA;        &amp;#34;org.label-schema.vendor&amp;#34;: &amp;#34;CentOS&amp;#34;&#xA;    },&#xA;    &amp;#34;Architecture&amp;#34;: &amp;#34;amd64&amp;#34;,&#xA;    &amp;#34;Os&amp;#34;: &amp;#34;linux&amp;#34;,&#xA;    &amp;#34;Layers&amp;#34;: [&#xA;        &amp;#34;sha256:8ba884070f611d31cb2c42eddb691319dc9facf5e0ec67672fcfa135181ab3df&amp;#34;,&#xA;        ...&#xA;    ],&#xA;    &amp;#34;Env&amp;#34;: [&#xA;        &amp;#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&amp;#34;,&#xA;        &amp;#34;PIP_NO_CACHE_DIR=off&amp;#34;,&#xA;        &amp;#34;VENV_BIN=/var/lib/galaxy/venv/bin&amp;#34;,&#xA;        &amp;#34;TINI_VERSION=v0.16.1&amp;#34;,&#xA;        &amp;#34;HOME=/var/lib/galaxy&amp;#34;,&#xA;        &amp;#34;DJANGO_SETTINGS_MODULE=galaxy.settings.production&amp;#34;,&#xA;        &amp;#34;GIT_COMMITTER_NAME=Ansible Galaxy&amp;#34;,&#xA;        &amp;#34;GIT_COMMITTER_EMAIL=galaxy@ansible.com&amp;#34;&#xA;    ]&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/dockerhub-registry-api/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Creating CI Pipelines with Tekton (Part 2/2)</title>
    <updated>2020-05-03T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-05-03:/creating-ci-pipelines-with-tekton-part-2/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;            In this blog post we&amp;#39;re going to continue&#xA;            creating a CI pipeline with &lt;a href=&#34;https://tekton.dev&#34;&gt;Tekton&lt;/a&gt;. In &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/&#34;&gt;Part 1&lt;/a&gt; we&#xA;            installed Tekton on a local &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; cluster and defined&#xA;            our first Task which clones a GitHub repository and runs application tests for a Go application (&lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;repo&lt;/a&gt;).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            In this part we&amp;#39;re going to create a Task&#xA;            that will build a Docker image for our Go application&#xA;            and push it to &lt;a href=&#34;https://hub.docker.com&#34;&gt;DockerHub&lt;/a&gt;. Afterward we will combine&#xA;            our tasks into a Pipeline.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Adding DockerHub Credentials&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        To build and push our Docker image we use &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;Kaniko&lt;/a&gt;, which&#xA;        can build Docker images inside a Kubernetes cluster without depending on a Docker daemon.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        Kaniko will build and push the image in the same command. This means before running our task we need to set up&#xA;        credentials for DockerHub so that the docker image can be pushed to the registry.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The credentials are saved in a Kubernetes Secret. Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/04-secret.yaml&#34;&gt;secret.yaml&lt;/a&gt;&lt;/code&gt; with&#xA;        the following content and replace &lt;code&gt;myusername&lt;/code&gt; and &lt;code&gt;mypassword&lt;/code&gt; with your DockerHub credentials:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: v1&#xA;kind: Secret&#xA;metadata:&#xA;  name: basic-user-pass&#xA;  annotations:&#xA;    tekton.dev/docker-0: https://index.docker.io/v1/&#xA;type: kubernetes.io/basic-auth&#xA;stringData:&#xA;    username: myusername&#xA;    password: mypassword&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note the &lt;code&gt;tekton.dev/docker-0&lt;/code&gt; annotation in the metadata&#xA;            which tells Tekton the Docker registry these credentials belong to.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Next we create a &lt;code&gt;ServiceAccount&lt;/code&gt; that uses the &lt;code&gt;basic-user-pass&lt;/code&gt; Secret. Create&#xA;        a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/05-serviceaccount.yaml&#34;&gt;serviceaccount.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: v1&#xA;kind: ServiceAccount&#xA;metadata:&#xA;  name: build-bot&#xA;secrets:&#xA;  - name: basic-user-pass&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Apply both files with kubectl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f secret.yaml&#xA;secret/basic-user-pass created&#xA;&#xA;$ kubectl apply -f serviceaccount.yaml&#xA;serviceaccount/build-bot created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now use this ServiceAccount (named &lt;code&gt;build-bot&lt;/code&gt;) when&#xA;            running Tekton tasks or pipelines by specifying a &lt;code&gt;serviceAccountName&lt;/code&gt;.&#xA;            We will see examples of this below.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating a Task to build and push a Docker image&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Now that the credentials are set up we can continue by creating&#xA;            the Task that will build and push the Docker image.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/06-task-build-push.yaml&#34;&gt;task-build-push.yaml&lt;/a&gt;&lt;/code&gt; with&#xA;            the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Task&#xA;metadata:&#xA;  name: build-and-push&#xA;spec:&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        type: git&#xA;  steps:&#xA;    - name: build-and-push&#xA;      image: gcr.io/kaniko-project/executor:v0.19.0&#xA;      env:&#xA;        - name: DOCKER_CONFIG&#xA;          value: /tekton/home/.docker&#xA;      command:&#xA;        - /kaniko/executor&#xA;        - --dockerfile=Dockerfile&#xA;        - --context=/workspace/repo/src&#xA;        - --destination=arthurk/tekton-test:latest&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Similarly to the first task this task takes a git repo as an input (the input name is &lt;code&gt;repo&lt;/code&gt;)&#xA;            and consists of only a single step since Kaniko builds and pushes the image in the same command.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Make sure to create a DockerHub repository and replace &lt;code&gt;arthurk/tekton-test&lt;/code&gt;&#xA;            with your repository name. In this example it will always tag and push the image&#xA;            with the &lt;code&gt;latest&lt;/code&gt; tag.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Tekton has support for&#xA;            &lt;a href=&#34;https://github.com/tektoncd/pipeline/blob/master/docs/pipelines.md#specifying-parameters&#34;&gt;parameters&lt;/a&gt;&#xA;            to avoid hardcoding values like this.&#xA;            However to keep this tutorial simple I&amp;#39;ve left them out.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            The &lt;code&gt;DOCKER_CONFIG&lt;/code&gt; env var is required for Kaniko to be able to &lt;a href=&#34;https://github.com/tektoncd/pipeline/pull/706&#34;&gt;find the Docker credentials&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Apply the file with kubectl:&lt;/p&gt;&#xA;&lt;pre&gt;$ kubectl apply -f task-build-push.yaml&#xA;task.tekton.dev/build-and-push created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            There are two ways we can test this Task, either by manually creating a TaskRun&#xA;            definition and then applying it with &lt;code&gt;kubectl&lt;/code&gt; or by using the Tekton CLI (&lt;code&gt;tkn&lt;/code&gt;).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In the following two sections I will show both methods.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Run the Task with kubectl&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            To run the Task with &lt;code&gt;kubectl&lt;/code&gt; we create a TaskRun that looks identical to the&#xA;            &lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/03-taskrun.yaml&#34;&gt;previous&lt;/a&gt; with the&#xA;            exception that we now specify a ServiceAccount (&lt;code&gt;serviceAccountName&lt;/code&gt;) to use when executing the Task.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/07-taskrun-build-push.yaml&#34;&gt;taskrun-build-push.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: TaskRun&#xA;metadata:&#xA;  name: build-and-push&#xA;spec:&#xA;  serviceAccountName: build-bot&#xA;  taskRef:&#xA;    name: build-and-push&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        resourceRef:&#xA;          name: arthurk-tekton-example&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Apply the task and check the log of the Pod by listing all Pods that start with the Task name &lt;code&gt;build-and-push&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f taskrun-build-push.yaml&#xA;taskrun.tekton.dev/build-and-push created&#xA;&#xA;$ kubectl get pods | grep build-and-push&#xA;build-and-push-pod-c698q   2/2     Running     0          4s&#xA;&#xA;$ kubectl logs --all-containers build-and-push-pod-c698q --follow&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588478267.3476844,&amp;#34;caller&amp;#34;:&amp;#34;creds-init/main.go:44&amp;#34;, &amp;#34;msg&amp;#34;:&amp;#34;Credentials initialized.&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588478279.2681644,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588478279.3249557,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;INFO[0004] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;INFO[0004] Retrieving image manifest golang:1.14-alpine&#xA;INFO[0012] Built cross stage deps: map[]&#xA;...&#xA;INFO[0048] Taking snapshot of full filesystem...&#xA;INFO[0048] Resolving paths&#xA;INFO[0050] CMD [&amp;#34;app&amp;#34;]&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;The task executed without problems and we can now pull/run our Docker image:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ docker run arthurk/tekton-test:latest&#xA;hello world&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Run the Task with the Tekton CLI&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Running the Task with the Tekton CLI is more convenient. With a single command&#xA;            it generates a TaskRun manifest from the Task definition, applies it, and follows the logs.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ tkn task start build-and-push --inputresource repo=arthurk-tekton-example --serviceaccount build-bot --showlog&#xA;Taskrun started: build-and-push-run-ctjvv&#xA;Waiting for logs to be available...&#xA;[git-source-arthurk-tekton-example-p9zxz] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588479279.271127,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[git-source-arthurk-tekton-example-p9zxz] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588479279.329212,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[build-and-push] INFO[0004] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;[build-and-push] INFO[0008] Retrieving image manifest golang:1.14-alpine&#xA;[build-and-push] INFO[0012] Built cross stage deps: map[]&#xA;...&#xA;[build-and-push] INFO[0049] Taking snapshot of full filesystem...&#xA;[build-and-push] INFO[0049] Resolving paths&#xA;[build-and-push] INFO[0051] CMD [&amp;#34;app&amp;#34;]&#xA;&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            What happens in the background is similar to what we did with kubectl in the&#xA;            previous section but this time we only have to run a single command.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating a Pipeline&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Now that we have both of our Tasks ready (test, build-and-push) we can create&#xA;        a Pipeline that will run them sequentially: First it will run the application&#xA;        tests and if they pass it will build the Docker image and push it to DockerHub.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/08-pipeline.yaml&#34;&gt;pipeline.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Pipeline&#xA;metadata:&#xA;  name: test-build-push&#xA;spec:&#xA;  resources:&#xA;    - name: repo&#xA;      type: git&#xA;  tasks:&#xA;    # Run application tests&#xA;    - name: test&#xA;      taskRef:&#xA;        name: test&#xA;      resources:&#xA;        inputs:&#xA;          - name: repo      # name of the Task input (see Task definition)&#xA;            resource: repo  # name of the Pipeline resource&#xA;&#xA;    # Build docker image and push to registry&#xA;    - name: build-and-push&#xA;      taskRef:&#xA;        name: build-and-push&#xA;      runAfter:&#xA;        - test&#xA;      resources:&#xA;        inputs:&#xA;          - name: repo      # name of the Task input (see Task definition)&#xA;            resource: repo  # name of the Pipeline resource&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The first thing we need to define is what resources our Pipeline requires. A resource&#xA;        can either be an input or an output. In our case we only have an input: the git repo with&#xA;        our application source code. We name the resource &lt;code&gt;repo&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Next we define our tasks. Each task has a &lt;code&gt;taskRef&lt;/code&gt; (a reference to a Task)&#xA;        and passes the tasks required inputs.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Apply the file with kubectl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f pipeline.yaml&#xA;pipeline.tekton.dev/test-build-push created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Similar to how we can run as Task by creating a TaskRun, we can run&#xA;            a Pipeline by creating a PipelineRun.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            This can either be done with kubectl or the Tekton CLI. In the following&#xA;            two sections I will show both ways.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Run the Pipeline with kubectl&#xA;        &lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To run the file with kubectl we have to create a PipelineRun.&#xA;        Create a file named &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/09-pipelinerun.yaml&#34;&gt;pipelinerun.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: PipelineRun&#xA;metadata:&#xA;  name: test-build-push-pr&#xA;spec:&#xA;  serviceAccountName: build-bot&#xA;  pipelineRef:&#xA;    name: test-build-push&#xA;  resources:&#xA;  - name: repo&#xA;    resourceRef:&#xA;      name: arthurk-tekton-example&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Apply the file, get the Pods that are prefixed with the PiplelineRun name, and&#xA;            view the logs to get the container output:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ kubectl apply -f pipelinerun.yaml&#xA;pipelinerun.tekton.dev/test-build-push-pr created&#xA;&#xA;$ kubectl get pods | grep test-build-push-pr&#xA;test-build-push-pr-build-and-push-gh4f4-pod-nn7k7   0/2     Completed   0          2m39s&#xA;test-build-push-pr-test-d2tck-pod-zh5hn             0/2     Completed   0          2m51s&#xA;&#xA;$ kubectl logs test-build-push-pr-build-and-push-gh4f4-pod-nn7k7 --all-containers --follow&#xA;INFO[0005] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;INFO[0005] Retrieving image manifest golang:1.14-alpine&#xA;...&#xA;INFO[0048] Taking snapshot of full filesystem...&#xA;INFO[0048] Resolving paths&#xA;INFO[0050] CMD [&amp;#34;app&amp;#34;]&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Next we will run the same Pipeline but we&amp;#39;re going to use the Tekton CLI instead.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Run the Pipeline with Tekton CLI&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        When using the CLI we don&amp;#39;t have to write a PipelineRun, it will be generated&#xA;        from the Pipeline manifest. By using the &lt;code&gt;--showlog&lt;/code&gt; argument it will&#xA;        also display the Task (container) logs:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ tkn pipeline start test-build-push --resource repo=arthurk-tekton-example --serviceaccount build-bot --showlog&#xA;&#xA;Pipelinerun started: test-build-push-run-9lmfj&#xA;Waiting for logs to be available...&#xA;[test : git-source-arthurk-tekton-example-k98k8] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483940.4913514,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[test : git-source-arthurk-tekton-example-k98k8] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483940.5485842,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[test : run-test] PASS&#xA;[test : run-test] ok  &#x9;_/workspace/repo/src&#x9;0.006s&#xA;&#xA;[build-and-push : git-source-arthurk-tekton-example-2vqls] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483950.2051432,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[build-and-push : git-source-arthurk-tekton-example-2vqls] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588483950.2610846,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[build-and-push : build-and-push] INFO[0003] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;[build-and-push : build-and-push] INFO[0003] Resolved base name golang:1.14-alpine to golang:1.14-alpine&#xA;[build-and-push : build-and-push] INFO[0003] Retrieving image manifest golang:1.14-alpine&#xA;...&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Summary&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            In &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/&#34;&gt;Part 1&lt;/a&gt; we installed Tekton on a local Kubernetes cluster, defined a Task, and tested it by creating a TaskRun via YAML manifest as well as the Tekton CLI tkn.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In this part we created our first Tektok Pipeline that consists of two tasks.&#xA;            The first one clones a repo from GitHub and runs application tests.&#xA;            The second one builds a Docker image and pushes it to DockerHub.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            All code examples are available &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-2/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Creating CI Pipelines with Tekton (Part 1/2)</title>
    <updated>2020-04-26T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-26:/creating-ci-pipelines-with-tekton-part-1/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post we&amp;#39;re going to build a continuous integration (CI) pipeline with &lt;a href=&#34;https://tekton.dev&#34;&gt;Tekton&lt;/a&gt;,&#xA;        an open-source framework for creating CI/CD pipelines in Kubernetes.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We&amp;#39;re going to provision a local Kubernetes cluster via &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; and install Tekton on it.&#xA;        After that we&amp;#39;ll create a pipeline consisting of two steps which will run application unit tests, build a Docker image,&#xA;        and push it to DockerHub.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;This is part 1 of 2 in which we will install Tekton and create a task that runs our application test.&#xA;        The second part is available &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-2/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating the k8s cluster&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We use &lt;a href=&#34;http://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; to create a Kubernetes cluster for our Tekton installation:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind create cluster --name tekton&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Installing Tekton&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can install Tekton by applying the &lt;code&gt;release.yaml&lt;/code&gt; file from the latest release of the &lt;a href=&#34;https://github.com/tektoncd/pipeline&#34;&gt;tektoncd/pipeline&lt;/a&gt; GitHub repo:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/tektoncd/pipeline/releases/download/v0.12.0/release.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        This will install Tekton into the &lt;code&gt;tekton-pipelines&lt;/code&gt; namespace. We can check&#xA;        that the installation succeeded by listing the Pods in that namespace and&#xA;        making sure they&amp;#39;re in &lt;code&gt;Running&lt;/code&gt; state.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods --namespace tekton-pipelines&#xA;NAME                                           READY   STATUS    RESTARTS   AGE&#xA;tekton-pipelines-controller-74848c44df-m42gf   1/1     Running   0          20s&#xA;tekton-pipelines-webhook-6f764dc8bf-zq44s      1/1     Running   0          19s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Setting up the Tekton CLI&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Installing the CLI is optional but I found it to be more convenient&#xA;        than &lt;code&gt;kubectl&lt;/code&gt; when managing Tekton resources. The examples&#xA;        later on will show both ways.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can install it via Homebrew:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; brew tap tektoncd/tools&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; brew install tektoncd/tools/tektoncd-cli&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; tkn version&#xA;Client version: 0.9.0&#xA;Pipeline version: v0.12.0&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Concepts&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Tekton provides custom resource definitions (&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;CRDs&lt;/a&gt;) for Kubernetes that can be used to define our Pipelines.&#xA;        In this tutorial we will use the following custom resources:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Task: A series of steps that execute commands (In CircleCI this is called a &lt;em&gt;Job&lt;/em&gt;)&lt;/li&gt;&#xA;            &lt;li&gt;Pipeline: A set of Tasks (In CircleCI this is called a &lt;em&gt;Workflow&lt;/em&gt;)&lt;/li&gt;&#xA;            &lt;li&gt;PipelineResource: Input or Output of a Pipeline (for example a git repo or a tar file)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;We will use the following two resources to define the execution of our Tasks and Pipeline:&lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;TaskRun: Defines the execution of a Task&lt;/li&gt;&#xA;            &lt;li&gt;PipelineRun: Defines the execution of a Pipeline&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;For example,&#xA;        if we write a Task and want to test it we can execute it with a TaskRun. The same applies for a Pipeline:&#xA;        To execute a Pipeline we need to create a PipelineRun.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Application Code&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        In our example Pipeline we&amp;#39;re going to use a Go application&#xA;        that simply prints the sum of two integers. You can find the application code, test, and Dockerfile in the &lt;code&gt;src/&lt;/code&gt; directory in &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;this repo&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating our first task&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Our first Task will run the application tests inside the cloned git repo.&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/01-task-test.yaml&#34;&gt;01-task-test.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: Task&#xA;metadata:&#xA;  name: test&#xA;spec:&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        type: git&#xA;  steps:&#xA;    - name: run-test&#xA;      image: golang:1.14-alpine&#xA;      workingDir: /workspace/repo/src&#xA;      command: [&amp;#34;go&amp;#34;]&#xA;      args: [&amp;#34;test&amp;#34;]&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;code&gt;resources:&lt;/code&gt; block defines the inputs that our task needs to execute its steps.&#xA;        Our step (named &lt;code&gt;run-test&lt;/code&gt;) needs the cloned &lt;a href=&#34;https://github.com/arthurk/tekton-example/&#34;&gt;tekton-example&lt;/a&gt; git repository as an input and we can create this input with a PipelineResource.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file called &lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/02-pipelineresource.yaml&#34;&gt;02-pipelineresource.yaml&lt;/a&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1alpha1&#xA;kind: PipelineResource&#xA;metadata:&#xA;  name: arthurk-tekton-example&#xA;spec:&#xA;  type: git&#xA;  params:&#xA;    - name: url&#xA;      value: https://github.com/arthurk/tekton-example&#xA;    - name: revision&#xA;      value: master&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;code&gt;git&lt;/code&gt; resource type will use git to clone the repo into the &lt;code&gt;/workspace/$input_name&lt;/code&gt; directory everytime the Task is run.&#xA;        Since our input is named &lt;code&gt;repo&lt;/code&gt; the code will be cloned to &lt;code&gt;/workspace/repo&lt;/code&gt;. If our input would be named &lt;code&gt;foobar&lt;/code&gt; it would be cloned into &lt;code&gt;/workspace/foobar&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The next block in our Task (&lt;code&gt;steps:&lt;/code&gt;) specifies the command to execute and the Docker image in which to run that command. We&amp;#39;re going to use the &lt;a href=&#34;https://hub.docker.com/_/golang&#34;&gt;golang&lt;/a&gt; Docker image as it already has Go installed.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        For the &lt;code&gt;go test&lt;/code&gt; command to run we need to change the directory. By default the command will run&#xA;        in the &lt;code&gt;/workspace/repo&lt;/code&gt; directory but in our &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;tekton-example&lt;/a&gt; repo the Go application is in the &lt;code&gt;src&lt;/code&gt; directory. We do this by setting &lt;code&gt;workingDir: /workspace/repo/src&lt;/code&gt;.&#xA;&#xA;        &lt;/p&gt;&lt;p&gt;&#xA;        Next we specify the command to run (&lt;code&gt;go test&lt;/code&gt;) but note that the command (&lt;code&gt;go&lt;/code&gt;) and args (&lt;code&gt;test&lt;/code&gt;) need to be defined separately&#xA;        in the YAML file.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Apply the Task and the PipelineResource with kubectl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f 01-task-test.yaml&#xA;task.tekton.dev/test created&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f 02-pipelineresource.yaml&#xA;pipelineresource.tekton.dev/arthurk-tekton-example created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Running our task&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To run our &lt;code&gt;Task&lt;/code&gt; we have to create a &lt;code&gt;TaskRun&lt;/code&gt; that references the previously created &lt;code&gt;Task&lt;/code&gt; and passes in all required inputs (&lt;code&gt;PipelineResource&lt;/code&gt;).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file called &lt;code&gt;&lt;a href=&#34;https://github.com/arthurk/tekton-example/blob/master/03-taskrun.yaml&#34;&gt;03-taskrun.yaml&lt;/a&gt;&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: tekton.dev/v1beta1&#xA;kind: TaskRun&#xA;metadata:&#xA;  name: testrun&#xA;spec:&#xA;  taskRef:&#xA;    name: test&#xA;  resources:&#xA;    inputs:&#xA;      - name: repo&#xA;        resourceRef:&#xA;          name: arthurk-tekton-example&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        This will take our Task (&lt;code&gt;taskRef&lt;/code&gt; is a reference to our previously created task named &lt;code&gt;test&lt;/code&gt;)&#xA;        with our &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;tekton-example&lt;/a&gt; git repo as an input (&lt;code&gt;resourceRef&lt;/code&gt; is a reference to our PipelineResource named &lt;code&gt;arthurk-tekton-example&lt;/code&gt;)&#xA;        and execute it.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Apply the file with kubectl and then check the Pods and TaskRun resources. The Pod will go through the &lt;code&gt;Init:0/2&lt;/code&gt; and &lt;code&gt;PodInitializing&lt;/code&gt; status and then succeed:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f 03-taskrun.yaml&#xA;pipelineresource.tekton.dev/arthurk-tekton-example created&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods&#xA;NAME                READY   STATUS      RESTARTS   AGE&#xA;testrun-pod-pds5z   0/2     Completed   0          4m27s&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get taskrun&#xA;NAME      SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME&#xA;testrun   True        Succeeded   70s         57s&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To see the output of the containers we can run the following command. Make sure to replace &lt;code&gt;testrun-pod-pds5z&lt;/code&gt; with the the Pod name from the output above (it will be different for each run).&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl logs testrun-pod-pds5z --all-containers&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477119.3692405,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477119.4230678,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;PASS&#xA;ok  &#x9;_/workspace/repo/src&#x9;0.003s&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Our tests passed and our task succeeded. Next we will&#xA;        use the Tekton CLI to see how we can make this whole process easier.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Using the Tekton CLI to run a Task&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The Tekton CLI provides a faster and more convenient way to run Tasks.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Instead of manually&#xA;        writing a &lt;code&gt;TaskRun&lt;/code&gt; manifest we can run the following command&#xA;        which takes our Task (named &lt;code&gt;test&lt;/code&gt;), generates a &lt;code&gt;TaskRun&lt;/code&gt; (with a random name) and shows&#xA;        its logs:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; tkn task start test --inputresource repo=arthurk-tekton-example --showlog&#xA;Taskrun started: test-run-8t46m&#xA;Waiting for logs to be available...&#xA;[git-source-arthurk-tekton-example-dqjfb] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477372.740875,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:136&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully cloned https://github.com/arthurk/tekton-example @ 301aeaa8f7fa6ec01218ba6c5ddf9095b24d5d98 (grafted, HEAD, origin/master) in path /workspace/repo&amp;#34;}&#xA;[git-source-arthurk-tekton-example-dqjfb] {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1588477372.7954974,&amp;#34;caller&amp;#34;:&amp;#34;git/git.go:177&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Successfully initialized and updated submodules in path /workspace/repo&amp;#34;}&#xA;&#xA;[run-test] PASS&#xA;[run-test] ok  &#x9;_/workspace/repo/src&#x9;0.006s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            We have successfully installed Tekton on a local Kubernetes&#xA;            cluster, defined a Task, and tested it by creating a TaskRun via&#xA;            YAML manifest as well as the Tekton CLI &lt;code&gt;tkn&lt;/code&gt;.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            All example code is available &lt;a href=&#34;https://github.com/arthurk/tekton-example&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        In the next part we&amp;#39;re going to create a task that will&#xA;        use &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;Kaniko&lt;/a&gt; to build a Docker image&#xA;        for our application and then push it to DockerHub.&#xA;        We will then create a Pipeline that runs both of our tasks sequentially (run application tests, build and push).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;Part 2 is available &lt;a href=&#34;https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-2/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/creating-ci-pipelines-with-tekton-part-1/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Running Knative with Istio in a Kind Cluster</title>
    <updated>2020-04-19T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-19:/running-knative-with-istio-in-kind/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post I&amp;#39;m going to show how to run &lt;a href=&#34;https://knative.dev&#34;&gt;Knative&lt;/a&gt; with &lt;a href=&#34;https://istio.io&#34;&gt;Istio&lt;/a&gt;&#xA;        as a networking layer on a local &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt; cluster.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        I&amp;#39;m assuming that kind and kubectl are installed. Installation instructions for kind are &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/#installation&#34;&gt;here&lt;/a&gt;&#xA;        and kubectl &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind --version&#xA;kind kind version 0.7.0&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl version --client&#xA;Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;15&amp;#34;, GitVersion:&amp;#34;v1.15.5&amp;#34;, GitCommit:&amp;#34;20c265fef0741dd71a66480e35bd69f18351daea&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2019-10-15T19:16:51Z&amp;#34;, GoVersion:&amp;#34;go1.12.10&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Setup a kind cluster&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To get traffic into our cluster we need to create our kind cluster with a&#xA;        custom configuration that sets up a port forward from host to ingress&#xA;        controller.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        In this setup we&amp;#39;re going to use port &lt;code&gt;32000&lt;/code&gt;. Later we will configure&#xA;        the Istio ingress gateway to accept connections on this port.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;kind-config-istio.yml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;kind&lt;/span&gt;: Cluster&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;apiVersion&lt;/span&gt;: kind.sigs.k8s.io/v1alpha3&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;nodes&lt;/span&gt;:&#xA;- &lt;span style=&#34;color:#66d9ef&#34;&gt;role&lt;/span&gt;: control-plane&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;extraPortMappings&lt;/span&gt;:&#xA;  - &lt;span style=&#34;color:#66d9ef&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32000&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;hostPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To create the cluster with our custom configuration we use the &lt;code&gt;--config&lt;/code&gt; argument:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kind create cluster --config kind-config-istio.yml&#xA;&#xA;Creating cluster &amp;#34;kind&amp;#34; ...&#xA;  Ensuring node image (kindest/node:v1.17.0) &#xA;  Preparing nodes &#xA;  Writing configuration &#xA;  Starting control-plane &#xA;  Installing CNI &#xA;  Installing StorageClass &#xA;Set kubectl context to &amp;#34;kind-kind&amp;#34;&#xA;You can now use your cluster with:&#xA;&#xA;kubectl cluster-info --context kind-kind&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Install Istio&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We&amp;#39;re going to install Istio via the &lt;a href=&#34;https://istio.io/docs/reference/commands/istioctl/&#34;&gt;istioctl&lt;/a&gt;&#xA;        command-line tool.&#xA;        The following command will download version istioctl v1.5.1 for macOS and extract it into the current directory:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl -L https://github.com/istio/istio/releases/download/1.5.1/istioctl-1.5.1-osx.tar.gz | tar xvz -&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl version --remote=false&#xA;1.5.1&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Istio can be installed with different configuration profiles. In this example&#xA;        we are going to use the &lt;code&gt;default&lt;/code&gt; profile which will&#xA;        install the pilot, ingressgateway and prometheus.&#xA;        A list of all built-in&#xA;        configuration profiles and their differences can be found &lt;a href=&#34;https://istio.io/docs/setup/additional-setup/config-profiles/&#34;&gt;here&lt;/a&gt;.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The following command will perform the installation:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl manifest apply --set profile=default&#xA;&#xA;Detected that your cluster does not support third party JWT authentication. Falling back to less secure first party JWT. See https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens for details.&#xA;- Applying manifest for component Base...&#xA; Finished applying manifest for component Base.&#xA;- Applying manifest for component Pilot...&#xA; Finished applying manifest for component Pilot.&#xA;- Applying manifest for component IngressGateways...&#xA;- Applying manifest for component AddonComponents...&#xA; Finished applying manifest for component AddonComponents.&#xA; Finished applying manifest for component IngressGateways.&#xA;&#xA; Installation complete&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can check that the pods are running via kubectl:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods -n istio-system&#xA;&#xA;NAME                                    READY   STATUS    RESTARTS   AGE&#xA;istio-ingressgateway-5f54974979-crw9d   1/1     Running   0          21s&#xA;istiod-6548b95486-djvd6                 1/1     Running   0          6m57s&#xA;prometheus-6c88c4cb8-wtdtn              2/2     Running   0          21s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To verify the installation we can run the &lt;code&gt;verify-install&lt;/code&gt; command&#xA;        and pass in the manifest of the default configuration profile:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./istioctl manifest generate --set profile=default | ./istioctl verify-install -f -&#xA;...&#xA;Checked 25 crds&#xA;Checked 1 Istio Deployments&#xA;Istio is installed successfully&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The configuration profile&#xA;        will set the ingress type to &lt;code&gt;LoadBalancer&lt;/code&gt;, which&#xA;        is not working on a local cluster.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        For the ingress gateway to accept incoming connections we have to change&#xA;        the type from &lt;code&gt;LoadBalancer&lt;/code&gt; to &lt;code&gt;NodePort&lt;/code&gt; and change the assigned port&#xA;        to &lt;code&gt;32000&lt;/code&gt; (the port we forwarded during the cluster creation).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Create a file named &lt;code&gt;patch-ingressgateway-nodeport.yaml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;type&lt;/span&gt;: NodePort&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;ports&lt;/span&gt;:&#xA;  - &lt;span style=&#34;color:#66d9ef&#34;&gt;name&lt;/span&gt;: http2&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32000&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;protocol&lt;/span&gt;: TCP&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We apply the file with &lt;code&gt;kubectl patch&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl patch service istio-ingressgateway -n istio-system --patch &amp;#34;$(cat patch-ingressgateway-nodeport.yaml)&amp;#34;&#xA;&#xA;service/istio-ingressgateway patched&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Istio is now set up and ready to accept connections.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Install Knative&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative consists of two components: &lt;a href=&#34;https://knative.dev/docs/serving/&#34;&gt;Serving&lt;/a&gt; and &lt;a href=&#34;https://knative.dev/docs/eventing/&#34;&gt;Eventing&lt;/a&gt;. In this example we&amp;#39;re going to install the Serving component.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        We start by applying the Kubernetes manifests for the CRDs, Core and Istio ingress controller:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/serving/releases/download/v0.14.0/serving-crds.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/serving/releases/download/v0.14.0/serving-core.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f https://github.com/knative/net-istio/releases/download/v0.14.0/net-istio.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We check the pods via kubectl and wait until they have the status &lt;code&gt;Running&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods --namespace knative-serving&#xA;&#xA;NAME                                READY   STATUS    RESTARTS   AGE&#xA;activator-65fc4d666-2bj8r           2/2     Running   0          9m&#xA;autoscaler-74b4bb97bd-9rql4         2/2     Running   0          9m&#xA;controller-6b6978c965-rks25         2/2     Running   0          9m&#xA;istio-webhook-856d84fbf9-8nswp      2/2     Running   0          8m58s&#xA;networking-istio-6845f7cf59-6h25b   1/1     Running   0          8m58s&#xA;webhook-577576647-rw264             2/2     Running   0          9m&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative will create a custom URL for each service and for this to work&#xA;        it needs to have DNS configured. Since our cluster is running locally we need to&#xA;        use a wildcard DNS service (for example &lt;a href=&#34;https://nip.io&#34;&gt;nip.io&lt;/a&gt;).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        We patch the Knative config via kubectl and set the domain to &lt;code&gt;127.0.0.1.nip.io&lt;/code&gt;&#xA;        which will forward all requests to &lt;code&gt;127.0.0.1&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl patch configmap/config-domain \&#xA;  --namespace knative-serving \&#xA;  --type merge \&#xA;  --patch &amp;#39;{&amp;#34;data&amp;#34;:{&amp;#34;127.0.0.1.nip.io&amp;#34;:&amp;#34;&amp;#34;}}&amp;#39;&#xA;&#xA;configmap/config-domain patched&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Knative is now installed and ready to use.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;&#xA;            Creating a test service&#xA;        &lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        To check that Knative is working correctly we deploy a test service&#xA;        that consists of an &lt;a href=&#34;https://github.com/jmalloc/echo-server&#34;&gt;echo-server&lt;/a&gt; which will return the request&#xA;        headers and body.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We start by creating a file named &lt;code&gt;knative-echoserver.yaml&lt;/code&gt; with the following content:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;apiVersion&lt;/span&gt;: serving.knative.dev/v1&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;kind&lt;/span&gt;: Service&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;metadata&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;name&lt;/span&gt;: helloworld&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt;: default&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;  &lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt;:&#xA;    &lt;span style=&#34;color:#66d9ef&#34;&gt;spec&lt;/span&gt;:&#xA;      &lt;span style=&#34;color:#66d9ef&#34;&gt;containers&lt;/span&gt;:&#xA;        - &lt;span style=&#34;color:#66d9ef&#34;&gt;image&lt;/span&gt;: jmalloc/echo-server&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We enable Istio sidecar injection for the default namespace&#xA;        and deploy the Knative service in it:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl label namespace default istio-injection=enabled&#xA;&#xA;namespace/default labeled&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl apply -f knative-echoserver.yaml&#xA;&#xA;service.serving.knative.dev/helloworld created&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        We can check the deployment of the pods via kubectl:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get pods&#xA;&#xA;NAME                                           READY   STATUS    RESTARTS   AGE&#xA;helloworld-96c68-deployment-6744444b5f-6htld   3/3     Running   0          108s&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        When all pods are running we can get the URL of the service and&#xA;        make an HTTP request to it:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; kubectl get ksvc&#xA;&#xA;NAME         URL                                          LATESTCREATED      LATESTREADY        READY   REASON&#xA;helloworld   http://helloworld.default.127.0.0.1.nip.io   helloworld-96c68   helloworld-96c68   True&#xA;&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; curl http://helloworld.default.127.0.0.1.nip.io&#xA;&#xA;Request served by helloworld-96c68-deployment-6744444b5f-6htld&#xA;&#xA;HTTP/1.1 GET /&#xA;&#xA;Host: helloworld.default.127.0.0.1.nip.io&#xA;X-Request-Id: 9e5bf3c9-0bc8-4551-9302-ea2eca5f6446&#xA;User-Agent: curl/7.64.1&#xA;Accept-Encoding: gzip&#xA;Forwarded: for=10.244.0.1;proto=http, for=127.0.0.1&#xA;X-B3-Traceid: d22e218318367687170ce339b13b0c91&#xA;X-Forwarded-For: 10.244.0.1, 127.0.0.1, 127.0.0.1&#xA;X-B3-Spanid: 0e3174748253699d&#xA;X-Forwarded-Proto: http&#xA;Accept: */*&#xA;K-Proxy-Request: activator&#xA;X-B3-Parentspanid: c39ad4d28b42b25f&#xA;X-B3-Sampled: 0&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            The response shows the pod which served the request (&lt;code&gt;helloworld-96c68-deployment-6744444b5f-6htld&lt;/code&gt;)&#xA;            and the tracing headers that Istio will add to every request.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;        If we wait a few minutes we can see that Knative will scale down our&#xA;        service to zero replicas (no incoming requests). In this case&#xA;        we can make another request to the service and see it scale up again.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/running-knative-with-istio-in-kind/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Writing tar.gz files in Go</title>
    <updated>2020-04-12T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-12:/writing-tar-gz-files-in-go/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            In this blog post I&amp;#39;m going to explain how to use the Go &lt;code&gt;archive/tar&lt;/code&gt; and &lt;code&gt;compress/gzip&lt;/code&gt;&#xA;            packages to create a tar archive and compress it with gzip.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;Below is the full example code and after that there&amp;#39;s an explanation of the parts.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Full Code&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#f92672&#34;&gt;package&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;io&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;archive/tar&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;compress/gzip&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fmt&amp;#34;&lt;/span&gt;&#xA;&#x9;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;os&amp;#34;&lt;/span&gt;&#xA;)&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Files which to include in the tar.gz archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test/test.txt&amp;#34;&lt;/span&gt;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create output file&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Create&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output.tar.gz&amp;#34;&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error writing archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create the archive and write the output to the &amp;#34;out&amp;#34; Writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error creating archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Println&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Archive created successfully&amp;#34;&lt;/span&gt;)&#xA;}&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create new Writers for gzip and tar&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// These writers are chained. Writing to the tar writer will&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// write to the gzip writer which in turn will write to&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// the &amp;#34;buf&amp;#34; writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gzip&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Iterate over files and add them to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;range&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;&#x9;}&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&#xA;&#xA;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Open the file which will be written into the archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Open&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Get FileInfo about our file providing file size, mode, etc.&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Stat&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create a tar Header from the FileInfo data&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;FileInfoHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt;())&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Use full path as name (FileInfoHeader only takes the basename)&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// If we don&amp;#39;t do this the directory strucuture would&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// not be preserved&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// https://golang.org/src/archive/tar/common.go?#L626&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Write file header to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;WriteHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Copy file content to tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Copy&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Explanation&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            In the main function we first declare &lt;code&gt;files&lt;/code&gt; as a string slice.&#xA;            It contains the paths of the files that will be included in the archive.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            For this example I&amp;#39;ve created two text files. I placed one of them in the same&#xA;            directory as the &lt;code&gt;main.go&lt;/code&gt; file and the other one in a subdirectory. The purpose&#xA;            of this is to test that the directory structure will be correctly restored after extraction.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We then create the output file with &lt;code&gt;&lt;a href=&#34;https://golang.org/pkg/os/#Create&#34;&gt;os.Create()&lt;/a&gt;&lt;/code&gt;&#xA;            and pass it to the &lt;code&gt;createArchive&lt;/code&gt; function along with our file paths.&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Files which to include in the tar.gz archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test/test.txt&amp;#34;&lt;/span&gt;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create output file&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Create&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output.tar.gz&amp;#34;&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error writing archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create the archive and write the output to the &amp;#34;out&amp;#34; Writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;out&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Fatalln&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error creating archive:&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;)&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;fmt&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Println&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Archive created successfully&amp;#34;&lt;/span&gt;)&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;&#xA;        The &lt;code&gt;createArchive&lt;/code&gt; function creates two Writer: The &lt;a href=&#34;https://golang.org/pkg/archive/tar/#NewWriter&#34;&gt;tar Writer&lt;/a&gt;&#xA;        and the &lt;a href=&#34;https://golang.org/pkg/compress/gzip/#NewWriter&#34;&gt;gzip Writer&lt;/a&gt;. Both implement the &lt;a href=&#34;https://golang.org/pkg/io/#Writer&#34;&gt;io.Writer&lt;/a&gt; interface.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        The Writers are chained which means that bytes written to the tar Writer &lt;code&gt;tw&lt;/code&gt;&#xA;        will simultaneously be written to the gzip Writer &lt;code&gt;gw&lt;/code&gt;.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        We will then iterate over the files in the &lt;code&gt;files&lt;/code&gt; slice and&#xA;        call the &lt;code&gt;addToArchive&lt;/code&gt; function for each of them with the filename and the tar Writer&#xA;        as arguments.&#xA;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;createArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; []&lt;span style=&#34;color:#66d9ef&#34;&gt;string&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Writer&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;error&lt;/span&gt; {&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create new Writers for gzip and tar&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// These writers are chained. Writing to the tar Writer will&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// write to the gzip writer which in turn will write to&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// the &amp;#34;buf&amp;#34; writer&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gzip&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;buf&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;NewWriter&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;gw&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Iterate over files and and add them to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;range&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;files&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;addToArchive&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;&#x9;}&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt;&#xA;}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Inside the &lt;code&gt;addToArchive&lt;/code&gt; function we open the file and get&#xA;            a &lt;code&gt;&lt;a href=&#34;https://golang.org/pkg/os/#FileInfo&#34;&gt;FileInfo&lt;/a&gt;&lt;/code&gt;.&#xA;            The FileInfo contains information such as the file name, size or mode which is necessary for the next step.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Open the file which will be written into the archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;os&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Open&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Close&lt;/span&gt;()&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Get FileInfo about our file providing file size, mode, etc.&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Stat&lt;/span&gt;()&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;&#xA;        Each file in a tar archive has a &lt;a href=&#34;https://golang.org/pkg/archive/tar/#Header&#34;&gt;header&lt;/a&gt; containing metadata about the file&#xA;        followed by the file content. In this step we create the header by&#xA;        calling &lt;a href=&#34;https://golang.org/pkg/archive/tar/#FileInfoHeader&#34;&gt;FileInfoHeader&lt;/a&gt; which will take our FileInfo &lt;code&gt;info&lt;/code&gt;&#xA;        and generate a valid tar Header from it.&#xA;    &lt;/p&gt;&#xA;    &lt;p&gt;&#xA;        The os.FileInfo &lt;code&gt;info&lt;/code&gt; only stores the base name of the file. For example if we pass in &lt;code&gt;test/test.txt&lt;/code&gt; it&#xA;        will only store the filename &lt;code&gt;test.txt&lt;/code&gt;. This is a problem when creating the tar archive as it would omit&#xA;        the directory structure of our files.&#xA;        To fix this we have to set &lt;code&gt;header.Name&lt;/code&gt; to the full file path.&#xA;    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Create a tar Header from the FileInfo data&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tar&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;FileInfoHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;info&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt;())&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Use full path as name (FileInfoHeader only takes the basename)&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// If we don&amp;#39;t do this the directory strucuture would&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// not be preserved&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// https://golang.org/src/archive/tar/common.go?#L626&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Name&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;filename&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Now we can write the header and the file content to the Writer.&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Write file header to the tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;WriteHeader&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;header&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&#xA;&#x9;&lt;span style=&#34;color:#75715e&#34;&gt;// Copy file content to tar archive&#xA;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#x9;&lt;span style=&#34;color:#a6e22e&#34;&gt;_&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; = &lt;span style=&#34;color:#a6e22e&#34;&gt;io&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;Copy&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;tw&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;file&lt;/span&gt;)&#xA;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nil&lt;/span&gt; {&#xA;&#x9;&#x9;&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;err&lt;/span&gt;&#xA;&#x9;}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;h2&gt;Run the program&lt;/h2&gt;&#xA;    &lt;p&gt;We can now run our program and check that the files can be extracted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; go run main.go&#xA;Archive created successfully&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; tar xzfv output.tar.gz&#xA;x example.txt&#xA;x test/test.txt&#xA;&#xA;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt; exa --tree&#xA;.&#xA; example.txt&#xA; output.tar.gz&#xA; test&#xA;    test.txt&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Both files have been extracted successfully.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/writing-tar-gz-files-in-go/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Notes about Google CloudSQL for Postgres</title>
    <updated>2020-04-05T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-04-05:/notes-about-google-cloudsql-for-postgres/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            Here are a few things that I&amp;#39;ve learned about Google CloudSQL for Postgres&#xA;            during the last 2 years in which I&amp;#39;ve been using it.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Price&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;It&amp;#39;s usually the most expensive part of the project accounting for around 80% of the total cost. The parts that need to be paid for are CPU Cores, RAM, Disk Storage and Network Internet Egress. Automated backups and HA are optional and cost extra.&lt;/li&gt;&#xA;            &lt;li&gt;There&amp;#39;s a &lt;a href=&#34;https://cloud.google.com/products/calculator&#34;&gt;pricing calculator&lt;/a&gt; available but I haven&amp;#39;t been able to replicate the price on the invoice (at least in my case the total on the invoice was &lt;em&gt;lower&lt;/em&gt; than what the calculator showed). I suggest to just try it for one or two months and see for yourself. Make sure to set a budget in GCP so the cost doesn&amp;#39;t go too high.&lt;/li&gt;&#xA;            &lt;li&gt;Read-only replicas need to have at least the same hardware as the master instance. This means each read-replica will double the cost.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Replication&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;As mentioned above a read-replica needs to have at least the same hardware (cores, memory, storage) as the master instance. It can have better hardware.&lt;/li&gt;&#xA;            &lt;li&gt;External replication is not supported. You can create read-only replicas in CloudSQL but it&amp;#39;s not possible to use streaming replication to an external Postgres instance.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;HA (High Availability)&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Failover will take place after the master instance is unresponsive for 1 minute. In total it takes 2-3 minutes for connections to be re-established.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Backups&lt;/h2&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Deleting the instance will delete all of its backups too. Always make sure to have an additional backup job running that will export the data to another location.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Performance&lt;/h2&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Network throughput (MB/s) depends on the number of CPU cores. More CPU Cores = More throughput. 1 CPU core has 250 MB/s throughput and the maximum is 2000 MB/s which is reached at 8 cores.&lt;/li&gt;&#xA;            &lt;li&gt;Disk throughput and IOPS depend on the disk size. The minimum size is 10 GB which has 4.8 MB/s of read/write throughput and 300 IOPS (read/write). The maximum is 800 MB/s read and 400 MB/s write throughput with 15,000 IOPS (read/write) which is reached at 500 GB disk size.&lt;/li&gt;&#xA;            &lt;li&gt;The network latency from a GKE instance to CloudSQL is around 3ms. There is no difference in latency between using a private and public ip.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;h2&gt;Maintenance&lt;/h2&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;The maintenance downtime is 1-2 minutes and occurs during a selected time window.&lt;/li&gt;&#xA;            &lt;li&gt;Maintenance notifications were recently added. Only e-mail notifications are supported.&lt;/li&gt;&#xA;            &lt;li&gt;Upgrading Postgres to a new major version is only possible by dumping the data and then importing it after the upgrade. For our 128 GB database it took around 40 minutes to export and 5 hours to import (pg_restore). This is not including the time it took to download the export from Cloud Storage.&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/notes-about-google-cloudsql-for-postgres/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Managing Helm Charts with Helmfile</title>
    <updated>2020-03-29T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-29:/managing-helm-charts-with-helmfile/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            In this blog post I&amp;#39;m going to show how &lt;a href=&#34;https://github.com/roboll/helmfile&#34;&gt;Helmfile&lt;/a&gt; makes it easier&#xA;            to manage Helm charts and environments.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this I&amp;#39;m going to walk through an example where at the beginning&#xA;            we install helm charts over the CLI using the &lt;code&gt;helm&lt;/code&gt; command,&#xA;            and then refactor the code in steps to use the &lt;code&gt;helmfile&lt;/code&gt; command instead.&#xA;        &lt;/p&gt;&#xA;        &lt;h2&gt;Setup&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Our setup consists of 2 applications (backend and frontend) and Prometheus&#xA;            for metrics. We have helm charts for:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Backend (custom chart)&lt;/li&gt;&#xA;            &lt;li&gt;Frontend (custom chart)&lt;/li&gt;&#xA;            &lt;li&gt;Prometheus (chart from the &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus&#34;&gt;helm stable repo&lt;/a&gt;)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            which are deployed into these environments:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Development&lt;/li&gt;&#xA;            &lt;li&gt;Staging&lt;/li&gt;&#xA;            &lt;li&gt;Production&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            The files are organized in this directory structure:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;.&#xA; charts&#xA;    backend&#xA;      Chart.yaml&#xA;      templates&#xA;      values-development.yaml&#xA;      values-staging.yaml&#xA;      values-production.yaml&#xA;      secrets-development.yaml&#xA;      secrets-staging.yaml&#xA;      secrets-production.yaml&#xA;    frontend&#xA;      Chart.yaml&#xA;      templates&#xA;      values-development.yaml&#xA;      values-staging.yaml&#xA;      values-production.yaml&#xA;      secrets-development.yaml&#xA;      secrets-staging.yaml&#xA;      secrets-production.yaml&#xA;    prometheus&#xA;       values-development.yaml&#xA;       values-staging.yaml&#xA;       values-production.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            Each values-development.yaml, values-staging.yaml, values-production.yaml file&#xA;            contains values that are specific to that environment.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            For example the development environment only needs to deploy 1 replica of&#xA;            the backend while the staging and production environments need 3 replicas.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We use &lt;a href=&#34;https://github.com/fstech/helm-secrets&#34;&gt;helm-secrets&lt;/a&gt; to manage secrets.&#xA;            Each secrets file is encrypted and has to be manually decrypted before deploying the chart.&#xA;            After the deployment is done the decrypted file has to be deleted.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Installation and Upgrades&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            With the above setup we use the following commands to deploy&#xA;            (install/upgrade) the backend chart in the staging environment:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helm secrets dec ./charts/backend/secrets-backend.yaml&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helm upgrade --install --atomic --cleanup-on-fail -f ./charts/backend/values-staging.yaml -f ./charts/backend/secrets-staging.yaml backend ./charts/backend&#xA;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; rm ./charts/backend/secrets-backend.yaml.dec&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            We use the &lt;code&gt;helm upgrade&lt;/code&gt; command with the &lt;code&gt;--install&lt;/code&gt; flag to&#xA;            be able to install and upgrade charts with the same command.&#xA;&#xA;            We also use the &lt;code&gt;--atomic&lt;/code&gt; and &lt;code&gt;--cleanup-on-fail&lt;/code&gt;&#xA;            flags to rollback changes in case a chart upgrade fails.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To deploy the other charts we have to repeat the same commands (for the prometheus&#xA;            chart we can leave out the part that handles secrets).&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Now the problem is that it&amp;#39;s hard to remember the exact commands to&#xA;            run when deploying a chart (especially when the upgrades are not very frequent).&#xA;&#xA;            When multiple people are responsible for deployments it&amp;#39;s also difficult to&#xA;            make sure the same commands are used. If, for example, the secrets were not&#xA;            decrypted beforehand it will lead to encrypted values being deployed and probably&#xA;            crash the application.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Writing Bash Scripts&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To fix the issues mentioned above we can write bash scripts that execute&#xA;            the exact commands needed for a deployment.&#xA;&#xA;            We create one script per environment in each chart directory which leads to the&#xA;            following directory tree for the backend chart:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;.&#xA; charts&#xA;    backend&#xA;       Chart.yaml&#xA;       templates/&#xA;       values-development.yaml&#xA;       values-staging.yaml&#xA;       values-production.yaml&#xA;       secrets-development.yaml&#xA;       secrets-staging.yaml&#xA;       secrets-production.yaml&#xA;       deploy-development.sh&#xA;       deploy-staging.sh&#xA;       deploy-production.sh&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            When we want to deploy the backend chart in the staging environment&#xA;            we can run:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; ./charts/backend/deploy-staging.sh&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            This works fine for small environments like in the example above, but&#xA;            for larger environments with 15 or 20 charts it will lead to a lot of&#xA;            similar-looking bash scripts with large amounts of code duplication.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Provisioning a new environment would mean that a new deploy script&#xA;            has to be created in each chart directory. If we have 15 charts&#xA;            that means we have to copy one of the existing deploy scripts 15 times&#xA;            and search/replace the contents to match the new environment name.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To avoid duplicating the same code over and over again we could consolidate&#xA;            all of our small deploy scripts into one large deploy script. But this comes&#xA;            with a cost: We have to spend time maintaining it, fixing bugs and possibly&#xA;            extend it to handle new environments.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            At this point Helmfile comes in handy. Instead of writing our custom deploy&#xA;            script we can declare our environments in a YAML file and let it handle the&#xA;            deployment logic for us.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Using a Helmfile&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Using the backend chart as an example we can write the following content&#xA;            into a &lt;code&gt;helmfile.yaml&lt;/code&gt; file to manage the staging deployment:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-staging.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-staging.yaml&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can deploy the chart by running:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            In the background Helmfile will run the same &lt;code&gt;helm upgrade --install ...&lt;/code&gt; command as before.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note that there&amp;#39;s no need to manually decrypt secrets anymore as Helmfile has built-in&#xA;            support for helm-secrets.&#xA;            This means that any file that is listed under &lt;code&gt;secrets:&lt;/code&gt; will automatically be decrypted&#xA;            and after the deployment is finished the decrypted file will automatically be removed.&#xA;        &lt;/p&gt;&#xA;&#xA;&#xA;        &lt;h2&gt;Environments&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            The above example uses the &lt;code&gt;values-staging.yaml&lt;/code&gt; file as chart&#xA;            values. To be able to use multiple environments we can list them under&#xA;            the &lt;code&gt;environments:&lt;/code&gt; key at the beginning of the helmfile&#xA;            and then use the environment name as a variable in the release definition.&#xA;            The file would now look like this:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-{{ .Environment.Name }}.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            When deploying the chart we now have to use the &lt;code&gt;--environment/-e&lt;/code&gt; option when&#xA;            executing the helmfile command:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging sync&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now easily create new environments by listing them under&#xA;            &lt;code&gt;environments&lt;/code&gt; instead of duplicating our bash scripts.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Templates&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            After adding all of our helm charts into the helmfile the file content&#xA;            would look like this:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;releases:&#xA;- name: backend&#xA;  chart: charts/backend&#xA;  values:&#xA;  - charts/backend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/backend/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;- name: frontend&#xA;  chart: charts/frontend&#xA;  values:&#xA;  - charts/frontend/values-{{ .Environment.Name }}.yaml&#xA;  secrets:&#xA;  - charts/frontend/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;- name: prometheus&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;  values:&#xA;  - charts/prometheus/values-{{ .Environment.Name }}.yaml&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The same pattern (for values and secrets) is repeated for each release. While in the&#xA;            example above we only have 3 releases the pattern will continue for future additions and&#xA;            eventually lead to much duplicated code.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can avoid copy/pasting the release definitions by using Helmfile templates.&#xA;            A template is defined at the top of the file and then referenced in the release by using&#xA;            &lt;a href=&#34;https://confluence.atlassian.com/bitbucket/yaml-anchors-960154027.html&#34;&gt;YAML anchors&lt;/a&gt;.&#xA;            This is our helmfile after using templates:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;templates:&#xA;  default: &amp;amp;default&#xA;    chart: charts/{{`{{ .Release.Name }}`}}&#xA;    missingFileHandler: Warn&#xA;    values:&#xA;    - charts/{{`{{ .Release.Name }}`}}/values-{{ .Environment.Name }}.yaml&#xA;    secrets:&#xA;    - charts/{{`{{ .Release.Name }}`}}/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;releases:&#xA;- name: backend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: frontend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: prometheus&#xA;  &amp;lt;&amp;lt;: *default&#xA;  # override the defaults since it&amp;#39;s a remote chart&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            We have removed much of the duplicated code from our helmfile and can&#xA;            now easily add new environments and releases.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Helm Defaults&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We&amp;#39;ve previously used the the &lt;code&gt;--atomic&lt;/code&gt; and &lt;code&gt;--cleanup-on-fail&lt;/code&gt;&#xA;            options when deploying charts. To do the same when using Helmfile we just&#xA;            have to specify them under &lt;code&gt;helmDefaults&lt;/code&gt;:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;helmDefaults:&#xA;  atomic: true&#xA;  cleanupOnFail: true&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Running Helmfile Commands&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Here are a few examples of helmfile commands for common operations.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To install or upgrade all charts in an environment (using staging as an example)&#xA;            we run:&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            If we just want to sync (meaning to install/upgrade) a single chart we can use selectors. This command&#xA;            will sync the backend chart in the staging environment with our local values:&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging -l name=backend sync&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            To show the changes an operation would perform on a cluster without&#xA;            actually applying them we can run the following command (requires the&#xA;            &lt;a href=&#34;https://github.com/databus23/helm-diff&#34;&gt;helm-diff&lt;/a&gt; plugin):&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span class=&#34;cmd&#34;&gt;$&lt;/span&gt; helmfile -e staging -l name=prometheus diff&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Full Example Code&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            This is the final content of our helmfile.yaml file:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;environments:&#xA;  development:&#xA;  staging:&#xA;  production:&#xA;&#xA;helmDefaults:&#xA;  atomic: true&#xA;  cleanupOnFail: true&#xA;&#xA;templates:&#xA;  default: &amp;amp;default&#xA;    chart: charts/{{`{{ .Release.Name }}`}}&#xA;    missingFileHandler: Warn&#xA;    values:&#xA;    - charts/{{`{{ .Release.Name }}`}}/values-{{ .Environment.Name }}.yaml&#xA;    secrets:&#xA;    - charts/{{`{{ .Release.Name }}`}}/secrets-{{ .Environment.Name }}.yaml&#xA;&#xA;releases:&#xA;- name: backend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: frontend&#xA;  &amp;lt;&amp;lt;: *default&#xA;&#xA;- name: prometheus&#xA;  &amp;lt;&amp;lt;: *default&#xA;  chart: stable/prometheus&#xA;  version: 11.0.4&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;        The directory structure did not change and is the same as described at the top of the post.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/managing-helm-charts-with-helmfile/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Setting up Vim for YAML editing</title>
    <updated>2020-03-21T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-21:/setting-up-vim-for-yaml/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;&#xA;        In this blog post I&amp;#39;m going to show how to set up Vim for easier YAML editing.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;full-example.png&#34; alt=&#34;Screenshot of Vim&#34;/&gt;&#xA;&#xA;        &lt;p&gt;You can scroll down to the end for a summary of all installed plugins and config file changes.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Syntax Highlighting&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s not much to do here. VIM has YAML syntax highlighting built-in and it&amp;#39;s great.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s one thing I want to mention though. A few years back YAML highlighting in Vim&#xA;            was very slow, and there was often a noticeable lag when opening large files.&#xA;            The workaround was to use the &lt;a href=&#34;https://github.com/stephpy/vim-yaml&#34;&gt;vim-yaml&lt;/a&gt; plugin for&#xA;            fast syntax highlighting.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Not sure if it&amp;#39;s still worth installing I decided to make a performance benchmark.&#xA;        I loaded up a &lt;a href=&#34;https://github.com/istio/istio/blob/master/manifests/base/files/gen-istio-cluster.yaml&#34;&gt;large YAML file&lt;/a&gt; (6100 lines) and compared the time:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span class=&#34;comment&#34;&gt;# default syntax highlighting&lt;/span&gt;&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; vim gen-istio-cluster.yaml --startuptime default.log&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; tail -1 default.log&#xA;&lt;span class=&#34;out&#34;&gt;055.563&lt;/span&gt;&#xA;&#xA;&lt;span class=&#34;comment&#34;&gt;# vim-yaml plugin&lt;/span&gt;&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; vim gen-istio-cluster.yaml --startuptime vimyaml.log&#xA;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; tail -1 vimyaml.log&#xA;&lt;span class=&#34;out&#34;&gt;060.320&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            As we can see the default syntax highlighting is just as fast as the plugin and there&amp;#39;s&#xA;            no need to install a separate plugin to fix the slow syntax highlighting anymore.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Indentation&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Indentation probably the most annoying part about editing YAML files.&#xA;            Large documents with deeply nested blocks are often hard to track and&#xA;            errors are easily made.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            YAML documents are required to have a 2 space indentation. However, Vim does not set this&#xA;            by default but it&amp;#39;s an easy fix by putting the following line in the vim config:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;autocmd FileType yaml setlocal ts=2 sts=2 sw=2 expandtab&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can also setup Indentation guides.&#xA;            Indentation guides are thin vertical lines at each indentation level and useful&#xA;            to help line up nested YAML blocks.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can display those lines by using the &lt;a href=&#34;https://github.com/Yggdroot/indentLine&#34;&gt;indentLine plugin&lt;/a&gt;.&#xA;            I&amp;#39;ve modified the indentation character to display a thinner line (default is &amp;#34;&amp;#34;):&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;let g:indentLine_char = &amp;#39;&amp;#39;&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The result should look like this:&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;indentlines.png&#34; alt=&#34;Screenshot of Vim showing the indentLine plugin&#34;/&gt;&#xA;&#xA;        &lt;h2&gt;Folding&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            With folding we can hide parts of the file that are not relevant to our current task.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Vim has built-in support for folding based on the indentation level but the default&#xA;            folding rules make it hard to tell what is folded. This is because the folding starts&#xA;            on the line &lt;em&gt;following&lt;/em&gt; the start of a block. To change this we can install&#xA;            the &lt;a href=&#34;https://github.com/pedrohdz/vim-yaml-folds&#34;&gt;vim-yaml-folds&lt;/a&gt; plugin.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Here&amp;#39;s a side-by-side comparison of the default folding (left) compared to vim-yaml-folds (right):&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;folding-compare.png&#34; alt=&#34;comparison of default folding with vim-yaml-folds&#34;/&gt;&#xA;        &lt;p&gt;&#xA;            To work with folding we need to remember a few keyboard commands. Vimcasts has&#xA;            a great episode on this &lt;a href=&#34;http://vimcasts.org/episodes/how-to-fold/&#34;&gt;here&lt;/a&gt;.&#xA;            Most of the time I use the following commands:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;za: Toggle current fold&lt;/li&gt;&#xA;            &lt;li&gt;zR: Expand all folds&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            After the plugin is installed and folding is enabled the default settings will fold all&#xA;            blocks by default. To start with unfolded content we can set:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;set foldlevelstart=20&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            There&amp;#39;s also a plugin called &lt;a href=&#34;https://www.vim.org/scripts/script.php?script_id=4021&#34;&gt;restore_view&lt;/a&gt;&#xA;            which will save the folds for each file. But be aware that this plugin will create an&#xA;            extra file with folding information for each opened document.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Linting&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            Linting will analyze the code and show any potential errors while we&amp;#39;re writing it which helps&#xA;            us catch formatting or syntax errors early on.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this in Vim we can use &lt;a href=&#34;https://github.com/dense-analysis/ale&#34;&gt;ALE&lt;/a&gt;,&#xA;            an asynchronous linting framework that has support for many languages and tools including YAML.&#xA;            To enable YAML linting in ALE we have to install &lt;a href=&#34;https://github.com/adrienverge/yamllint&#34;&gt;yamllint&lt;/a&gt;,&#xA;            a Python-based YAML linter.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Installation instructions are &lt;a href=&#34;https://yamllint.readthedocs.io/en/stable/quickstart.html#installing-yamllint&#34;&gt;here&lt;/a&gt;.&#xA;            On macOS we can install it with Homebrew:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;&lt;span class=&#34;shell&#34;&gt;$&lt;/span&gt; brew install yamllint&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The default configuration is fairly strict and shows errors in document style such as&#xA;            line length, trailing spaces or comment indentation.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            We can modify the configuration to be less strict.&#xA;            Yamllint already comes with a &lt;a href=&#34;https://github.com/adrienverge/yamllint/blob/master/yamllint/conf/relaxed.yaml&#34;&gt;relaxed&lt;/a&gt;&#xA;            version of the default config that is a good starting point.&#xA;            The only additional thing I&amp;#39;ve decided to disable is line length checking.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            To do this we open up &lt;code&gt;~/.config/yamllint/config&lt;/code&gt; and&#xA;            paste the following:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;extends: relaxed&#xA;&#xA;rules:&#xA;  line-length: disable&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;I&amp;#39;ve modified the ALE configuration to change the message format, error symbols and only lint&#xA;    when the file is saved:&lt;/p&gt;&#xA;&lt;pre&gt;let g:ale_echo_msg_format = &amp;#39;[%linter%] %s [%severity%]&amp;#39;&#xA;let g:ale_sign_error = &amp;#39;&amp;#39;&#xA;let g:ale_sign_warning = &amp;#39;&amp;#39;&#xA;let g:ale_lint_on_text_changed = &amp;#39;never&amp;#39;&#xA;&lt;/pre&gt;&#xA;    &lt;p&gt;&#xA;        We can see the errors and warnings on the left side:&#xA;    &lt;/p&gt;&#xA;        &lt;img src=&#34;full-example.png&#34; alt=&#34;Screenshot of Vim&#34;/&gt;&#xA;&#xA;    &lt;h2&gt;Summary&lt;/h2&gt;&#xA;    &lt;p&gt;&#xA;        Here&amp;#39;s a summary of the plugins, applications and config modifications:&#xA;    &lt;/p&gt;&#xA;    &lt;h3&gt;Vim Plugins&lt;/h3&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/Yggdroot/indentLine&#34;&gt;indentLine&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/pedrohdz/vim-yaml-folds&#34;&gt;vim-yaml-folds&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/dense-analysis/ale&#34;&gt;ALE&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;h3&gt;Applicatins&lt;/h3&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;https://github.com/adrienverge/yamllint&#34;&gt;yamllint&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;h3&gt;Config&lt;/h3&gt;&#xA;    &lt;p&gt;In &lt;code&gt;~/.vimrc&lt;/code&gt; or &lt;code&gt;~/.config/nvim/init.vim&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;autocmd FileType yaml setlocal ts=2 sts=2 sw=2 expandtab&#xA;&#xA;set foldlevelstart=20&#xA;&#xA;let g:ale_echo_msg_format = &amp;#39;[%linter%] %s [%severity%]&amp;#39;&#xA;let g:ale_sign_error = &amp;#39;&amp;#39;&#xA;let g:ale_sign_warning = &amp;#39;&amp;#39;&#xA;let g:ale_lint_on_text_changed = &amp;#39;never&amp;#39;&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;p&gt;In &lt;code&gt;~/.config/yamllint/config&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;extends: relaxed&#xA;&#xA;rules:&#xA;  line-length: disable&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/setting-up-vim-for-yaml/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Private Helm Repo with GCS and GitHub Actions</title>
    <updated>2020-03-08T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-08:/private-helm-repo-with-gcs-and-github-actions/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;In this blog post I&amp;#39;m going to show how to setup a private Helm chart repository on&#xA;Google Cloud Storage (GCS) and use GitHub Actions to automatically push charts on new commits.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Setting up the GCS Bucket&lt;/h2&gt;&#xA;        &lt;p&gt;The first step is to create a GCS bucket that will hold our charts. We can do this over the CLI with the gcloud-sdk or over the Web UI. I&amp;#39;m going to use the CLI for the following examples.&lt;/p&gt;&lt;p&gt;To make it easier to handle access permissions we use the &lt;code&gt;-b on&lt;/code&gt; flag to enable uniform bucket-level access. It let&amp;#39;s us manage permissions on a bucket-level rather than on an object-level:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil mb -b on gs://my-chart-repo-arthurk&#xA;&lt;span class=&#34;out&#34;&gt;Creating gs://my-chart-repo-arthurk/...&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For Helm to be able to push charts to this bucket we need a Cloud IAM service account (with key) with &lt;em&gt;Storage Object Admin&lt;/em&gt; permissions:&lt;/p&gt;&#xA;&#xA;        &lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud iam service-accounts create my-chart-repo-svc-acc&#xA;&lt;span class=&#34;out&#34;&gt;Created service account [my-chart-repo-svc-acc].&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud iam service-accounts keys create service-account.json --iam-account=my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com&#xA;&lt;span class=&#34;out&#34;&gt;created key [123123123] of type [json] as [service-account.json] for [my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com]&lt;/span&gt;&#xA;&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil iam ch serviceAccount:my-chart-repo-svc-acc@PROJECT.iam.gserviceaccount.com:roles/storage.objectAdmin gs://my-chart-repo-arthurk&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;When referring to the service account we have to use the email (not the name) which has the format &lt;code&gt;SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Setting up GitHub Actions&lt;/h2&gt;&#xA;        &lt;p&gt;In this step we&amp;#39;re going to setup GitHub Actions to detect charts that have changed and add them to our&#xA;            helm repo.&lt;/p&gt;&#xA;        &lt;p&gt;We start by creating the &lt;code&gt;.github/workflows/helm-ci.yml&lt;/code&gt; file and add:&lt;/p&gt;&#xA;        &lt;pre&gt;name: Helm Charts&#xA;on: [push]&#xA;&#xA;jobs:&#xA;  release:&#xA;    name: Release&#xA;    runs-on: ubuntu-latest&#xA;    steps:&#xA;      - name: Checkout&#xA;        uses: actions/checkout@v2&#xA;        with:&#xA;          fetch-depth: 2&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;By default the checkout action will clone the repo with a detached HEAD. To later compare files that have changed between the current HEAD and the previous commit we have to pass &lt;code&gt;fetch-depth: 2&lt;/code&gt; to the action.&lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            After pushing the code we can open GitHub Actions in the browser and check the workflow. It should look like this:&#xA;        &lt;/p&gt;&#xA;&#xA;            &lt;img src=&#34;gh-actions-1.png&#34; alt=&#34;GitHub Actions checkout step finished successfully&#34;/&gt;&#xA;&#xA;        &lt;h2&gt;Installing Helm and helm-gcs&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            The next step in the CI pipeline is to install Helm and the &lt;a href=&#34;https://github.com/hayorov/helm-gcs&#34;&gt;helm-gcs plugin&lt;/a&gt;. We add the following step to our workflow:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;- name: Install helm and plugins&#xA;  run: ./scripts/install.sh&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;and then create the &lt;code&gt;scripts/install.sh&lt;/code&gt; file with the following content:&lt;/p&gt;&#xA;        &lt;pre&gt;#!/usr/bin/env bash&#xA;&#xA;set -o errexit&#xA;&#xA;HELM_VERSION=3.1.1&#xA;HELM_GCS_VERSION=0.3.1&#xA;&#xA;echo &amp;#34;Installing Helm...&amp;#34;&#xA;wget -q https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz&#xA;tar -zxf helm-v${HELM_VERSION}-linux-amd64.tar.gz&#xA;sudo mv linux-amd64/helm /usr/local/bin/helm&#xA;helm version&#xA;&#xA;echo &amp;#34;Installing helm-gcs plugin...&amp;#34;&#xA;helm plugin install https://github.com/hayorov/helm-gcs --version ${HELM_GCS_VERSION}&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Set &lt;code&gt;chmod u+x scripts/install.sh&lt;/code&gt; and push the file. We can check GitHub Actions to make sure everything installed correctly:&lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;gh-actions-2.png&#34; alt=&#34;GitHub Actions showing that helm and helm-gcs have been installed successfully&#34;/&gt;&#xA;&#xA;        &lt;p&gt;This shows us that Helm 3.1.1 and helm-gcs 0.3.0 have been successfully installed&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Initializing the helm repository&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;We can now initialize the helm repository. For this to work we need to add our previously created&#xA;            service account key to GitHub. To do this we navigate to the repository and click on&#xA;            &lt;strong&gt;&amp;#34;Settings&amp;#34;  &amp;#34;Secrets&amp;#34;  &amp;#34;Add a new secret&amp;#34;&lt;/strong&gt;. There we set the name to be&#xA;            &lt;code&gt;GCLOUD_SERVICE_ACCOUNT_KEY&lt;/code&gt; and as a value add the content of the service-account.json file.&#xA;            After saving the secret it should look like this:&#xA;        &lt;/p&gt;&#xA;        &lt;img src=&#34;gh-actions-3.png&#34; alt=&#34;GitHub showing that a secret has been added to the project&#34;/&gt;&#xA;        &lt;p&gt;We can now modify the workflow to pass the secret as an environment variable to our next shell script:&lt;/p&gt;&#xA;&lt;pre&gt;- name: Release charts&#xA;  run: ./scripts/release.sh&#xA;  env:&#xA;    GCLOUD_SERVICE_ACCOUNT_KEY: ${{ secrets.GCLOUD_SERVICE_ACCOUNT_KEY }}&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;In the release.sh script we save the service account to a file and point the&#xA;            &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt; environment variable to it. This is needed for helm-gcs&#xA;            plugin to authenticate. Afterwards we initialize the GCS repo which will create an&#xA;            empty &lt;code&gt;index.yaml&lt;/code&gt; file in the GCS bucket. Finally we can add the repo to helm&#xA;            so it can access its packages.&#xA;        &lt;/p&gt;&#xA;        &lt;pre&gt;#!/usr/bin/env bash&#xA;&#xA;set -o errexit&#xA;set -o nounset&#xA;set -o pipefail&#xA;&#xA;GCS_BUCKET_NAME=&amp;#34;gs://my-chart-repo-arthurk&amp;#34;&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# setup service account for helm-gcs plugin&lt;/span&gt;&#xA;echo &amp;#34;${GCLOUD_SERVICE_ACCOUNT_KEY}&amp;#34; &amp;gt; svc-acc.json&#xA;export GOOGLE_APPLICATION_CREDENTIALS=svc-acc.json&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# initializing helm repo&lt;/span&gt;&#xA;&lt;span class=&#34;out&#34;&gt;# (only needed on first run but will do nothing if already exists)&lt;/span&gt;&#xA;echo &amp;#34;Initializing helm repo&amp;#34;&#xA;helm gcs init ${GCS_BUCKET_NAME}&#xA;&#xA;&lt;span class=&#34;out&#34;&gt;# add gcs bucket as helm repo&lt;/span&gt;&#xA;echo &amp;#34;Adding gcs bucket repo ${GCS_BUCKET_NAME}&amp;#34;&#xA;helm repo add private ${GCS_BUCKET_NAME}&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;Before committing the file make sure to mark it as executable with &lt;code&gt;chmod u+x scripts/release.sh&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Packaging and Pushing changed Charts&lt;/h2&gt;&#xA;        &lt;p&gt;In the final step of our CI script we need to identify which charts have changed and then&#xA;            package and push them to the helm repo. We do this by running &lt;code&gt;git diff&lt;/code&gt; on the previous&#xA;            revision with the following arguments:&lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&lt;code&gt;--find-renames&lt;/code&gt; detect if a file has been renamed&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;--diff-filter=d&lt;/code&gt; will ignore deleted files (we can&amp;#39;t package/push a deleted chart)&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;--name-only&lt;/code&gt; only print the name of the changed file&lt;/li&gt;&#xA;            &lt;li&gt;&lt;code&gt;cut -d &amp;#39;/&amp;#39; -f 2 | uniq&lt;/code&gt; we only need unique directory names of files that have changed&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;We add the following content to the release.sh file:&lt;/p&gt;&#xA;&lt;pre&gt;prev_rev=$(git rev-parse HEAD^)&#xA;echo &amp;#34;Identifying changed charts since git rev ${prev_rev}&amp;#34;&#xA;&#xA;changed_charts=()&#xA;readarray -t changed_charts &amp;lt;&amp;lt;&amp;lt; &amp;#34;$(git diff --find-renames --diff-filter=d --name-only &amp;#34;$prev_rev&amp;#34; -- charts | cut -d &amp;#39;/&amp;#39; -f 2 | uniq)&amp;#34;&#xA;&#xA;if [[ -n &amp;#34;${changed_charts[*]}&amp;#34; ]]; then&#xA;    for chart in &amp;#34;${changed_charts[@]}&amp;#34;; do&#xA;        echo &amp;#34;Packaging chart &amp;#39;$chart&amp;#39;...&amp;#34;&#xA;        chart_file=$(helm package &amp;#34;charts/$chart&amp;#34; | awk &amp;#39;{print $NF}&amp;#39;)&#xA;&#xA;        echo &amp;#34;Pushing $chart_file...&amp;#34;&#xA;        helm gcs push &amp;#34;$chart_file&amp;#34; private&#xA;    done&#xA;else&#xA;    echo &amp;#34;No chart changes detected&amp;#34;&#xA;fi&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;Commit and push the changes. After the CI run has finished, the GCS&#xA;            bucket will be initialized and have an &lt;code&gt;index.yaml&lt;/code&gt; file in it.&#xA;        This file is an index of all the helm charts in the repo. As we currently have no charts indexed it has&#xA;        the following content:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gsutil cat gs://my-chart-repo-arthurk/index.yaml&#xA;&lt;span class=&#34;out&#34;&gt;apiVersion: v1&#xA;entries: {}&#xA;generated: &amp;#34;2020-03-08T06:51:49.496564824Z&amp;#34;&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Releasing our first chart&lt;/h2&gt;&#xA;        &lt;p&gt;We can now create and add the first chart to our helm repository. We do this by creating a &lt;em&gt;chart/&lt;/em&gt;&#xA;            directory and running &lt;code&gt;helm create&lt;/code&gt; to create an example chart:&#xA;&#xA;&lt;/p&gt;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; mkdir charts&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm create charts/foo&#xA;&lt;span class=&#34;out&#34;&gt;Creating charts/foo&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;Add, commit and push all new files, then check GitHub Actions. It will show us that the chart&#xA;            was successfully packaged and pushed to the repo:&lt;/p&gt;&#xA;&#xA;        &lt;img src=&#34;gh-actions-4.png&#34; alt=&#34;GitHub Actions showing that the chart has been successfully added to the repo&#34;/&gt;&#xA;&#xA;        &lt;p&gt;Note that it&amp;#39;s not possible to push the same chart version to the same repo. The push will fail.&#xA;        We need to always make sure to increase the &lt;code&gt;version&lt;/code&gt; value in the &lt;code&gt;Chart.yaml&lt;/code&gt; file when releasing a new chart.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Trying it out&lt;/h2&gt;&#xA;        &lt;p&gt;To try out our private helm repo we can add it to helm on our client machine and list the repo contents:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm plugin install https://github.com/hayorov/helm-gcs&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; gcloud auth application-default login&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm repo add private-repo gs://my-chart-repo-arthurk&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm repo update&#xA;&lt;span style=&#34;color: #CD5C5C&#34;&gt;$&lt;/span&gt; helm search repo private-repo -l&#xA;&lt;span class=&#34;out&#34;&gt;NAME            &#x9;CHART VERSION&#x9;APP VERSION&#x9;DESCRIPTION&#xA;private-repo/foo&#x9;0.1.0        &#x9;1.16.0     &#x9;A Helm chart for Kubernetes&lt;/span&gt;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;As you can see the chart was successfully added to the registry. It can now be used as any other chart, for example by installing it with &lt;code&gt;helm install private-repo/foo --version 0.1.0&lt;/code&gt;.&lt;/p&gt;&#xA;        &lt;p&gt;The source code for all examples is available &lt;a href=&#34;https://github.com/arthurk/private-gcs-helm&#34;&gt;in this GitHub repo&lt;/a&gt;.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/private-helm-repo-with-gcs-and-github-actions/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Writing Reusable Helm Charts</title>
    <updated>2020-03-01T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-03-01:/writing-reusable-helm-charts/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm charts&lt;/a&gt; make it possible to easily package &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; manifests, version them and share them with&#xA;            other developers. To use a Helm chart across projects with different requirements it needs to be &lt;em&gt;reusable&lt;/em&gt;, meaning&#xA;            that common parts of the Kubernetes manifests can be changed in a values file without having to re-write the templates.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Let&amp;#39;s say we are looking into deploying Prometheus via Helm into our Kubernetes cluster. We search&#xA;            around and find a chart that is stable, well documented and actively maintained. It looks like a good choice.&#xA;            But there are a few options that you need to change in order to fit our requirements. Normally this could be done&#xA;            by creating a &lt;code&gt;values.yaml&lt;/code&gt; file and overriding the default settings. However, the chart that is available&#xA;            is not reusable enough and the options that we need to change are not available.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In such a case the only option for us is to copy the whole chart and modify it to fit the requirements even if the modification&#xA;            is only 1 or 2 lines of code. After copying the chart we also have to maintain it and keep it up to date with the upstream&#xA;            branch. It would&amp;#39;ve saved us a lot of time and work if the chart added a few options to make it reusable&#xA;            for projects that have different requirements.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            In the next sections I&amp;#39;m going to go over the templates from the default Helm chart template (that is created when running&#xA;            &lt;code&gt;helm create&lt;/code&gt;) and explain what makes them reusable (and what can be improved).&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Ingress&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;An Ingress allows external users to access Kubernetes Services. It provides a reverse-proxy,&#xA;        configurable traffic routing and TLS termination. There are several Ingress controllers available&#xA;        such as &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;nginx&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes/ingress-gce&#34;&gt;GCE&lt;/a&gt;, &lt;a href=&#34;https://github.com/containous/traefik&#34;&gt;Traefik&lt;/a&gt;, &lt;a href=&#34;https://github.com/haproxytech/kubernetes-ingress/&#34;&gt;HAProxy&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34;&gt;and more&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;For a reusable Ingress template we need to consider the following requirements:&lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;Using the Ingress should be optional. Not every developer wants to expose their service to external users&lt;/li&gt;&#xA;            &lt;li&gt;It should be possible to choose an ingress controller such as nginx or GCE&lt;/li&gt;&#xA;            &lt;li&gt;Traffic routing should be configurable&lt;/li&gt;&#xA;            &lt;li&gt;TLS should be optional&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The &lt;a href=&#34;https://gist.github.com/arthurk/d872e92fabfca4f2e6af84662da10106&#34;&gt;default Ingress template&lt;/a&gt; meets all of&#xA;        our requirements and is a great example of a reusable template. The custom annotations are a very important part.&#xA;        A typical usage example of would look like this:&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;ingress:&#xA;  enabled: true&#xA;  annotations:&#xA;    kubernetes.io/ingress.class: nginx&#xA;    nginx.ingress.kubernetes.io/server-snippet: |&#xA;      add_header X-Frame-Options &amp;#34;DENY&amp;#34;;&#xA;      proxy_set_header X-Frame-Options &amp;#34;DENY&amp;#34;;&#xA;    certmanager.k8s.io/cluster-issuer: letsencrypt&#xA;    certmanager.k8s.io/acme-challenge-type: dns01&#xA;    certmanager.k8s.io/acme-dns01-provider: cloudflare&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;We have the ingress enabled and use nginx as a controller. We specify a custom&#xA;            &lt;code&gt;server-snippet&lt;/code&gt; (used by the nginx-ingress to inject custom code into the server config) that adds a custom header&#xA;        (&lt;code&gt;X-Frame-Options&lt;/code&gt;). We use annotations to signal &lt;a href=&#34;cert-manager.readthedocs.io/&#34;&gt;cert-manager&lt;/a&gt; to&#xA;        provision a SSL certificate for this host.&lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        Other ingress controllers such as &lt;a href=&#34;https://github.com/kubernetes/ingress-gce&#34;&gt;GCE&lt;/a&gt; also make use of annotations&#xA;        to integrate with Google Cloud services. In this example we assign an external static IP and provision an SSL certificate&#xA;        (with &lt;a href=&#34;http://github.com/GoogleCloudPlatform/gke-managed-certs&#34;&gt;gke-managed-certs&lt;/a&gt;):&#xA;        &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;ingress:&#xA;  enabled: true&#xA;  annotations:&#xA;    kubernetes.io/ingress.class: gce&#xA;    kubernetes.io/ingress.global-static-ip-name: my-external-ip&#xA;    kubernetes.io/ingress.allow-http: &amp;#39;false&amp;#39;&#xA;    networking.gke.io/managed-certificates: example-certificate&#xA;  hosts:&#xA;    - host: example.org&#xA;      paths:&#xA;        - &amp;#34;/*&amp;#34;&#xA;  tls:&#xA;    - hosts:&#xA;        - example.org&#xA;      secretName: &amp;#34;example-org-tls&amp;#34;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Service&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        A Service is an abstraction for a grouping of pods. It selects pods based on labels and&#xA;        allows network access to them. There are several Service types that Kubernetes supports such as&#xA;        ClusterIP, LoadBalancer or NodePort.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The requirements for a Service template in a reusable Helm chart are:&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;ul&gt;&#xA;          &lt;li&gt;It should be possible to pick a Service type. Not everyone wants to run an application behind a Load Balancer&lt;/li&gt;&#xA;          &lt;li&gt;It should be possible to add annotations&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            These are fairly simple requirements and the &lt;a href=&#34;https://gist.github.com/arthurk/e7bec72e9e7f4ea8785656f582846421&#34;&gt;default Service template&lt;/a&gt; meets all of them. A usage example looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;service:&#xA;  type: ClusterIP&#xA;  port: 80&#xA;  annotations:&#xA;    prometheus.io/scrape: &amp;#34;true&amp;#34;&#xA;    prometheus.io/port: &amp;#34;4000&amp;#34;&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;        The service has a &lt;code&gt;ClusterIP&lt;/code&gt; Service type. In environments where higher availability is required&#xA;        this could be changed to a &lt;code&gt;LoadBalancer&lt;/code&gt;. The annotations are used by the Prometheus Helm chart:&#xA;        The prometheus server looks for all services in a cluster that have the &lt;code&gt;prometheus.io/scrape: &amp;#34;true&amp;#34;&lt;/code&gt;&#xA;        annotation and automatically scrapes them every minute.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Deployment&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;        A Deployment is an abstraction for pods. It runs multiple replicas of an application and keeps them&#xA;        in the desired state. If an application fails or becomes unresponsive it will be replaced automatically.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;p&gt;In a reusable Deployment template we should be able to:&lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Set the number of replicas&lt;/strong&gt;: Depending on the environment we should be able to adjust&#xA;                this value. A test environment doesn&amp;#39;t need to run as many replicas as a production environment.&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add Pod annotations&lt;/strong&gt;: Applications such as &lt;a href=&#34;https://linkerd.io/&#34;&gt;Linkerd&lt;/a&gt; use&#xA;                annotations to identify Pods into which to inject a sidecar container (&lt;code&gt;linkerd.io/inject: enabled&lt;/code&gt;)&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Pull the Docker image from a custom (private) registry&lt;/strong&gt;&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Modify arguments and environment variables&lt;/strong&gt;: As an example&#xA;                we should be able to pass &lt;code&gt;--log.level=debug&lt;/code&gt; to a container to see debug logs in case we have&#xA;                to identify problems with our application. Environment variables such as &lt;code&gt;MIX_ENV=prod&lt;/code&gt; often&#xA;                tell the application in which environment it is running and which configuration it should load&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add custom ConfigMaps and Secrets&lt;/strong&gt;: It should be possible to load application-specific&#xA;                configuration files or secrets that were added externally (for example SSL certificates for a database connection or API keys)&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Add Liveness and Readiness probes&lt;/strong&gt; to check if the container is started and alive&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Configure container resource limits and requests&lt;/strong&gt;: In test or staging environments we should be able to&#xA;                disable it or set it to a low value&#xA;            &lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Run Sidecar Containers&lt;/strong&gt;: If the application requires a database connection but the database is on CloudSQL it&#xA;                is often recommended to run &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloudsql-proxy/&#34;&gt;cloudsql-proxy&lt;/a&gt; as a&#xA;                sidecar container to establish a secure connection to the database&lt;/li&gt;&#xA;            &lt;li&gt;&#xA;                &lt;strong&gt;Allow to set Affinity and Tolerations&lt;/strong&gt;: To optimize the performance of the application we should be able&#xA;                to run it on the same machine as certain other applications or have it scheduled on a specific node pool&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;&#xA;        &lt;p&gt;Unlike the Ingress and Service templates, the &lt;a href=&#34;https://gist.github.com/arthurk/5f833ec5b264b84b6e1bedbd8eac69ea&#34;&gt;default template&lt;/a&gt; doesn&amp;#39;t meet the requirements from above. Specifically we need to allow to:&#xA;&#xA;&lt;/p&gt;&lt;ul&gt;&#xA;    &lt;li&gt;Add Pod annotations so that other applications such as Linkerd know where to inject sidecar containers&lt;/li&gt;&#xA;    &lt;li&gt;Replace &lt;code&gt;appVersion&lt;/code&gt; with &lt;code&gt;image.tag&lt;/code&gt;. This allows to change the docker image tag&#xA;        without having to re-package the chart with a different &lt;code&gt;appVersion&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;extraArgs&lt;/code&gt; to allow additional arguments to be passed into the container&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;env&lt;/code&gt; to allow additional environment variables to be passed into the container&lt;/li&gt;&#xA;    &lt;li&gt;Replace the default livenessProbe/readinessProbe with a block that allows us to set all values (the default&#xA;        template only allows &lt;code&gt;httpGet&lt;/code&gt; probes on &lt;code&gt;/&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;extraVolumes&lt;/code&gt; and &lt;code&gt;extraVolumeMounts&lt;/code&gt; to allow mounting of custom ConfigMaps and Secrets&lt;/li&gt;&#xA;    &lt;li&gt;Add &lt;code&gt;sidecarContainers&lt;/code&gt; to allow to inject additional containers into the pod&#xA;        (such as &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloudsql-proxy/&#34;&gt;cloudsql-proxy&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The modified template code looks as follows (&lt;span class=&#34;diff_add&#34;&gt;green&lt;/span&gt; text marks added and changed code):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  name: {{ include &amp;#34;mychart.fullname&amp;#34; . }}&#xA;  labels:&#xA;    {{- include &amp;#34;mychart.labels&amp;#34; . | nindent 4 }}&#xA;spec:&#xA;  replicas: {{ .Values.replicaCount }}&#xA;  selector:&#xA;    matchLabels:&#xA;      {{- include &amp;#34;mychart.selectorLabels&amp;#34; . | nindent 6 }}&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        {{- include &amp;#34;mychart.selectorLabels&amp;#34; . | nindent 8 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;      annotations:&#xA;      {{- if .Values.podAnnotations }}&#xA;        {{- toYaml .Values.podAnnotations | nindent 8 }}&#xA;      {{- end }}&lt;/span&gt;&#xA;    spec:&#xA;    {{- with .Values.imagePullSecrets }}&#xA;      imagePullSecrets:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;      serviceAccountName: {{ include &amp;#34;mychart.serviceAccountName&amp;#34; . }}&#xA;      securityContext:&#xA;        {{- toYaml .Values.podSecurityContext | nindent 8 }}&#xA;      containers:&#xA;        - name: {{ .Chart.Name }}&#xA;          securityContext:&#xA;            {{- toYaml .Values.securityContext | nindent 12 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          image: &amp;#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}&amp;#34;&lt;/span&gt;&#xA;          imagePullPolicy: {{ .Values.image.pullPolicy }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          args:&#xA;          {{- range $key, $value := .Values.extraArgs }}&#xA;            - --{{ $key }}={{ $value }}&#xA;          {{- end }}&#xA;          {{- if .Values.env }}&#xA;          env:&#xA;            {{ toYaml .Values.env | nindent 12}}&#xA;          {{- end }}&lt;/span&gt;&#xA;          ports:&#xA;            - name: http&#xA;              containerPort: 80&#xA;              protocol: TCP&lt;span class=&#34;diff_add&#34;&gt;&#xA;          {{- with .Values.livenessProbe }}&#xA;          livenessProbe:&#xA;            {{- toYaml . | nindent 12 }}&#xA;          {{- end }}&#xA;          {{- with .Values.readinessProbe }}&#xA;          readinessProbe:&#xA;            {{- toYaml . | nindent 12 }}&#xA;          {{- end }}&lt;/span&gt;&#xA;          resources:&#xA;            {{- toYaml .Values.resources | nindent 12 }}&lt;span class=&#34;diff_add&#34;&gt;&#xA;          volumeMounts:&#xA;          {{- if .Values.extraVolumeMounts }}&#xA;          {{ toYaml .Values.extraVolumeMounts | nindent 12 }}&#xA;          {{- end }}&#xA;       {{- if .Values.sidecarContainers }}&#xA;       {{- toYaml .Values.sidecarContainers | nindent 8 }}&#xA;       {{- end }}&#xA;      volumes:&#xA;      {{- if .Values.extraVolumes }}&#xA;      {{ toYaml .Values.extraVolumes | nindent 8}}&#xA;      {{- end }}&lt;/span&gt;&#xA;      {{- with .Values.nodeSelector }}&#xA;      nodeSelector:&#xA;        {{- toYaml . | nindent 8 }}&#xA;      {{- end }}&#xA;    {{- with .Values.affinity }}&#xA;      affinity:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;    {{- with .Values.tolerations }}&#xA;      tolerations:&#xA;        {{- toYaml . | nindent 8 }}&#xA;    {{- end }}&#xA;&lt;/pre&gt;&#xA;&#xA;    &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;    &lt;p&gt;&#xA;    The default helm chart template is a great starting point for building reusable helm charts. The Ingress and Service&#xA;    templates are perfect examples. The Deployment template is lacking a few options to be reusable enough but can&#xA;    easily be modified and improved.&#xA;    &lt;/p&gt;&#xA;&#xA;    &lt;p&gt;&#xA;    For good examples of reusable Helm charts I recommend checking the&#xA;    &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable&#34;&gt;helm/charts stable repo&lt;/a&gt;.&#xA;    Charts such as &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus&#34;&gt;Prometheus&lt;/a&gt;, &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/grafana&#34;&gt;Grafana&lt;/a&gt; or &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/nginx-ingress&#34;&gt;nginx-ingress&lt;/a&gt; are actively maintained and constantly improved. They are good references to look at when writing a new Helm chart.&#xA;    &lt;/p&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/writing-reusable-helm-charts/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Pre-Installed Daemons on Google Compute Engine</title>
    <updated>2020-02-23T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-23:/pre-installed-daemons-on-google-compute-engine/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;I found out that Google Compute Engine instances will come with the &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Google Guest Environment&lt;/a&gt; pre-installed which runs daemons in the background. This is unlike AWS EC2 instances which don&amp;#39;t install any daemons (but come with &lt;a href=&#34;https://github.com/aws/aws-cli&#34;&gt;aws-cli&lt;/a&gt; pre-installed). We can see the following output when listing the running processes on a new Debian GCE intance:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ ps ax | grep google&#xA;&#xA;418 ?        Ssl    2:52 /usr/bin/google_osconfig_agent&#xA;526 ?        Ss     2:17 /usr/bin/python3 /usr/bin/google_network_daemon&#xA;528 ?        Ss     3:32 /usr/bin/python3 /usr/bin/google_accounts_daemon&#xA;529 ?        Ss     1:14 /usr/bin/python3 /usr/bin/google_clock_skew_daemon&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can check all installed google packages:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ apt list --installed | grep google&#xA;&#xA;gce-disk-expand&#xA;google-cloud-packages-archive-keyring&#xA;google-cloud-sdk&#xA;google-compute-engine-oslogin&#xA;google-compute-engine&#xA;google-osconfig-agent&#xA;python-google-compute-engine&#xA;python3-google-compute-engine&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and systemd services:&lt;/p&gt;&#xA;&lt;pre&gt;$ systemctl list-unit-files | grep google&#xA;&#xA;google-accounts-daemon.service         enabled&#xA;google-clock-skew-daemon.service       enabled&#xA;google-instance-setup.service          enabled&#xA;google-network-daemon.service          enabled&#xA;google-osconfig-agent.service          enabled&#xA;google-shutdown-scripts.service        enabled&#xA;google-startup-scripts.service         enabled&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;These packages and services are part of the Google &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Linux Guest Environment&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/managing-instance-access&#34;&gt;OS Login Guest Environment&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The GCP docs have some information on the &lt;a href=&#34;https://cloud.google.com/compute/docs/images/guest-environment&#34;&gt;Guest Environment&lt;/a&gt; but it lacks details on the specifics of each daemon/script. A better source is the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/compute-image-packages/tree/master/packages/python-google-compute-engine&#34;&gt;GitHub repo&lt;/a&gt; where we can find a good explanation for each daemon and script:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-network-daemon:&lt;/strong&gt; handles network setup for multiple network interfaces on boot and integrates network load balancing with forwarding rule changes into the guest&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-accounts-daemon:&lt;/strong&gt; daemon to setup and manage user accounts, and to enable SSH key based authentication&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-clock-skew-daemon:&lt;/strong&gt; daemon to keep the system clock in sync after VM start and stop events&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-instance-setup:&lt;/strong&gt; scripts to execute VM configuration scripts during boot&lt;/li&gt;&#xA;    &lt;li&gt;&lt;strong&gt;google-startup-scripts/google-shutdown-scripts:&lt;/strong&gt; run user-provided scripts at VM startup and shutdown&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The remaining daemon is the agent for the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/guest-oslogin&#34;&gt;OS Login Guest Environment&lt;/a&gt;. It manages access control when using the &lt;a href=&#34;https://cloud.google.com/compute/docs/oslogin/&#34;&gt;OS Login&lt;/a&gt; feature by linking linux user accounts to Google accounts (which can then be managed with Cloud IAM). This feature is disabled by default and I&amp;#39;m not sure why the package is installed and the daemon is running.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Uninstall&lt;/h2&gt;&#xA;&lt;p&gt;If all that&amp;#39;s needed is a simple VM instance without Google Cloud integration, all daemons and scripts can be uninstalled by removing the packages:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ apt-get remove python-google-compute-engine python3-google-compute-engine \&#xA;                 google-osconfig-agent google-compute-engine-oslogin&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;I think it&amp;#39;s good to at least remove the &lt;code&gt;google-osconfig-agent&lt;/code&gt; package and get rid of the &lt;code&gt;google_osconfig_agent&lt;/code&gt; daemon running in the background. The package can be re-installed before enabling OS Login.&lt;/p&gt;&#xA;&lt;p&gt;Each daemon can also be disabled separately:&lt;/p&gt;&#xA;&lt;pre&gt;$ systemctl disable google-accounts-daemon.service&#xA;&lt;/pre&gt;&#xA;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/pre-installed-daemons-on-google-compute-engine/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Generating Ethereum Addresses in Python</title>
    <updated>2020-02-16T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-16:/generating-ethereum-addresses-in-python/</id>
    <content type="html">&#xA;        &#xA;        &#xA;        &lt;p&gt;&#xA;            I&amp;#39;ve been wondering how long it would take to generate all Ethereum private keys with addresses on my laptop.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            I know there is &lt;a href=&#34;https://bitcointalk.org/index.php?topic=7769.msg1010711#msg1010711&#34;&gt;not enough energy in our star system&lt;/a&gt;&#xA;            to do this in a reasonable timeframe, even on an imaginative computer that would use the absolute minimum of energy possible.&#xA;            This was more of a learning experience for me to get to know more about SHA-3 and KECCAK hashes,&#xA;            ECDSA curves, Public Keys and Ethereum addresses.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Due to its slow interpreter, Python is usually not a good choice when it comes to writing performant applications.&#xA;            The exception being Python modules which use an interface that calls C/C++ code. These modules are usually very fast,&#xA;            popular examples are &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&#34;https://numpy.org/&#34;&gt;Numpy&lt;/a&gt;.&#xA;            To generate Ethereum addresses we can use the following two Python modules which are both C based and have a good performance:&#xA;        &lt;/p&gt;&#xA;        &lt;ul&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;https://github.com/ofek/coincurve/&#34;&gt;coincurve&lt;/a&gt;: Cross-platform Python CFFI bindings for libsecp256k1&lt;/li&gt;&#xA;            &lt;li&gt;&lt;a href=&#34;https://github.com/tiran/pysha3&#34;&gt;pysha3&lt;/a&gt;: SHA-3 wrapper for Python (with support for keccak)&lt;/li&gt;&#xA;        &lt;/ul&gt;&#xA;        &lt;p&gt;&#xA;            Generating Ethereum addresses is a 3-step process:&#xA;        &lt;/p&gt;&#xA;        &lt;ol&gt;&#xA;            &lt;li&gt;Generate a private key&lt;/li&gt;&#xA;            &lt;li&gt;Derive the public key from the private key&lt;/li&gt;&#xA;            &lt;li&gt;Derive the Ethereum address from the public key&lt;/li&gt;&#xA;        &lt;/ol&gt;&#xA;        &lt;p&gt;&#xA;            Note that public keys and Ethereum addresses are not the same. Addresses are hashes of public keys.&#xA;            It&amp;#39;s not possible to send funds to a public key.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 1: Generate a private key&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Ethereum private keys are based on &lt;a href=&#34;https://keccak.team/keccak.html&#34;&gt;KECCAK-256 hashes&lt;/a&gt;. To generate such a hash we&#xA;            use the &lt;code&gt;keccak_256&lt;/code&gt; function from the pysha3 module on a random 32 byte seed:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;import secrets&#xA;from sha3 import keccak_256&#xA;&#xA;private_key = keccak_256(secrets.token_bytes(32)).digest()&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            Note that a KECCAK hash is not the same as a SHA-3 hash.&#xA;            KECCAK won a competition to become the SHA-3 standard but was slightly modified&#xA;            before it became standardized. Some SHA3 libraries such as pysha3 include&#xA;            the legacy KECCAK algorithm while others, such as the &lt;a href=&#34;https://docs.python.org/3.7/library/hashlib.html&#34;&gt;Python hashlib module&lt;/a&gt;,&#xA;            only implement the official SHA-3 standard.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 2: Derive the public key from the private key&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            To get our public key we need to sign our private key with an Elliptic Curve Digital Signature Algorithm (ECDSA).&#xA;            Ethereum uses the &lt;a href=&#34;https://en.bitcoin.it/wiki/Secp256k1&#34;&gt;secp256k1 curve ECDSA&lt;/a&gt;. Coincurve uses this as a default&#xA;            so we don&amp;#39;t need to explicitly specify it when calling the function:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;from coincurve import PublicKey&#xA;&#xA;public_key = PublicKey.from_valid_secret(private_key).format(compressed=False)[1:]&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;&#xA;            The &lt;a href=&#34;https://ethereum.github.io/yellowpaper/paper.pdf&#34;&gt;Ethereum Yellow Paper&lt;/a&gt; states that the public key has to be a byte array of size 64.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            By default coincurve uses the compressed format for public keys (libsecp256k1 was developed for Bitcoin where compressed keys are commonly used) which is 33 bytes in size.&#xA;            Uncompressed keys are 65 bytes in size. Additionally all public keys are&#xA;            prepended with a single byte to indicate if they are compressed or uncompressed.&#xA;            This means we first need to get the uncompressed 65 byte key (&lt;code&gt;compressed=False&lt;/code&gt;)&#xA;            and then strip the first byte (&lt;code&gt;[1:]&lt;/code&gt;) to get our 64 byte Ethereum public key.&#xA;        &lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Step 3: Derive the Ethereum address from the public key&lt;/h2&gt;&#xA;&#xA;        &lt;p&gt;&#xA;            We can now generate our Ethereum address:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;addr = keccak_256(public_key).digest()[-20:]&#xA;&lt;/pre&gt;&#xA;        &lt;p&gt;As specified in the &lt;a href=&#34;https://ethereum.github.io/yellowpaper/paper.pdf&#34;&gt;Yellow Paper&lt;/a&gt; we take the right most 20 bytes of the 32 byte KECCAK hash of the corresponding ECDSA public key.&lt;/p&gt;&#xA;&#xA;        &lt;h2&gt;Full Example&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            This is the full example code from the above steps. It generates a random private key, derives the address&#xA;            and prints them in hex format:&#xA;        &lt;/p&gt;&#xA;&lt;pre&gt;from secrets import token_bytes&#xA;from coincurve import PublicKey&#xA;from sha3 import keccak_256&#xA;&#xA;private_key = keccak_256(token_bytes(32)).digest()&#xA;public_key = PublicKey.from_valid_secret(private_key).format(compressed=False)[1:]&#xA;addr = keccak_256(public_key).digest()[-20:]&#xA;&#xA;print(&amp;#39;private_key:&amp;#39;, private_key.hex())&#xA;print(&amp;#39;eth addr: 0x&amp;#39; + addr.hex())&#xA;&#xA;### Output ###&#xA;# private_key: 7bf19806aa6d5b31d7b7ea9e833c202e51ff8ee6311df6a036f0261f216f09ef&#xA;# eth addr: 0x3db763bbbb1ac900eb2eb8b106218f85f9f64a13&#xA;&lt;/pre&gt;&#xA;&#xA;        &lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;        &lt;p&gt;&#xA;            I used the Python &lt;code&gt;timeit&lt;/code&gt; module to do &lt;a href=&#34;https://gist.github.com/arthurk/fbc876951379e2b0c889ea71b5167b4e&#34;&gt;a quick benchmark&lt;/a&gt; with the above code.&#xA;            The result is that my laptop can generate 18k addresses per second on a single cpu core.&#xA;            Using all 4 cpu cores that&amp;#39;s 72k addresses per second, ~6.2 billion (6.220.800.000) addresses per day or around&#xA;            two trillion (2.270.592.000.000) addresses per year.&#xA;        &lt;/p&gt;&#xA;        &lt;p&gt;&#xA;            Ethereum&amp;#39;s address space is 2^160. This means that by using this method it would take my laptop&#xA;            643665439999999976814879449351716864 (six hundred and forty-three decillion ...) years&#xA;            to generate all Ethereum private keys with addresses.&#xA;        &lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/generating-ethereum-addresses-in-python/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Validating Helm Chart Values with JSON Schemas</title>
    <updated>2020-02-08T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-08:/validate-helm-chart-values-with-json-schemas/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;        &lt;p&gt;Helm v3 &lt;a href=&#34;https://github.com/helm/helm/pull/5350&#34;&gt;added support&lt;/a&gt; to validate values in a chart&amp;#39;s &lt;code&gt;values.yaml&lt;/code&gt; file with &lt;a href=&#34;https://json-schema.org/&#34;&gt;JSON schemas&lt;/a&gt;. It allows us to do:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;Requirement checks. Example: An &lt;code&gt;API_KEY&lt;/code&gt; environment variable is set&lt;/li&gt;&#xA;    &lt;li&gt;Type validation. Example: The image tag is a string such as &lt;code&gt;&amp;#34;1.5&amp;#34;&lt;/code&gt; and not the number &lt;code&gt;1.5&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Range validation. Example: The value for a CPU utilization percentage key is between 1 and 100&lt;/li&gt;&#xA;    &lt;li&gt;Constraint Validation. Example: The &lt;code&gt;pullPolicy&lt;/code&gt; is &lt;code&gt;IfNotPresent&lt;/code&gt;, &lt;code&gt;Always&lt;/code&gt;, or &lt;code&gt;Never&lt;/code&gt;; A URL has the format &lt;code&gt;http(s)://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In this post I&amp;#39;m going to show how to create a JSON schema and use it to validate a chart&amp;#39;s &lt;code&gt;values.yaml&lt;/code&gt; file. After that I&amp;#39;m going to show how to automatically generate a schema from an existing values file.&lt;/p&gt;&lt;p&gt;&#xA;&#xA;&lt;/p&gt;&lt;h2&gt;Example&lt;/h2&gt;&#xA;&lt;p&gt;For this example I&amp;#39;m using the chart that is created when running &lt;code&gt;helm create mychart&lt;/code&gt;. We&amp;#39;ll create a JSON schema that will validate that the following conditions are met:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;&lt;code&gt;image.repository&lt;/code&gt; is a valid docker image name&lt;/li&gt;&#xA;    &lt;li&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt; is &lt;code&gt;IfNotPresent&lt;/code&gt;, &lt;code&gt;Always&lt;/code&gt; or &lt;code&gt;Never&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;The relevant part in the &lt;code&gt;values.yaml&lt;/code&gt; file is:&#xA;&#xA;&lt;pre&gt;image:&#xA;  repository: my-docker-image&#xA;  pullPolicy: IfNotPresent&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The JSON schema needs to be in a file named &lt;code&gt;values.schema.json&lt;/code&gt;. It has to be located in the same directory as the &lt;code&gt;values.yaml&lt;/code&gt; file. To match the requirements from above the file needs to have the following content:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;{&#xA;  &amp;#34;$schema&amp;#34;: &amp;#34;http://json-schema.org/schema#&amp;#34;,&#xA;  &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;,&#xA;  &amp;#34;required&amp;#34;: [&#xA;    &amp;#34;image&amp;#34;&#xA;  ],&#xA;  &amp;#34;properties&amp;#34;: {&#xA;    &amp;#34;image&amp;#34;: {&#xA;      &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;,&#xA;      &amp;#34;required&amp;#34;: [&#xA;        &amp;#34;repository&amp;#34;,&#xA;        &amp;#34;pullPolicy&amp;#34;&#xA;      ],&#xA;      &amp;#34;properties&amp;#34;: {&#xA;        &amp;#34;repository&amp;#34;: {&#xA;          &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,&#xA;          &amp;#34;pattern&amp;#34;: &amp;#34;^[a-z0-9-_]+$&amp;#34;&#xA;        },&#xA;        &amp;#34;pullPolicy&amp;#34;: {&#xA;          &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;,&#xA;          &amp;#34;pattern&amp;#34;: &amp;#34;^(Always|Never|IfNotPresent)$&amp;#34;&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/pre&gt;&#xA;&lt;p&gt;Note that putting a key in the &lt;code&gt;required&lt;/code&gt; array does not mean that it has a value. In YAML if a key doesn&amp;#39;t have a value it will be set to an empty string. To make sure the value was set, a regex for the &lt;code&gt;pattern&lt;/code&gt; key has to be added that matches a non-empty string.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To demonstrate that the validation is working I&amp;#39;m leaving the &lt;code&gt;repo&lt;/code&gt; empty and set &lt;code&gt;pullPolicy&lt;/code&gt; to an invalid value. Running lint shows the following output:&lt;/p&gt;&#xA;&lt;pre&gt;$ helm lint .&#xA;&#xA;==&amp;gt; Linting .&#xA;[ERROR] values.yaml: - image.repository: Invalid type. Expected: string, given: null&#xA;- image.pullPolicy: Does not match pattern &amp;#39;^(Always|Never|IfNotPresent)$&amp;#39;&#xA;&#xA;[ERROR] templates/: values don&amp;#39;t meet the specifications of the schema(s) in the following chart(s):&#xA;mychart:&#xA;- image.repository: Invalid type. Expected: string, given: null&#xA;- image.pullPolicy: Does not match pattern &amp;#39;^(Always|Never|IfNotPresent)$&amp;#39;&#xA;&#xA;Error: 1 chart(s) linted, 1 chart(s) failed&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The schema is automatically validated when running the following commands:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;helm install&lt;/li&gt;&#xA;    &lt;li&gt;helm upgrade&lt;/li&gt;&#xA;    &lt;li&gt;helm lint&lt;/li&gt;&#xA;    &lt;li&gt;helm template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The YAML values and the JSON schema need to be kept in sync manually. Helm will not check if keys from the YAML values file are missing from the schema. It will only validate fields that are specified in the schema.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating a JSON Schema for existing YAML values&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can infer a schema from existing YAML values and use it as a starting point when writing a new schema. The steps are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;    &lt;li&gt;Convert your values YAML file to JSON on &lt;a href=&#34;https://www.json2yaml.com/&#34;&gt;https://www.json2yaml.com/&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;Paste the JSON on &lt;a href=&#34;https://www.jsonschema.net&#34;&gt;https://www.jsonschema.net/&lt;/a&gt; and click on &amp;#34;Infer Schema&amp;#34;&lt;/li&gt;&#xA;    &lt;li&gt;Paste the schema into the &lt;code&gt;values.schema.json&lt;/code&gt; file&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;We can run &lt;code&gt;helm lint&lt;/code&gt; to make sure the schema has been generated correctly:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;$ helm lint mychart/&#xA;&#xA;==&amp;gt; Linting .&#xA;&#xA;1 chart(s) linted, 0 chart(s) failed&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The inferred schema will mark all keys as required and set their type. A regex can be added to keys to make sure they have a value set. The &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;default&lt;/code&gt; and &lt;code&gt;examples&lt;/code&gt; fields are not necessary for validating helm charts and can be removed.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/validate-helm-chart-values-with-json-schemas/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
  <entry>
    <title>Extracting Data from Invoices with Google AutoML Natural Language</title>
    <updated>2020-02-02T00:00:00Z</updated>
    <id>tag:www.arthurkoziel.com,2020-02-02:/automl-invoice-data-extraction/</id>
    <content type="html">&#xA;        &#xA;        &#xA;&#xA;    &lt;p&gt;In this tutorial I will show how to use &lt;a href=&#34;https://cloud.google.com/natural-language/automl/docs/&#34;&gt;Google AutoML Natural Language&lt;/a&gt; to setup a machine learning model that will automatically extract the total from invoices.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Why?&lt;/h2&gt;&#xA;    &lt;p&gt;Manually extracting data from invoices and entering them into an accounting system is time-consuming and tedious work.&lt;/p&gt;&#xA;    &lt;p&gt;To automate this there are template-based systems like &lt;a href=&#34;https://github.com/invoice-x/invoice2data&#34;&gt;invoice2data&lt;/a&gt; available. They extract the data using predefined extraction rules (regular expressions):&lt;/p&gt;&#xA;    &lt;pre&gt;Invoice Total: \$(\d+.\d{2})&lt;/pre&gt;&#xA;    &lt;p&gt;With such a system there&amp;#39;s still manual work required. YAML templates with extraction rules need to be written for each supplier and then maintained as invoice structures change over time. In the example above the supplier could decide to change &lt;code&gt;Invoice Total&lt;/code&gt; to &lt;code&gt;Total&lt;/code&gt; on the next invoice. The extraction would fail and the rule would have to be adjusted.&lt;/p&gt;&#xA;    &lt;p&gt;A better solution is to use a machine learning model that can extract the information without writing extraction rules. In this post I&amp;#39;m going to show how to setup such a model.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Steps To Do&lt;/h2&gt;&#xA;    &lt;p&gt;To build our invoice data extraction ML model we have to do the following steps:&lt;/p&gt;&#xA;    &lt;ul&gt;&#xA;        &lt;li&gt;Collect the training documents&lt;/li&gt;&#xA;        &lt;li&gt;Upload the documents to Google Cloud Storage&lt;/li&gt;&#xA;        &lt;li&gt;Create a CSV and JSONL file for AutoML to import the uploaded documents&lt;/li&gt;&#xA;        &lt;li&gt;Import the documents&lt;/li&gt;&#xA;        &lt;li&gt;Annotate the documents&lt;/li&gt;&#xA;        &lt;li&gt;Build/Train the model&lt;/li&gt;&#xA;    &lt;/ul&gt;&#xA;    &lt;p&gt;After that we can manually test the model by uploading invoices and checking how well it&amp;#39;s able to extract the data.&lt;/p&gt;&#xA;    &lt;p&gt;I&amp;#39;m using Google AutoML Natural Language which is part of Google Cloud Platform and a GCP account is required. Regarding cost there are two things to note: Google will give $300 free credit for new GCP accounts and $25 promotional credit for developers using AutoML for the first time.&lt;/p&gt;&#xA;    &lt;p&gt;Note that for the &lt;code&gt;gsutil&lt;/code&gt; commands below I installed the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud SDK&lt;/a&gt; but it&amp;#39;s not necessary to do so, the GCS operations from below can also be done over the Web UI.&lt;/p&gt;&#xA;&#xA;    &lt;h2&gt;Training Documents&lt;/h2&gt;&#xA;    &lt;p&gt;The first step is to collect training documents that are structured in the same way as the documents we want the model to handle later on. I was able to collect 150 invoices from different consultants. All invoices are text based PDF files with either 1 or 2 pages and the total written somewhere in the bottom right (single page invoices) or top-right (two-page invoices).&lt;/p&gt;&#xA;    &lt;p&gt;This is almost the minimum amount of training documents that is required by AutoML to work. Feel free to add more. The more training documents the better the model&amp;#39;s performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It&amp;#39;s important to use documents that have a similar structure. AutoML will use those documents to build the model by trying out different algorithms to find patterns. If those documents are structured differently it won&amp;#39;t be able to find any patterns and the model will have a poor performance.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Uploading Documents to GCS&lt;/h2&gt;&#xA;&lt;p&gt;All documents need to be stored in a Google Cloud Storage (GCS) bucket. AutoML doesn&amp;#39;t support other document sources. There are three important restrictions when creating the bucket:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Location Type&lt;/em&gt; has to be &lt;code&gt;Region&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Location&lt;/em&gt; has to be &lt;code&gt;us-central1 (Iowa)&lt;/code&gt;&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;em&gt;Storage Class&lt;/em&gt; has to be &lt;code&gt;Standard&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The bucket can be created by running:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil mb -l us-central1 gs://automl-nlp-example&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have all my training documents in a folder called &lt;code&gt;invoices/&lt;/code&gt; and named with the same pattern: &lt;code&gt;invoice-X&lt;/code&gt; where X is the number of the invoice (1 to 150). I recommend using the same pattern for all the invoice files as we will need to iterate over them later on. We can upload the folder with the invoices by running:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;gsutil -m cp -r invoices/ gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Importing the documents&lt;/h2&gt;&#xA;&lt;p&gt;For AutoML to be able to import the training documents we need to create a CSV file. The CSV file contains a link to a &lt;a href=&#34;http://jsonlines.org/&#34;&gt;JSONL&lt;/a&gt; document and the JSONL file then contains links to the actual invoice PDF files. The files are imported in the following way:&lt;/p&gt;&#xA;&lt;p&gt;AutoML -&amp;gt; CSV file  JSONL file  PDF file(s)&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Creating the CSV file&lt;/h2&gt;&#xA;&lt;p&gt;Creating the CSV file is simple and requires only one line:&lt;/p&gt;&lt;p&gt;&#xA;&lt;/p&gt;&lt;pre&gt;,gs://automl-nlp-example/data.jsonl&lt;/pre&gt;&#xA;&lt;p&gt;It&amp;#39;s important to have the comma at the beginning of the line which will make AutoML randomly assign the documents to different sets (TRAIN, VALIDATION, TEST). It will use:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;80% of the PDF documents for training the model&lt;/li&gt;&#xA;    &lt;li&gt;10% for validating the results during training&lt;/li&gt;&#xA;    &lt;li&gt;10% for verifying the model&amp;#39;s results after it has been trained&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We can upload the CSV file to GCS:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil cp data.csv gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Creating the JSONL File&lt;/h2&gt;&#xA;&lt;p&gt;The JSONL file contains links to the invoice PDF documents. Each line links to one PDF document and needs to have the following structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;{&amp;#34;document&amp;#34;: {&amp;#34;input_config&amp;#34;: {&amp;#34;gcs_source&amp;#34;: {&amp;#34;input_uris&amp;#34;: [ &amp;#34;gs://automl-nlp-example/invoice-1.pdf&amp;#34; ]}}}}&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We need to repeat this line for all documents and change the value of &lt;code&gt;input_uris&lt;/code&gt; to use the actual filename of the PDF file. I used a small Python script to generate the file (this is easy since my documents follow the same filename pattern) and upload it to GCS:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;python3 -c &amp;#39;for x in range(1, 151): print(&amp;#34;&amp;#34;&amp;#34;{&amp;#34;document&amp;#34;: {&amp;#34;input_config&amp;#34;: {&amp;#34;gcs_source&amp;#34;: {&amp;#34;input_uris&amp;#34;: [ &amp;#34;gs://automl-nlp-example/invoice-%s.pdf&amp;#34; ]}}}}&amp;#34;&amp;#34;&amp;#34; % x)&amp;#39; &amp;gt; data.jsonl&#xA;&#xA;gsutil cp data.jsonl gs://automl-nlp-example/&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now we should have the following files in the GCS bucket:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;The &lt;code&gt;invoice/&lt;/code&gt; directory contains all invoices as PDF files&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;code&gt;data.csv&lt;/code&gt; file contains a link to the JSONL file&lt;/li&gt;&#xA;    &lt;li&gt;The &lt;code&gt;data.jsonl&lt;/code&gt; file contains links to the PDF files&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Creating the AutoML Dataset&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We can start creating the dataset in the GCP console. Go to &lt;em&gt;Natual Language&lt;/em&gt; and then &lt;em&gt;AutoML Entity Extraction&lt;/em&gt; to create the dataset. The location has to be &lt;em&gt;Global&lt;/em&gt; and the model objective has to be &lt;em&gt;Entity Extraction&lt;/em&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;create-new-dataset.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The CSV file can be imported from the GCS bucket at the bottom of the screen:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;import-csv.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case the import process took &lt;em&gt;13 minutes&lt;/em&gt; to finish.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;import-done.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I created a &lt;code&gt;totalPrice&lt;/code&gt; label. We can do this in the bottom by clicking on &amp;#34;&lt;em&gt;Add New Label&lt;/em&gt;&amp;#34;. We will use that label in the next step to annotate the entity we want to extract.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Annotating the Documents&lt;/h2&gt;&#xA;&lt;p&gt;Annotating the documents is the most time-consuming part. We need to go through the following number of invoice documents in each set and mark the total in them:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;    &lt;li&gt;100 documents in the Training set&lt;/li&gt;&#xA;    &lt;li&gt;10 documents in the Validation set&lt;/li&gt;&#xA;    &lt;li&gt;10 documents in the Test set&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;This is the minimum number of annotations to make the model train. Feel free to annotate more documents if there&amp;#39;s time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next we click on a document and navigate to the &lt;em&gt;Structured Text&lt;/em&gt; view which. This will show the content of the PDF file and make it easy to annotate by simply selecting the text with the mouse and picking a label in the overlay popup. In the example below I would annotate the total: &lt;code&gt;5,032.50&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;annotating.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using this view AutoML will use the PDFs annotation&amp;#39;s position during training and learn to distinguish between entities based on the position of the annotation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After all the documents have been annotated we can switch to the &amp;#34;TRAIN&amp;#34; tab and start the training:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;pre-start-training.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case the training process took &lt;em&gt;2 hours and 18 minutes&lt;/em&gt; to finish.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Manual Testing&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;After we trained the model we can try it out by uploading an invoice to GCS:&lt;/p&gt;&#xA;&lt;pre&gt;gsutil cp testing-invoice.pdf gs://automl-nlp-example/&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then selecting it in the &amp;#34;TEST &amp;amp; USE&amp;#34; tab and clicking on the &amp;#34;PREDICT&amp;#34; button:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;test-and-use.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The prediction will only take a second and shows the result in a PDF view:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;prediction-results.png&#34;/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the example above we can see that the &lt;code&gt;totalPrice&lt;/code&gt; was successfully extracted. Feel free to try it out with other invoices. Multiple documents can be submitted by using the &lt;a href=&#34;https://cloud.google.com/natural-language/automl/docs/predict#batch_prediction&#34;&gt;Batch prediction REST API&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I found Google AutoML Natural Language easy to use. Most of the tasks (except creating the CSV and JSONL files) can be done in the Web UI and the whole process doesn&amp;#39;t require any coding experience. I only had a very small training dataset available but the results are good enough. In my manual tests I was able to extract the total in around 80% of the cases. I&amp;#39;m sure this could be improved by making a higher quality training dataset. The downside is that the price for AutoML is high. It cost me around $25 to train, test and deploy this model.&lt;/p&gt;&#xA;    </content>
    <link href="https://www.arthurkoziel.com/automl-invoice-data-extraction/" rel="alternate"></link>
    <summary type="html"></summary>
    <author>
      <name>Arthur Koziel</name>
      <email>arthur@arthurkoziel.com</email>
    </author>
  </entry>
</feed>
